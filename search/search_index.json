{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome \ud83d\udc4b","text":"<p>This is my ML notes site.</p>"},{"location":"cheatsheets/","title":"Cheatsheets","text":""},{"location":"notes/lecture3/","title":"Dimensionality Reduction","text":"<p>Dimensionality reduction is the process of reducing the number of features (dimensions) in a dataset while preserving the most informative and discriminative aspects.</p> <p>It is one of the most critical techniques in machine learning and data science, because:</p> <ul> <li>Models often perform poorly when overwhelmed with too many features (curse of dimensionality).</li> <li>Redundant and irrelevant features introduce noise.</li> <li>High-dimensional spaces are sparse, which makes clustering and classification difficult.</li> <li>Reduced dimensionality leads to better generalization, interpretability, visualization, and faster computation.</li> </ul>"},{"location":"notes/lecture3/#1-the-usual-supervised-learning-approach","title":"1. The Usual Supervised Learning Approach","text":"<pre><code>flowchart LR\n    subgraph Training\n        A[Data: Features] --&gt; M[Learning Algorithm]\n        L[Labels] --&gt; M\n        M --&gt; B[Model]\n    end\n\n    subgraph Testing\n        T[Test Data] --&gt; B\n        B --&gt; P[Predicted Label]\n    end</code></pre> <ul> <li>A dataset consists of features (X) and labels (Y).</li> <li>A supervised learning algorithm learns a mapping X \u2192 Y.</li> <li>With high-dimensional data:</li> <li>The algorithm becomes overwhelmed.</li> <li>Training time is long.</li> <li>The risk of overfitting increases due to sparse data.</li> </ul>"},{"location":"notes/lecture3/#2-feature-types","title":"2. Feature Types","text":"<p>Not all features contribute equally to prediction.</p> <ul> <li> <p>Relevant Features</p> </li> <li> <p>Provide useful predictive information.</p> </li> <li> <p>Example: Diaper, Stroller, Bassinet, Cradle for predicting baby products.</p> </li> <li> <p>Irrelevant Features</p> </li> <li> <p>Provide no predictive signal.</p> </li> <li> <p>Example: Random identifiers.</p> </li> <li> <p>Redundant Features</p> </li> <li> <p>Highly correlated with other features, duplicating information.</p> </li> <li>Example: Stroller and wheels often co-occur.</li> </ul> <pre><code>mindmap\n  root((Features))\n    Relevant\n      \"Strong signal for prediction\"\n    Irrelevant\n      \"No contribution\"\n    Redundant\n      \"Overlaps with others\"</code></pre>"},{"location":"notes/lecture3/#3-approaches-to-dimensionality-reduction","title":"3. Approaches to Dimensionality Reduction","text":"<p>Two broad strategies:</p>"},{"location":"notes/lecture3/#31-feature-selection-downsizing-existing-features","title":"3.1 Feature Selection (downsizing existing features)","text":"<ul> <li>Removes noisy, irrelevant, or redundant features.</li> <li>Keeps original features intact.</li> <li>Preserves interpretability.</li> <li>Useful when:</li> <li>Budget constraints exist (e.g., costly medical tests).</li> <li>Small number of features is required for human interpretability.</li> </ul>"},{"location":"notes/lecture3/#32-low-dimensional-feature-learning-new-derived-features","title":"3.2 Low-Dimensional Feature Learning (new derived features)","text":"<ul> <li>Learns a new representation of the data.</li> <li>May lose interpretability but increases performance.</li> <li>Examples:</li> <li>Linear methods: PCA, MDS</li> <li>Non-linear methods: Kernel PCA, t-SNE</li> <li>Representation learning: Neural embeddings</li> </ul> <pre><code>graph TD\n    X[High-Dimensional Data] --&gt;|Feature Selection| S[Reduced Subset of Features]\n    X --&gt;|Feature Learning| F[New Derived Features]</code></pre>"},{"location":"notes/lecture3/#4-feature-selection-techniques","title":"4. Feature Selection Techniques","text":"Method Idea Pros Cons Examples Wrapper Use ML models to search best subset Captures feature interactions Computationally expensive Sequential Forward Selection, Backward Elimination Filter Rank features by statistical score Fast, scalable Ignores interactions Pearson Correlation, Chi-squared, Mutual Information Embedded Feature selection during training Efficient, task-specific Model-dependent Lasso Regression, Decision Trees"},{"location":"notes/lecture3/#5-wrapper-methods","title":"5. Wrapper Methods","text":""},{"location":"notes/lecture3/#51-exhaustive-search","title":"5.1 Exhaustive Search","text":"<ul> <li>\\(N\\) features \u2192 \\(2^N\\) possible subsets.</li> <li>Quickly infeasible:</li> <li>20 features \u2192 \\~1 million subsets</li> <li>25 features \u2192 \\~33.5 million subsets</li> <li>30 features \u2192 \\~1.1 billion subsets</li> </ul>"},{"location":"notes/lecture3/#52-sequential-forward-selection-sfs","title":"5.2 Sequential Forward Selection (SFS)","text":"<ol> <li>Start with empty set \\(S = \\emptyset\\).</li> <li>While stopping criteria not met:</li> <li>For each feature \\(X_f \\notin S\\):<ul> <li>\\(S' = S \\cup \\{X_f\\}\\)</li> <li>Train model on \\(S'\\).</li> <li>Evaluate accuracy.</li> </ul> </li> <li>Select feature with highest improvement.</li> <li>Return final \\(S\\).</li> </ol> <pre><code>flowchart TD\n    Start[Start with empty set S = empty] --&gt; Loop{Stopping Criteria Met?}\n    Loop -- No --&gt; Add[Add best feature]\n    Add --&gt; Train[Train + Evaluate]\n    Train --&gt; Loop\n    Loop -- Yes --&gt; End[Return Subset S]</code></pre> <ul> <li>Advantage: Captures strong features.</li> <li>Disadvantage: Cannot remove redundant features once added.</li> </ul>"},{"location":"notes/lecture3/#53-sequential-backward-elimination-sbe","title":"5.3 Sequential Backward Elimination (SBE)","text":"<ul> <li>Start with all features.</li> <li>Iteratively remove least useful feature.</li> <li>Stop when removal hurts performance.</li> </ul>"},{"location":"notes/lecture3/#54-heuristic-search","title":"5.4 Heuristic Search","text":"<ul> <li>Use optimization strategies like Genetic Algorithms, Simulated Annealing, Greedy search.</li> </ul>"},{"location":"notes/lecture3/#6-filter-methods","title":"6. Filter Methods","text":"<p>Principle: Replace costly model evaluation with fast statistics \\(J(X_f)\\).</p> <p>Examples:</p> <ul> <li>Mutual Information (MI): Captures dependence.</li> <li>Pearson Correlation: Measures linear relationships.</li> <li>Chi-squared test: For categorical features.</li> </ul>"},{"location":"notes/lecture3/#example-ranking-table","title":"Example Ranking Table","text":"Feature Index Score (\\(J(X_f)\\)) 32 0.98 5501 0.94 101 0.91 345 0.85 1104 0.81"},{"location":"notes/lecture3/#7-pearsons-correlation-coefficient","title":"7. Pearson\u2019s Correlation Coefficient","text":"<p>Captures linear relationships between feature \\(A\\) and target \\(Y\\):</p> \\[ \\rho(A,Y) = \\frac{\\text{cov}(A,Y)}{\\sigma_A \\cdot \\sigma_Y} \\] <p>Expanded:</p> \\[ \\rho(A,Y) = \\frac{\\sum_i (A_i - \\bar{A})(Y_i - \\bar{Y})}{\\sqrt{\\sum_i (A_i - \\bar{A})^2} \\cdot \\sqrt{\\sum_i (Y_i - \\bar{Y})^2}} \\] <ul> <li>\\(A_i, Y_i\\): sample values.</li> <li>\\(\\bar{A}, \\bar{Y}\\): means.</li> <li>Covariance matrix captures pairwise correlations.</li> </ul>"},{"location":"notes/lecture3/#8-embedded-methods","title":"8. Embedded Methods","text":"<ul> <li>Selection happens inside the learning algorithm.</li> <li>Examples:</li> <li>Lasso regression (L1): zeroes out unimportant features.</li> <li>Tree-based methods: select splits only on important features.</li> </ul> <pre><code>flowchart LR\n    Data --&gt; Model[Model with Embedded Selection]\n    Model --&gt; Output[Reduced Feature Set]</code></pre>"},{"location":"notes/lecture3/#extended-notes-on-dimensionality-reduction","title":"Extended Notes on Dimensionality Reduction","text":""},{"location":"notes/lecture3/#9-mutual-information-mi","title":"9. Mutual Information (MI)","text":"<p>Measures reduction in uncertainty:</p> \\[ I(A,Y) = \\sum_{x \\in A}\\sum_{y \\in Y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} \\] <ul> <li>If \\(A\\) and \\(Y\\) are independent \u2192 \\(I(A,Y)=0\\).</li> </ul>"},{"location":"notes/lecture3/#10-feature-interactions","title":"10. Feature Interactions","text":"<p>Individual features may appear weak, but combinations can be strong.</p> <ul> <li>Example: Two independent features become predictive when combined.</li> <li>Highlights the limitation of filter methods which evaluate features individually.</li> </ul>"},{"location":"notes/lecture3/#11-pros-cons-of-filter-methods","title":"11. Pros &amp; Cons of Filter Methods","text":"<p>Advantages:</p> <ul> <li>Simple, scalable.</li> <li>Easily parallelizable.</li> </ul> <p>Disadvantages:</p> <ul> <li>Cannot capture feature interactions.</li> <li>May select redundant features.</li> </ul>"},{"location":"notes/lecture3/#12-embedded-methods-lasso","title":"12. Embedded Methods: LASSO","text":"<p>Optimization problem:</p> \\[ \\min_\\beta \\sum_{i=1}^M (y_i - \\sum_j x_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^N |\\beta_j| \\] <ul> <li>L1 penalty induces sparsity.</li> <li>Contrast with Ridge (L2):</li> </ul> \\[ \\lambda \\sum_{j=1}^N \\beta_j^2 \\] <ul> <li>L2 shrinks weights but rarely eliminates them.</li> </ul>"},{"location":"notes/lecture3/#13-benefits-of-feature-selection","title":"13. Benefits of Feature Selection","text":"<ul> <li>Improves accuracy.</li> <li>Reduces computation.</li> <li>Improves interpretability.</li> <li>Prevents overfitting.</li> </ul>"},{"location":"notes/lecture3/#14-t-sne-motivation","title":"14. t-SNE Motivation","text":"<ul> <li>High-dimensional visualization is difficult.</li> <li>Project data into 2D or 3D for interpretability.</li> <li>t-SNE preserves local structure.</li> </ul>"},{"location":"notes/lecture3/#15-general-problem-statement","title":"15. General Problem Statement","text":"<p>Given high-dimensional \\(X = \\{x_1,...,x_M\\}\\), \\(x_i \\in \\mathbb{R}^N\\):</p> <p>Find \\(Y = \\{y_1,...,y_M\\}\\), \\(y_i \\in \\mathbb{R}^n, n &lt; N\\), minimizing information loss.</p>"},{"location":"notes/lecture3/#16-stochastic-neighbor-embedding-sne","title":"16. Stochastic Neighbor Embedding (SNE)","text":"<ul> <li>Preserves local distances.</li> <li>Converts distances to conditional probabilities:</li> </ul> \\[ p_{j|i} = \\frac{e^{-||x_i-x_j||^2/2\\sigma_i^2}}{\\sum_k e^{-||x_i-x_k||^2/2\\sigma_i^2}} \\] \\[ q_{j|i} = \\frac{e^{-||y_i-y_j||^2}}{\\sum_k e^{-||y_i-y_k||^2}} \\]"},{"location":"notes/lecture3/#17-from-sne-to-t-sne","title":"17. From SNE to t-SNE","text":"<ul> <li>High-dimensional similarities use Gaussian.</li> <li>Low-dimensional similarities use Student\u2019s t-distribution (heavier tails).</li> </ul> \\[ q_{ij} = \\frac{(1+||y_i-y_j||^2)^{-1}}{\\sum_{k \\neq l}(1+||y_k-y_l||^2)^{-1}} \\]"},{"location":"notes/lecture3/#18-kl-divergence-in-t-sne","title":"18. KL Divergence in t-SNE","text":"<p>Objective:</p> \\[ KL(P||Q) = \\sum_i \\sum_j p_{j|i}\\log \\frac{p_{j|i}}{q_{j|i}} \\] <ul> <li>Penalizes mismatches in similarity structure.</li> <li>Large \\(p_{j|i}\\) with small \\(q_{j|i}\\) \u2192 heavy penalty.</li> </ul>"},{"location":"notes/lecture3/#19-gradient-descent-optimization","title":"19. Gradient Descent Optimization","text":"<p>Gradient update rule:</p> \\[ \\frac{\\partial C}{\\partial y_i} = 2\\sum_j (y_j-y_i)(p_{j|i}-q_{j|i}+p_{i|j}-q_{i|j}) \\] <ul> <li>Emphasizes preserving close distances.</li> </ul>"},{"location":"notes/lecture3/#20-visualization-example-mnist","title":"20. Visualization Example (MNIST)","text":"<ul> <li>Applied to 6000 MNIST digits.</li> <li>t-SNE clusters digits cleanly compared to Sammon mapping.</li> </ul>"},{"location":"notes/lecture3/#21-summary-mindmap","title":"21. Summary Mindmap","text":"<pre><code>mindmap\n  root((Dimensionality Reduction))\n    Feature Selection\n      Wrapper\n        Forward Selection\n        Backward Elimination\n        Heuristic Search\n      Filter\n        Correlation\n        Chi-squared\n        Mutual Information\n      Embedded\n        Lasso (L1)\n        Decision Trees\n    Feature Learning\n      PCA\n      Kernel PCA\n      MDS\n      Autoencoders\n      t-SNE\n        SNE\n        KL Divergence\n        Gradient Descent</code></pre>"},{"location":"notes/lecture3/#key-takeaways","title":"\u2705 Key Takeaways","text":"<ul> <li>Dimensionality reduction combats the curse of dimensionality.</li> <li>Two approaches: Feature Selection (simpler, interpretable) vs. Feature Learning (powerful, less interpretable).</li> <li>Methods include Wrapper, Filter, Embedded selection, PCA, t-SNE, autoencoders.</li> <li>t-SNE is best for visualization, capturing local structures.</li> <li>Feature selection methods improve performance and interpretability for predictive modeling.</li> </ul>"},{"location":"notes/lecture3/#dimensionality-reduction-svd-pca-mf-nmf","title":"Dimensionality Reduction (SVD \u2022 PCA \u2022 MF \u2022 NMF)","text":""},{"location":"notes/lecture3/#motivation","title":"\ud83e\udded Motivation","text":"<p>We are deluged with data. Essential information often lives in a much smaller-dimensional subspace. Dimensionality reduction aims to:</p> <ul> <li>compress data (fewer numbers, less storage),</li> <li>denoise (throw away directions dominated by noise),</li> <li>uncover structure (clusters, topics, latent factors),</li> <li>speed up downstream learning.</li> </ul> <pre><code>flowchart LR\n  X[High-dim Data X \u2208 R^{m\u00d7n}] --&gt;|Decompose/Project| Z[Low-dim Representation]\n  Z --&gt;|Reconstruct| Xhat[Approximate X]\n  Xhat --&gt;|Error||e||Reconstruction Error||\n  Z --&gt; Models[Downstream models: kNN, SVM, regressors]\n  subgraph Families\n    A1[SVD / PCA (linear, global)]\n    A2[MF / NMF (parts-based, sparse)]\n    A3[t-SNE / UMAP (nonlinear, local)]\n  end\n  X --&gt; A1\n  X --&gt; A2\n  X --&gt; A3</code></pre>"},{"location":"notes/lecture3/#1-singular-value-decomposition-svd","title":"1) Singular Value Decomposition (SVD)","text":""},{"location":"notes/lecture3/#definition","title":"Definition","text":"<p>For any real matrix \\(X \\in \\mathbb{R}^{m\\times n}\\):</p> \\[ X = U\\,\\Sigma\\,V^T, \\] <p>where</p> <ul> <li>\\(U\\in\\mathbb{R}^{m\\times m}\\) and \\(V\\in\\mathbb{R}^{n\\times n}\\) are orthogonal (\\(U^TU=I\\), \\(V^TV=I\\)),</li> <li>\\(\\Sigma\\in\\mathbb{R}^{m\\times n}\\) is diagonal with non\u2011negative singular values \\(\\sigma_1\\ge\\sigma_2\\ge\\cdots\\ge0\\).</li> </ul>"},{"location":"notes/lecture3/#intuition","title":"Intuition","text":"<p>A linear map = rotate (V) \u2192 scale (\u03a3) \u2192 rotate (U). Large \\(\\sigma_i\\) indicate energetic directions.</p> <pre><code>graph TD\n  V[Rotate by V^T] --&gt; S[Scale by \u03a3] --&gt; U[Rotate by U]</code></pre>"},{"location":"notes/lecture3/#best-rankk-approximation-eckartyoung","title":"Best rank\u2011k approximation (Eckart\u2013Young)","text":"<p>Let \\(X_k = U_{:,1:k}\\,\\Sigma_{1:k,1:k}\\,V_{:,1:k}^T\\). Then for any matrix \\(Y\\) with \\(\\operatorname{rank}(Y)\\le k\\):</p> \\[ \\|X - X_k\\|_F \\le \\|X - Y\\|_F\\quad\\text{and}\\quad \\|X - X_k\\|_2 = \\sigma_{k+1}. \\]"},{"location":"notes/lecture3/#worked-example-exact-svd-of-a-22-matrix","title":"\ud83d\udd22 Worked Example \u2013 Exact SVD of a 2\u00d72 matrix","text":"<p>Let</p> \\[ X=\\begin{bmatrix}3 &amp; 1\\\\1 &amp; 3\\end{bmatrix}. \\] <ol> <li>Compute \\(X^TX=\\begin{bmatrix}10&amp;6\\\\6&amp;10\\end{bmatrix}\\).</li> <li>Eigenpairs of \\(X^TX\\):</li> <li>\\(\\lambda_1=16\\) with eigenvector \\(v_1=\\tfrac{1}{\\sqrt2}[1,\\,1]^T\\),</li> <li>\\(\\lambda_2=4\\) with eigenvector \\(v_2=\\tfrac{1}{\\sqrt2}[1,\\,-1]^T\\). Hence singular values: \\(\\sigma_1=\\sqrt{16}=4\\), \\(\\sigma_2=\\sqrt{4}=2\\). Let \\(V=[v_1\\ v_2]\\).</li> <li>Compute left vectors: \\(u_i = \\tfrac{1}{\\sigma_i} X v_i\\).</li> <li>\\(u_1 = \\tfrac{1}{4}Xv_1 = \\tfrac{1}{4}\\tfrac{1}{\\sqrt2}[4,\\,4]^T = \\tfrac{1}{\\sqrt2}[1,1]^T\\),</li> <li>\\(u_2 = \\tfrac{1}{2}Xv_2 = \\tfrac{1}{2}\\tfrac{1}{\\sqrt2}[2,\\,-2]^T = \\tfrac{1}{\\sqrt2}[1,-1]^T\\). Let \\(U=[u_1\\ u_2]\\), \\(\\Sigma=\\operatorname{diag}(4,2)\\).</li> <li>Verify: \\(U\\Sigma V^T=X\\).</li> </ol>"},{"location":"notes/lecture3/#worked-example-rank1-truncation-error","title":"\ud83d\udd22 Worked Example \u2013 Rank\u20111 truncation &amp; error","text":"<p>Rank\u20111 approximation \\(X_1=4\\,u_1 v_1^T\\). Error in spectral norm = \\(\\sigma_2=2\\). In Frobenius norm: \\(\\|X-X_1\\|_F=\\sqrt{\\sigma_2^2}=2\\).</p>"},{"location":"notes/lecture3/#power-iteration-computing-top-singular-vector","title":"Power Iteration (computing top singular vector)","text":"<ul> <li>Initialize \\(u^{(0)}\\) randomly.</li> <li>Iterate: \\(v^{(t)}=X^Tu^{(t)};\\ u^{(t+1)} = Xv^{(t)}/\\|Xv^{(t)}\\|\\).</li> <li>Converges to top left singular vector.</li> </ul> <pre><code>sequenceDiagram\n  participant U as u^(t)\n  participant XT as X^T\n  participant V as v^(t)\n  participant X as X\n  U-&gt;&gt;XT: v = X^T u\n  XT--&gt;&gt;V: v^(t)\n  V-&gt;&gt;X: u' = X v\n  X--&gt;&gt;U: normalize(u')</code></pre>"},{"location":"notes/lecture3/#2-principal-component-analysis-pca","title":"2) Principal Component Analysis (PCA)","text":""},{"location":"notes/lecture3/#two-equivalent-views","title":"Two equivalent views","text":"<ol> <li>Eigenview on covariance: with centered data \\(X_c\\), \\(\\tfrac{1}{n}X_cX_c^T = U\\Lambda U^T\\). PCs are columns of \\(U\\).</li> <li>SVD view: \\(X_c=U\\Sigma V^T\\). Then principal directions = columns of \\(U\\); variances = \\(\\sigma_i^2/(n-1)\\).</li> </ol>"},{"location":"notes/lecture3/#algorithm","title":"Algorithm","text":"<ol> <li>Center features: \\(X_c = X - \\mu 1^T\\).</li> <li>Compute SVD: \\(X_c=U\\Sigma V^T\\).</li> <li>Choose \\(k\\) via explained variance ratio \\(\\sum_{i=1}^k \\sigma_i^2 / \\sum_{i} \\sigma_i^2\\).</li> <li>Low\u2011dim representation (scores): \\(Z=U_{:,1:k}^T X_c\\).</li> <li>Reconstruction: \\(\\hat X = U_{:,1:k}Z + \\mu 1^T\\).</li> </ol> <pre><code>flowchart LR\n  A[Raw data X] --&gt; B[Center by mean \u03bc]\n  B --&gt; C[SVD of X_c]\n  C --&gt; D[Pick k by EVR]\n  D --&gt; E[Scores Z = U_k^T X_c]\n  E --&gt; F[Use Z for ML]\n  E --&gt; G[Reconstruct X\u0302 = U_k Z + \u03bc]</code></pre>"},{"location":"notes/lecture3/#worked-example-pca-on-a-2d-toy-dataset","title":"\ud83d\udd22 Worked Example \u2013 PCA on a 2D toy dataset","text":"<p>Data (n=5 points): \\((2,0),(0,2),(3,1),(4,0),(0,3)\\).</p> <ol> <li>Mean: \\(\\mu=[1.8,\\ 1.2]^T\\). Centered matrix</li> </ol> \\[ X_c=\\begin{bmatrix} 0.2 &amp; -1.2\\\\ -1.8 &amp; 0.8\\\\ 1.2 &amp; -0.2\\\\ 2.2 &amp; -1.2\\\\ -1.8 &amp; 1.8 \\end{bmatrix}. \\] <ol> <li>Covariance: \\(C=\\tfrac{1}{n-1}X_c^TX_c=\\begin{bmatrix}3.7 &amp; -2.9\\\\ -2.9 &amp; 2.3\\end{bmatrix}.\\)</li> <li>Eigenpairs (rounded): \\(\\lambda_1\\approx5.89\\) (dir. \\(v_1\\propto[0.82,-0.57]\\)), \\(\\lambda_2\\approx0.11\\).</li> <li>Explained variance of PC1: \\(5.89/(5.89+0.11)\\approx98.2\\%\\).</li> <li>Project onto PC1: scores = \\(Z = X_c v_1\\) (one scalar per point).</li> <li>Reconstruction with \\(k=1\\): \\(\\hat X = (v_1 v_1^T)X_c + \\mu\\).\\    Result: good 1D compression with tiny error (since \\(\\lambda_2\\) small).</li> </ol>"},{"location":"notes/lecture3/#pca-for-images-patch-pca","title":"PCA for images (patch PCA)","text":"<ul> <li>Patch size 12\u00d712 \u2192 144\u2011D.</li> <li>Compute PCs \u2192 basis filters (edges, blobs).</li> <li>Reconstruction error vs k typically decays fast; keep \\(k\\) around 20\u201360 for strong compression on patches.</li> </ul>"},{"location":"notes/lecture3/#3-latent-semantic-indexing-lsi-via-svd-text","title":"3) Latent Semantic Indexing (LSI) via SVD (text)","text":"<p>Given a term\u2013document matrix \\(X\\) (tf or tf\u2013idf). Compute rank\u2011\\(k\\) SVD: \\(X\\approx U_k\\Sigma_kV_k^T\\).</p> <ul> <li>Columns of \\(U_k\\) give term embeddings; columns of \\(V_k\\) give document embeddings; \\(\\Sigma_k\\) rescales.</li> <li>Similarity in this space captures latent topics even if exact words differ.</li> </ul>"},{"location":"notes/lecture3/#worked-example-mini-tdm-3-terms-3-docs","title":"\ud83d\udd22 Worked Example \u2013 Mini TDM (3 terms \u00d7 3 docs)","text":"\\[ X=\\begin{bmatrix} 1&amp;1&amp;0\\\\ 1&amp;0&amp;1\\\\ 0&amp;1&amp;1 \\end{bmatrix}. \\] <ul> <li>Full SVD gives \\(\\sigma\\approx(2,1,0)\\).</li> <li>Rank\u20112 LSI keeps first two singular values/vectors; cosine similarities between docs improve (synonymy effect).</li> </ul>"},{"location":"notes/lecture3/#4-matrix-factorization-mf","title":"4) Matrix Factorization (MF)","text":"<p>We model an (often incomplete) matrix \\(R\\in\\mathbb{R}^{n_u\\times n_i}\\) of user\u2013item interactions as</p> \\[ R \\approx U^T V + b\\,1^T + 1\\,\\tilde b^T, \\] <p>where \\(U\\in\\mathbb{R}^{r\\times n_u}\\) and \\(V\\in\\mathbb{R}^{r\\times n_i}\\) are latent factors, and \\(b,\\tilde b\\) biases.</p>"},{"location":"notes/lecture3/#objective-with-l2-regularization","title":"Objective (with L2 regularization)","text":"\\[ \\min_{U,V,b,\\tilde b} \\sum_{(i,j)\\in\\Omega} (r_{ij}-u_i^T v_j - b_i - \\tilde b_j)^2  + \\lambda(\\|U\\|_F^2+\\|V\\|_F^2 + \\|b\\|_2^2 + \\|\\tilde b\\|_2^2), \\] <p>where \\(\\Omega\\) is the set of observed entries.</p>"},{"location":"notes/lecture3/#algorithms","title":"Algorithms","text":"<ul> <li>ALS (Alternating Least Squares): fix \\(U\\), solve least squares for \\(V\\); swap. Scales well in Spark.</li> <li>SGD: update on each observed triple using gradients; supports online/streaming.</li> </ul> <pre><code>flowchart LR\n  R[Observed ratings R] --&gt; ALS{ALS Loop}\n  ALS --&gt;|fix U| SolveV[Solve for each v_j]\n  ALS --&gt;|fix V| SolveU[Solve for each u_i]\n  SolveU --&gt; ALS\n  SolveV --&gt; ALS\n  ALS --&gt; Pred[Predict r\u0302_ij = u_i^T v_j + b_i + b\u0303_j]</code></pre>"},{"location":"notes/lecture3/#worked-example-tiny-mf-with-als-rank-r2","title":"\ud83d\udd22 Worked Example \u2013 Tiny MF with ALS (rank r=2)","text":"<p>Observed ratings (\"?\" missing):</p> \\[ R=\\begin{array}{c|ccc}  &amp; i_1 &amp; i_2 &amp; i_3 \\\\\\hline u_1 &amp; 5 &amp; 3 &amp; ?\\\\ u_2 &amp; 4 &amp; ? &amp; 1\\\\ \\end{array} \\] <p>Initialize \\(U=\\begin{bmatrix}1&amp;0\\\\0&amp;1\\end{bmatrix}\\), \\(V=\\begin{bmatrix}1&amp;1&amp;1\\\\1&amp;1&amp;1\\end{bmatrix}\\). One ALS sweep (showing item 2):</p> <ul> <li>Users who rated \\(i_2\\): only \\(u_1\\) with rating 3. Solve \\(\\min_{v_2}\\ (3 - u_1^Tv_2)^2 + \\lambda\\|v_2\\|^2\\). With \\(u_1=[1,0]^T\\) and \\(\\lambda=0.1\\), closed form gives \\(v_2 = (U^{(2)T}U^{(2)}+\\lambda I)^{-1}U^{(2)T}r^{(2)} = (1.1I)^{-1}[3,0]^T = [2.727,0]^T\\). Repeat for other items and then solve for user vectors given items. After a few ALS rounds, predict missing: \\(\\hat r_{1,3}=u_1^Tv_3 + b_1 + \\tilde b_3\\) (biases often raise accuracy by 5\u201310%).</li> </ul>"},{"location":"notes/lecture3/#worked-example-onestep-sgd-update","title":"\ud83d\udd22 Worked Example \u2013 One\u2011step SGD update","text":"<p>Loss on observed \\((i,j)=(1,2)\\): \\(\\ell = (r_{12}-u_1^Tv_2)^2 + \\lambda(\\|u_1\\|^2+\\|v_2\\|^2)\\). Gradients: \\(\\nabla_{u_1}=-2(r_{12}-u_1^Tv_2)v_2 + 2\\lambda u_1\\), similarly for \\(v_2\\). Update with step \\(\\eta\\): \\(u_1 \\leftarrow u_1 - \\eta\\nabla_{u_1}\\), \\(v_2 \\leftarrow v_2 - \\eta\\nabla_{v_2}\\).</p>"},{"location":"notes/lecture3/#5-nonnegative-matrix-factorization-nmf","title":"5) Non\u2011negative Matrix Factorization (NMF)","text":"<p>We seek non\u2011negative factors \\(W\\in\\mathbb{R}_+^{m\\times r}\\), \\(H\\in\\mathbb{R}_+^{r\\times n}\\) such that \\(X \\approx WH.\\) This encourages parts\u2011based representations (e.g., eyes, nose, mouth for faces; topics for text).</p>"},{"location":"notes/lecture3/#multiplicative-updates-lee-seung","title":"Multiplicative updates (Lee &amp; Seung)","text":"<p>For objective \\(\\min_{W,H}\\ \\|X-WH\\|_F^2\\):</p> \\[ H \\leftarrow H \\odot \\frac{W^T X}{W^T W H},\\quad W \\leftarrow W \\odot \\frac{X H^T}{W H H^T}\\quad (\\odot: \\text{elementwise}). \\]"},{"location":"notes/lecture3/#worked-example-nmf-on-a-43-matrix-1-iteration","title":"\ud83d\udd22 Worked Example \u2013 NMF on a 4\u00d73 matrix (1 iteration)","text":"\\[ X = \\begin{bmatrix}4&amp;1&amp;0\\\\3&amp;0&amp;1\\\\0&amp;2&amp;3\\\\0&amp;1&amp;4\\end{bmatrix},\\quad r=2, \\ W=\\begin{bmatrix}1&amp;0.5\\\\1&amp;0.5\\\\0.5&amp;1\\\\0.5&amp;1\\end{bmatrix},\\ H=\\begin{bmatrix}1&amp;1&amp;1\\\\1&amp;1&amp;1\\end{bmatrix}. \\] <p>Compute updates (showing \\(H\\) numerator/denominator first column):</p> <ul> <li>\\(W^TX = \\begin{bmatrix}7&amp;2&amp;1\\\\4.5&amp;2&amp;4.5\\end{bmatrix}\\Rightarrow (W^TX)_{:,1}=[7,\\ 4.5]^T\\).</li> <li>\\(W^TWH = (W^TW)H\\) with \\(W^TW=\\begin{bmatrix}2.5&amp;2\\\\2&amp;2.5\\end{bmatrix}\\) gives first col \\([4.5,\\ 4.5]^T\\).</li> <li>New first column of \\(H\\): elementwise multiply by \\([7/4.5,\\ 4.5/4.5]=[1.556,\\ 1]\\). Repeat for other columns; then update \\(W\\) analogously. Observation: factors stay non\u2011negative and begin specializing columns/rows.</li> </ul>"},{"location":"notes/lecture3/#6-pca-vs-autoencoders-ae-encoderdecoder-view","title":"6) PCA vs Autoencoders (AE) \u2013 Encoder\u2013Decoder view","text":"<ul> <li>PCA: linear AE with encoder \\(U_k^T\\), decoder \\(U_k\\), MSE loss, orthogonal columns.</li> <li>AE: can be deep/nonlinear; objective can include sparsity, denoising, contrastive losses.</li> </ul> <pre><code>flowchart LR\n  X[\"X \u2208 R^{m\u00d7n}\"] --&gt; E[\"Encoder f\u03b8\"]\n  E --&gt; Z[\"z \u2208 R^{k\u00d7n}\"]\n  Z --&gt; D[\"Decoder g\u03c6\"]\n  D --&gt; Xhat[\"X\u0302\"]\n\n  X --- PCA[\"PCA\"]\n  PCA --- Xhat\n  X --- AE[\"Autoencoder\"]\n  AE --- Xhat\n\n  %% Style edge labels as separate nodes\n  PCA:::linear\n  AE:::nonlinear\n\n  classDef linear fill:#d9ead3,stroke:#333,stroke-width:1px;\n  classDef nonlinear fill:#fce5cd,stroke:#333,stroke-width:1px;\n</code></pre>"},{"location":"notes/lecture3/#7-practical-guidance-diagnostics","title":"7) Practical guidance &amp; diagnostics","text":"<ul> <li>Centering &amp; Scaling: PCA requires centering; consider standardizing features with different scales.</li> <li>Choosing k: Scree plot (\\(\\sigma_i\\)), cumulative EVR \u2265 0.90\u20130.99 for compression; cross\u2011validate for downstream task.</li> <li>Out\u2011of\u2011sample transforms: Store \\(\\mu\\) and \\(U_k\\) to project new data: \\(z=U_k^T(x-\\mu)\\).</li> <li>Cold start in MF: use content features or priors; biases stabilize.</li> <li>Regularization: \\(\\lambda\\) prevents overfitting in MF; shrink small PCs if noisy (Tikhonov).</li> <li>Sparsity: Prefer MF/NMF when matrices are highly sparse; SVD on dense TDM often uses truncated solvers (Lanczos).</li> </ul>"},{"location":"notes/lecture3/#8-extended-solved-miniproblems","title":"8) Extended solved mini\u2011problems","text":""},{"location":"notes/lecture3/#81-pca-by-hand-32-matrix","title":"8.1 PCA by hand (3\u00d72 matrix)","text":"\\[ X=\\begin{bmatrix}2&amp;0\\\\0&amp;2\\\\3&amp;1\\end{bmatrix},\\ \\mu=\\tfrac{1}{3}[5,\\ 1]^T,\\ X_c=X-\\mu 1^T. \\] <p>Compute SVD of \\(X_c\\) (algebra similar to the earlier 2\u00d72 SVD). Keep largest \\(\\sigma\\) only; reconstruct \\(\\hat X\\); compute error \\(\\|X-\\hat X\\|_F\\).</p>"},{"location":"notes/lecture3/#82-filling-a-missing-rating-with-mf-closed-form-no-bias","title":"8.2 Filling a missing rating with MF (closed form, no bias)","text":"<p>Given two users/two items with observed \\(r_{11}=5, r_{21}=4, r_{12}=3\\); estimate \\(r_{22}\\) with rank\u20111 MF and \\(\\lambda=0\\).\\ Solve \\(\\min_{u_1,u_2,v_1,v_2}\\sum (r_{ij}-u_iv_j)^2\\). Closed form (normal equations) gives \\(u_1=\\tfrac{5}{v_1}, u_2=\\tfrac{4}{v_1}, v_2=\\tfrac{3}{u_1}\\Rightarrow r_{22}=u_2v_2=\\tfrac{4}{v_1}\\cdot\\tfrac{3}{u_1}=\\tfrac{12}{5}\\approx2.4\\) (one of the valid minima). Adding \\(\\lambda&gt;0\\) stabilizes.</p>"},{"location":"notes/lecture3/#83-choosing-k-via-error-bound","title":"8.3 Choosing k via error bound","text":"<p>If singular values (descending) are \\((10, 3, 1, 0.2, \u2026)\\) and we keep \\(k=2\\), then</p> <ul> <li>spectral\u2011norm error = \\(\\sigma_{3}=1\\),</li> <li>relative Frobenius error \\(\\approx\\sqrt{1^2+0.2^2}/\\sqrt{10^2+3^2+1^2+0.2^2}\\approx\\tfrac{1.02}{10.53}\\approx9.7\\%\\).</li> </ul>"},{"location":"notes/lecture3/#84-nmf-topic-sketch-on-tdm-toy","title":"8.4 NMF topic sketch on TDM (toy)","text":"<p>Terms = {color, fabric, size}, Docs = {d1,d2,d3} with \\(X=\\begin{bmatrix}4&amp;1&amp;0\\\\3&amp;0&amp;1\\\\0&amp;2&amp;3\\end{bmatrix}\\). With \\(r=2\\), one topic leans on {color,fabric}, the other on {fabric,size}. Multiplicative updates separate them over iterations.</p>"},{"location":"notes/lecture3/#9-quick-reference-cheatsheet","title":"9) Quick reference (cheatsheet)","text":"<ul> <li>SVD: \\(X=U\\Sigma V^T\\); rank\u2011k truncation is optimal (EYM theorem).</li> <li>PCA: SVD of centered data; scores = \\(U_k^T X_c\\); reconstruction = \\(U_k U_k^T X_c + \\mu\\).</li> <li>MF (ALS): solve \\((U^{(j)T}U^{(j)}+\\lambda I)v_j=U^{(j)T}r^{(j)}\\); symmetric for \\(u_i\\).</li> <li>NMF: multiplicative updates; non\u2011negativity \u21d2 parts\u2011based, interpretable factors.</li> </ul>"},{"location":"notes/lecture3/#appendix-a-notation","title":"Appendix A \u2014 Notation","text":"<ul> <li>\\(m\\): features; \\(n\\): samples; \\(r\\): reduced rank.</li> <li>\\(\\|\\cdot\\|_F\\): Frobenius norm; \\(\\|\\cdot\\|_2\\): spectral norm.</li> <li>\\(1\\): all\u2011ones vector; \\(I\\): identity.</li> </ul>"},{"location":"notes/supervised/","title":"Supervised Learning","text":""}]}