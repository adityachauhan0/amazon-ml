{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome \ud83d\udc4b","text":"<p>This is my ML notes site.</p>"},{"location":"cheatsheets/","title":"Cheatsheets","text":""},{"location":"notes/lec5/","title":"Sequential Learning \u2014 Sequence Tagging with Hidden Markov Models","text":"<p>About these notes</p> <p>These MkDocs\u2011ready notes were produced from your slide snapshots. They include Mermaid diagrams and LaTeX. We can expand or rearrange sections any time.</p>"},{"location":"notes/lec5/#1-parts-of-speech-pos-tagging","title":"1. Parts of Speech (POS) Tagging","text":"<p>Goal: classify each token in a sentence into a syntactic category (e.g., noun, verb, adjective).</p> <ul> <li>Words are ambiguous and can take multiple POS tags depending on context.</li> <li>Example: the word play</li> <li>As verb: \u201cI like to play cricket.\u201d</li> <li>As noun: \u201cI would like to act in that play.\u201d</li> <li>Disambiguation requires sequence context.</li> </ul>"},{"location":"notes/lec5/#2-problem-formulation-sentence-tagging","title":"2. Problem Formulation (Sentence Tagging)","text":"<p>Given an input token sequence \\(x_{1:N} = (x_1, \\ldots, x_N)\\), predict a label sequence \\(y_{1:N} = (y_1, \\ldots, y_N)\\).</p> <p>Two broad modeling paradigms:</p> <ul> <li>Generative: model the joint \\(p_\\theta(x_{1:N}, y_{1:N})\\) and decode   \\(\\(\\hat{y}_{1:N} = \\arg\\max_{y_{1:N}} p_\\theta(x_{1:N}, y_{1:N}).\\)\\)</li> <li>Discriminative: model the conditional \\(p_\\phi(y_{1:N}\\mid x_{1:N})\\) (or a direct structured predictor) and decode   \\(\\(\\hat{y}_{1:N} = \\arg\\max_{y_{1:N}} p_\\phi(y_{1:N}\\mid x_{1:N}).\\)\\)</li> </ul> <p>Choosing a paradigm</p> <p>Generative models (like HMMs) are simple, often data\u2011efficient, and interpretable; discriminative models (CRF, BiLSTM\u2011CRF, Transformers) can capture richer features.</p>"},{"location":"notes/lec5/#3-generative-models","title":"3. Generative Models","text":"<p>Given training pairs \\((x^{(k)}, y^{(k)})\\) for \\(k = 1, \\dots, K\\), fit a probabilistic model for the joint \\(p(X,Y)\\). At inference, choose the label sequence maximizing the joint probability:</p> \\[\\hat{y} = \\arg\\max_{y} p(x, y).\\] <p>HMMs are the canonical generative model for sequence labeling.</p>"},{"location":"notes/lec5/#4-hidden-markov-models-hmms","title":"4. Hidden Markov Models (HMMs)","text":""},{"location":"notes/lec5/#41-graphical-view","title":"4.1 Graphical view","text":"<pre><code>flowchart LR\n    subgraph Hidden_States\n      y1((y\u2081)) --&gt; y2((y\u2082)) --&gt; y3((y\u2083)) --&gt; y4((\u2026))\n    end\n    x1([x\u2081]) --- y1\n    x2([x\u2082]) --- y2\n    x3([x\u2083]) --- y3\n    x4([x\u2084]) --- y4</code></pre> <ul> <li>Hidden states \\(y_t\\) are POS tags (e.g., det, noun, verb).</li> <li>Observations \\(x_t\\) are words (the, dog, barks).</li> <li>We assume:</li> <li>Markov property: \\(p(y_t\\mid y_{1:t-1}) = p(y_t\\mid y_{t-1})\\).</li> <li>Emission independence: \\(p(x_t\\mid x_{1:t-1}, y_{1:t}) = p(x_t\\mid y_t)\\).</li> </ul>"},{"location":"notes/lec5/#42-generative-story-toy-pos-tagger","title":"4.2 Generative story (toy POS tagger)","text":"<pre><code>flowchart LR\n    Start([Start]) --&gt; det((det))\n    Start --&gt; noun((noun))\n    Start --&gt; verb((verb))\n\n    det --&gt; det\n    det --&gt; noun\n    det --&gt; verb\n\n    noun --&gt; det\n    noun --&gt; noun\n    noun --&gt; verb\n\n    verb --&gt; det\n    verb --&gt; noun\n    verb --&gt; verb\n\n    det -.emit.-&gt; the([\"the\"]) \n    noun -.emit.-&gt; dog([\"dog\"]) \n    verb -.emit.-&gt; barks([\"barks\"]) \n\n    classDef default fill:none,stroke-width:1px</code></pre>"},{"location":"notes/lec5/#43-notation","title":"4.3 Notation","text":"<ul> <li>Vocabulary \\(\\mathcal{V}\\): set of all words.</li> <li>Tag set \\(\\mathcal{S}\\): e.g., \\(\\{\\text{det}, \\text{noun}, \\text{verb}, \\dots\\}\\).</li> <li>Sentence \\(X = x_{1:N}\\) with \\(x_i \\in \\mathcal{V}\\).</li> <li>Tag sequence \\(Y = y_{1:N}\\) with \\(y_i \\in \\mathcal{S}\\).</li> </ul>"},{"location":"notes/lec5/#44-parameters","title":"4.4 Parameters","text":"<p>HMM parameters \\(\\theta = (\\boldsymbol{\\pi}, \\mathbf{A}, \\mathbf{B})\\):</p> <ul> <li>Initial distribution \\(\\boldsymbol{\\pi}\\) with \\(\\pi_{s} = p(y_1 = s)\\).</li> <li>Transition matrix \\(\\mathbf{A}\\) with \\(a_{uv} = p(y_t = v \\mid y_{t-1} = u)\\).</li> <li>Emission probabilities \\(\\mathbf{B}\\) with \\(b_{s,w} = p(x_t = w \\mid y_t = s)\\).</li> </ul> <p>Example (toy numbers from slides):</p> \\(\\pi\\) value det 0.50 noun 0.40 verb 0.10 \\(\\mathbf{A}\\) det noun verb det 0.01 0.99 0.00 noun 0.30 0.30 0.40 verb 0.40 0.40 0.20 \\(\\mathbf{B}\\) the dog barks det 0.40 0.00 0.00 noun 0.00 0.015 0.0031 verb 0.00 0.0004 0.020 <p>Examples: \\(a_{\\text{det},\\text{noun}}=0.99\\), \\(a_{\\text{noun},\\text{verb}}=0.40\\), \\(b_{\\text{det},\\text{the}}=0.40\\), \\(b_{\\text{noun},\\text{barks}}=0.0031\\).</p>"},{"location":"notes/lec5/#45-likelihood-and-factorization","title":"4.5 Likelihood and factorization","text":"<p>For a first\u2011order HMM,</p> \\[ p_\\theta(x_{1:N}, y_{1:N}) = p(y_1)\\, p(x_1\\mid y_1) \\prod_{t=2}^N p(y_t\\mid y_{t-1})\\, p(x_t\\mid y_t). \\] <p>The marginal likelihood over observations is</p> \\[ p_\\theta(x_{1:N}) = \\sum_{y_{1:N}} p_\\theta(x_{1:N}, y_{1:N}). \\]"},{"location":"notes/lec5/#46-inference-tasks","title":"4.6 Inference tasks","text":"<ul> <li>Decoding (Viterbi): \\(\\hat{y}_{1:N} = \\arg\\max_{y_{1:N}} p_\\theta(y_{1:N}\\mid x_{1:N})\\).   Recurrence for the best path score \\(\\delta_t(s)\\) and backpointers \\(\\psi_t(s)\\):   $$   \\begin{aligned}   \\delta_1(s) &amp;= \\log \\pi_s + \\log b_{s,x_1},\\   \\delta_t(s) &amp;= \\max_{u}\\; \\delta_{t-1}(u) + \\log a_{u,s} + \\log b_{s,x_t}.    \\end{aligned}   $$</li> <li>Sequence likelihood (Forward): \\(\\alpha_t(s)\\)   $$   \\alpha_1(s) = \\pi_s\\, b_{s,x_1},\\qquad   \\alpha_t(s) = b_{s,x_t} \\sum_{u} \\alpha_{t-1}(u) a_{u,s}.   $$</li> <li>Posterior decoding (Forward\u2011Backward): \\(\\gamma_t(s) = p(y_t=s\\mid x_{1:N})\\) using \\(\\alpha\\) and \\(\\beta\\).</li> </ul>"},{"location":"notes/lec5/#47-learning-brief","title":"4.7 Learning (brief)","text":"<p>With labeled data, estimate \\(\\pi, A, B\\) by count normalization (MLE) with smoothing (e.g., add\u2011\\(\\lambda\\)).</p> <p>With unlabeled data, use EM/Baum\u2013Welch:</p> <ul> <li>E\u2011step: compute expected counts via forward\u2011backward (\\(\\gamma, \\xi\\)).</li> <li>M\u2011step: renormalize to update \\(\\pi, A, B\\).</li> </ul> <p>Smoothing &amp; OOV</p> <p>Emission smoothing and an UNK token are essential for robust tagging.</p>"},{"location":"notes/lec5/#5-worked-toy-example","title":"5. Worked toy example","text":"<p>Sentence: \u201cthe dog barks\u201d.</p> <p>We illustrate the decoding objective:</p> \\[ \\hat{y}_{1:3} = \\arg\\max_{y_1,y_2,y_3} \\; p(y_1)\\,p(x_1\\mid y_1)\\,p(y_2\\mid y_1)\\,p(x_2\\mid y_2)\\,p(y_3\\mid y_2)\\,p(x_3\\mid y_3). \\] <pre><code>flowchart LR\n    y1((y\u2081)) --&gt; y2((y\u2082)) --&gt; y3((y\u2083))\n    y1 -.-&gt; x1([the])\n    y2 -.-&gt; x2([dog])\n    y3 -.-&gt; x3([barks])\n    classDef default fill:none,stroke-width:1px</code></pre> <p>A plausible best path is det \u2192 noun \u2192 verb, matching the POS intuition.</p>"},{"location":"notes/lec5/#6-summary-next-steps","title":"6. Summary &amp; Next Steps","text":"<ul> <li>Sequence tagging maps \\(x_{1:N} \\to y_{1:N}\\); context resolves ambiguity.</li> <li>HMMs provide a clean generative baseline with efficient dynamic programming for likelihood and decoding.</li> <li>Next, we can add: CRFs, BiLSTM\u2011CRF, transformer\u2011based taggers, and comparisons (accuracy, data efficiency).</li> </ul>"},{"location":"notes/lec5/#appendix-quick-reference","title":"Appendix: Quick reference","text":"<p>Key equations</p> <ul> <li>Joint factorization for HMM:   \\(\\(p(x_{1:N},y_{1:N}) = p(y_1) p(x_1\\mid y_1) \\prod_{t=2}^N p(y_t\\mid y_{t-1}) p(x_t\\mid y_t).\\)\\)</li> <li>Marginal likelihood:   \\(\\(p(x_{1:N}) = \\sum_{y_{1:N}} p(x_{1:N},y_{1:N}).\\)\\)</li> <li>Viterbi recurrence:   \\(\\(\\delta_t(s) = \\max_u \\{\\delta_{t-1}(u) + \\log a_{u,s}\\} + \\log b_{s,x_t}.\\)\\)</li> <li>Forward recurrence:   \\(\\(\\alpha_t(s) = b_{s,x_t} \\sum_u \\alpha_{t-1}(u) a_{u,s}.\\)\\)</li> </ul> <p>Glossary</p> <ul> <li>\\(\\mathcal{V}\\): vocabulary of tokens.</li> <li>\\(\\mathcal{S}\\): set of tags/states.</li> <li>\\(\\pi\\): initial distribution over tags.</li> <li>\\(A\\): transition matrix.</li> <li>\\(B\\): emission probabilities.</li> </ul>"},{"location":"notes/lec5/#hidden-markov-models-ultradetailed-addendum-mkdocs-mermaid-latex","title":"Hidden Markov Models \u2014 Ultra\u2011Detailed Addendum (MkDocs + Mermaid + LaTeX)","text":"<p>Scope</p> <p>This addendum expands your slides on HMM core tasks, Viterbi inference, Forward/Backward, likelihood, and training. It is MkDocs\u2011ready (Material admonitions, Mermaid diagrams, LaTeX math). Fully modular so we can grow it chapter by chapter.</p>"},{"location":"notes/lec5/#1-hmm-three-problems-to-solve","title":"1. HMM: Three Problems to Solve","text":"<p>Working with HMMs generally involves three canonical problems:</p> <ol> <li>Inference / Decoding    Given observations \\(x_{1:T}\\) and HMM parameters \\((\\pi, A, B)\\), return the most probable hidden state sequence \\(\\hat{y}_{1:T}\\).</li> <li>Likelihood    Compute the overall likelihood of the observation sequence: \\(\\(p_\\theta(x_{1:T}) = \\sum_{y_{1:T}} p_\\theta(x_{1:T}, y_{1:T}).\\)\\)</li> <li>Training (Learning)    Given data (labeled or unlabeled), learn \\(\\theta=(\\pi, A, B)\\) that best explains the data.</li> </ol> <p>Notation mapping</p> <p>Slide convention: \\(N=\\text{#states}\\), \\(T=\\text{sequence length}\\). Elsewhere in these notes we sometimes write \\(|\\mathcal{S}|\\) for #states.</p>"},{"location":"notes/lec5/#2-lattice-view-paths-and-complexity","title":"2. Lattice View, Paths, and Complexity","text":"<p>For a length\u2011\\(T\\) sentence and \\(N\\) tags/states, the state lattice has \\(T\\) columns and \\(N\\) nodes per column; edges represent transitions.</p> <pre><code>flowchart LR\n  subgraph t1[ t=1 ]\n    d1((det))\n    n1((noun))\n    v1((verb))\n  end\n  subgraph t2[ t=2 ]\n    d2((det))\n    n2((noun))\n    v2((verb))\n  end\n  subgraph t3[ t=3 ]\n    d3((det))\n    n3((noun))\n    v3((verb))\n  end\n  d1--&gt;d2; d1--&gt;n2; d1--&gt;v2\n  n1--&gt;d2; n1--&gt;n2; n1--&gt;v2\n  v1--&gt;d2; v1--&gt;n2; v1--&gt;v2\n  d2--&gt;d3; d2--&gt;n3; d2--&gt;v3\n  n2--&gt;d3; n2--&gt;n3; n2--&gt;v3\n  v2--&gt;d3; v2--&gt;n3; v2--&gt;v3</code></pre> <ul> <li>#Sequences: \\(N^T\\) (e.g., 3 tags over 3 tokens \u21d2 \\(3^3=27\\) sequences).  </li> <li>Dynamic programming avoids enumeration: inference/likelihood in \\(\\mathcal{O}(TN^2)\\).</li> </ul>"},{"location":"notes/lec5/#3-viterbi-algorithm-map-path","title":"3. Viterbi Algorithm \u2014 MAP Path","text":"<p>We seek \\(\\hat{y}_{1:T} = \\arg\\max_{y_{1:T}} p(y_{1:T}\\mid x_{1:T})\\); since \\(p(x_{1:T})\\) is constant wrt \\(y_{1:T}\\), equivalently maximize the joint \\(p(y_{1:T},x_{1:T})\\).</p>"},{"location":"notes/lec5/#31-recurrence-probability-domain","title":"3.1 Recurrence (probability domain)","text":"<p>Let \\(\\delta_t(s)\\) be the best path probability ending at state \\(s\\) at time \\(t\\), and \\(\\psi_t(s)\\) the backpointer:</p> \\[ \\begin{aligned} \\delta_1(s) &amp;= \\pi_s\\, b_{s,x_1},\\\\ \\delta_t(s) &amp;= b_{s,x_t}\\, \\max_{u\\in\\mathcal{S}} \\delta_{t-1}(u)\\, a_{u,s},\\\\ \\psi_t(s) &amp;= \\arg\\max_{u\\in\\mathcal{S}} \\delta_{t-1}(u)\\, a_{u,s}. \\end{aligned} \\] <p>Termination: \\(\\hat{y}_T = \\arg\\max_s \\delta_T(s)\\) Backtrace: \\(\\hat{y}_{t-1} = \\psi_t(\\hat{y}_t)\\).</p>"},{"location":"notes/lec5/#32-logdomain-numerically-stable","title":"3.2 Log\u2011domain (numerically stable)","text":"<p>Define \\(\\Delta_t(s) = \\log \\delta_t(s)\\):</p> \\[ \\Delta_1(s)=\\log\\pi_s+\\log b_{s,x_1},\\qquad \\Delta_t(s)=\\log b_{s,x_t}+\\max_u\\{\\Delta_{t-1}(u)+\\log a_{u,s}\\}. \\]"},{"location":"notes/lec5/#33-worked-example-pos-the-dog-barks","title":"3.3 Worked example (POS: the dog barks)","text":"<p>Parameters from your slide tables (\\(\\pi, A, B\\) over tags det/noun/verb and words the/dog/barks):</p> <ul> <li>Init (t=1, \"the\") \\(\\delta_1(\\text{det})=0.5\\cdot0.40=0.20\\), \\(\\delta_1(\\text{noun})=0\\), \\(\\delta_1(\\text{verb})=0\\).</li> <li>t=2 (\"dog\") \\(\\delta_2(\\text{noun})=0.015\\cdot\\max\\{0.20\\cdot0.99,0,0\\}=\\mathbf{0.00297}\\); others \\(\\approx0\\).   Backpointer: \\(\\psi_2(\\text{noun})=\\text{det}\\).</li> <li>t=3 (\"barks\") \\(\\delta_3(\\text{verb})=0.020\\cdot\\max\\{0,\\;0.00297\\cdot0.40,\\;0\\}=\\mathbf{2.376\\times10^{-5}}\\); \\(\\delta_3(\\text{noun})=0.0031\\cdot\\max\\{0,\\;0.00297\\cdot0.30,\\;0\\}=2.7621\\times10^{-6}\\).</li> </ul> <p>Decoded path: det \u2192 noun \u2192 verb.</p> <pre><code>flowchart LR\n  det1((det)) --&gt; noun2((noun)) --&gt; verb3((verb))\n  the([the]) -.-&gt; det1\n  dog([dog]) -.-&gt; noun2\n  barks([barks]) -.-&gt; verb3\n  classDef default fill:none,stroke-width:1px</code></pre> <p>Viterbi vs. marginal argmax</p> <p>Viterbi gives the single best sequence. Tagging each position with \\(\\arg\\max_s p(y_t=s\\mid x_{1:T})\\) can yield a different (and suboptimal) global path.</p>"},{"location":"notes/lec5/#4-likelihood-via-the-forward-algorithm","title":"4. Likelihood via the Forward Algorithm","text":"<p>We want \\(p_\\theta(x_{1:T})\\) without enumerating \\(N^T\\) sequences.</p>"},{"location":"notes/lec5/#41-forward-recurrences","title":"4.1 Forward recurrences","text":"<p>Let \\(\\alpha_t(s) = p(x_{1:t}, y_t=s)\\):</p> \\[ \\alpha_1(s)=\\pi_s\\,b_{s,x_1},\\qquad \\alpha_t(s)=b_{s,x_t}\\sum_{u}\\alpha_{t-1}(u) a_{u,s}. \\] <p>Likelihood: \\(p_\\theta(x_{1:T}) = \\sum_s \\alpha_T(s)\\).</p>"},{"location":"notes/lec5/#42-complexity","title":"4.2 Complexity","text":"<ul> <li>Na\u00efve: \\(\\mathcal{O}(N^T)\\) (impractical).  </li> <li>Forward DP: \\(\\mathcal{O}(TN^2)\\) time; memory \\(\\mathcal{O}(TN)\\) (or \\(\\mathcal{O}(N)\\) streaming if no backpointers are needed).</li> </ul>"},{"location":"notes/lec5/#43-numeric-check-same-toy-example","title":"4.3 Numeric check (same toy example)","text":"<p>\\(\\alpha_1=(0.20,0,0)\\); \\(\\alpha_2(\\text{noun})=0.00297\\). \\(\\alpha_3(\\text{verb})=2.376\\times10^{-5}\\); \\(\\alpha_3(\\text{noun})=2.7621\\times10^{-6}\\); hence \\(\\boxed{p(x)=2.652\\times10^{-5}}\\) and \\(\\log p(x)\\approx-10.538\\).</p>"},{"location":"notes/lec5/#44-scaling-stability","title":"4.4 Scaling / stability","text":"<ul> <li>Scaled forward: define \\(c_t = \\big(\\sum_s \\tilde{\\alpha}_t(s)\\big)^{-1}\\) and set \\(\\alpha_t(s)=c_t\\tilde{\\alpha}_t(s)\\) so \\(\\sum_s \\alpha_t(s)=1\\). Then \\(\\log p(x_{1:T})=-\\sum_{t=1}^T \\log c_t\\).</li> <li>Log\u2011space: maintain \\(\\log\\alpha_t(s)\\) using <code>logsumexp</code>.</li> </ul>"},{"location":"notes/lec5/#5-backward-algorithm-and-posteriors","title":"5. Backward Algorithm and Posteriors","text":"<p>Define \\(\\beta_t(s)=p(x_{t+1:T}\\mid y_t=s)\\) with</p> \\[ \\beta_T(s)=1,\\qquad \\beta_t(s)=\\sum_{v} a_{s,v}\\, b_{v,x_{t+1}}\\, \\beta_{t+1}(v). \\] <p>Posterior quantities:</p> \\[ \\gamma_t(s)=\\frac{\\alpha_t(s)\\,\\beta_t(s)}{\\sum_j \\alpha_t(j)\\,\\beta_t(j)},\\qquad \\xi_t(u,v)=\\frac{\\alpha_t(u)\\,a_{u,v}\\,b_{v,x_{t+1}}\\,\\beta_{t+1}(v)}{\\sum_{i,j} \\alpha_t(i)\\,a_{i,j}\\,b_{j,x_{t+1}}\\,\\beta_{t+1}(j)}. \\] <p>These enable posterior decoding and Baum\u2013Welch (EM).</p>"},{"location":"notes/lec5/#6-training-supervised-em","title":"6. Training (Supervised &amp; EM)","text":""},{"location":"notes/lec5/#61-supervised-mle-labeled-sequences","title":"6.1 Supervised MLE (labeled sequences)","text":"<p>Counts \u2192 normalized probabilities (with smoothing):</p> \\[ \\hat{\\pi}_s = \\operatorname{norm}_1\\big(\\text{count}(y_1=s)\\big),\\quad \\hat{a}_{u,v} = \\operatorname{norm}_1\\big(\\text{count}(y_{t-1}=u, y_t=v)\\big),\\quad \\hat{b}_{s,w} = \\operatorname{norm}_1\\big(\\text{count}(y_t=s, x_t=w)\\big). \\]"},{"location":"notes/lec5/#62-unlabeled-data-baumwelch-em","title":"6.2 Unlabeled data \u2014 Baum\u2013Welch (EM)","text":"<ul> <li>E\u2011step: compute expected counts with forward\u2013backward: \\(\\bar{\\pi}_s\\leftarrow\\gamma_1(s)\\), \\(\\bar{a}_{u,v}\\leftarrow\\sum_{t=1}^{T-1}\\xi_t(u,v)\\), \\(\\bar{b}_{s,w}\\leftarrow\\sum_{t=1}^{T}\\mathbb{1}[x_t=w]\\,\\gamma_t(s)\\).</li> <li>M\u2011step: renormalize rows/PMFs to update \\(\\pi, A, B\\) (plus add\u2011\\(\\lambda\\) smoothing).</li> </ul> <p>Smoothing &amp; OOV</p> <p>Use add\u2011\\(\\lambda\\) smoothing and an UNK token (or character/feature\u2011based emissions) for robustness.</p>"},{"location":"notes/lec5/#7-pseudocode-copypaste-ready","title":"7. Pseudocode (copy\u2011paste ready)","text":"<pre><code>Viterbi(x[1..T], \u03c0, A, B):\n  for s in S: \u03b4[1,s] = \u03c0[s]*B[s,x1]; \u03c8[1,s]=NULL\n  for t=2..T:\n    for s in S:\n      (best, arg)=max_u (\u03b4[t-1,u]*A[u,s])\n      \u03b4[t,s] = B[s,xt]*best\n      \u03c8[t,s] = arg\n  y\u0302_T = argmax_s \u03b4[T,s]\n  for t=T..2: y\u0302_{t-1} = \u03c8[t, y\u0302_t]\n  return y\u0302\n\nForward(x[1..T], \u03c0, A, B):\n  for s in S: \u03b1[1,s] = \u03c0[s]*B[s,x1]\n  for t=2..T:\n    for s in S: \u03b1[t,s] = B[s,xt] * \u03a3_u \u03b1[t-1,u]*A[u,s]\n  return \u03a3_s \u03b1[T,s]\n\nBackward(x[1..T], \u03c0, A, B):\n  for s in S: \u03b2[T,s] = 1\n  for t=T-1..1:\n    for s in S: \u03b2[t,s] = \u03a3_v A[s,v]*B[v,x_{t+1}]*\u03b2[t+1,v]\n</code></pre>"},{"location":"notes/lec5/#8-implementation-notes-edge-cases","title":"8. Implementation Notes &amp; Edge Cases","text":"<ul> <li>Start/End states: optionally include explicit START/END; easiest is to fold START into \\(\\pi\\) and END into a final termination step.</li> <li>Forbidden transitions: set \\(a_{u,v}=0\\) (or \\(-\\infty\\) in log\u2011space); Viterbi will ignore them.</li> <li>Beam search: prune states with low partial scores to speed decoding at a small accuracy cost.</li> <li>Higher\u2011order HMMs: 2nd\u2011order uses \\(p(y_t\\mid y_{t-1},y_{t-2})\\); cost \\(\\mathcal{O}(TN^3)\\).</li> <li>HSMMs (semi\u2011Markov): model explicit durations; useful for long spans.</li> <li>Evaluation: token accuracy, sentence accuracy, per\u2011tag precision/recall, confusion matrices. Compare to trivial baselines (majority tag, suffix memorization).</li> </ul>"},{"location":"notes/lec5/#9-quick-reference","title":"9. Quick Reference","text":"<ul> <li>Joint factorization \\(\\(p(x_{1:T},y_{1:T}) = p(y_1) p(x_1\\mid y_1) \\prod_{t=2}^{T} p(y_t\\mid y_{t-1}) p(x_t\\mid y_t).\\)\\)</li> <li>Likelihood \\(\\(p(x_{1:T}) = \\sum_{y_{1:T}} p(x_{1:T},y_{1:T}).\\)\\)</li> <li>Viterbi \\(\\(\\delta_t(s) = b_{s,x_t} \\max_u \\delta_{t-1}(u) a_{u,s},\\quad \\psi_t(s)=\\arg\\max_u \\delta_{t-1}(u) a_{u,s}.\\)\\)</li> <li>Forward \\(\\(\\alpha_t(s) = b_{s,x_t} \\sum_u \\alpha_{t-1}(u) a_{u,s}.\\)\\)</li> <li>Backward \\(\\(\\beta_t(s) = \\sum_v a_{s,v} b_{v,x_{t+1}} \\beta_{t+1}(v).\\)\\)</li> <li>Posteriors \\(\\(\\gamma_t(s) = \\frac{\\alpha_t(s)\\beta_t(s)}{\\sum_j \\alpha_t(j)\\beta_t(j)},\\quad \\xi_t(u,v) = \\frac{\\alpha_t(u)a_{u,v}b_{v,x_{t+1}}\\beta_{t+1}(v)}{\\sum_{i,j} \\alpha_t(i)a_{i,j}b_{j,x_{t+1}}\\beta_{t+1}(j)}.\\)\\)</li> </ul>"},{"location":"notes/lec5/#hidden-markov-models-hmm-em-posteriors-applications-in-speech","title":"Hidden Markov Models (HMM) \u2014 EM Posteriors &amp; Applications in Speech","text":"<p>This document explains the Expectation\u2013Maximization (EM, Baum\u2013Welch) training procedure for HMMs, with posterior definitions, update rules, and applications in speech recognition (ASR). Includes multiple Mermaid diagrams for structure and intuition.</p>"},{"location":"notes/lec5/#1-objective-1-initial-distribution","title":"1. Objective 1 \u2014 Initial Distribution","text":"<p>The initial state probabilities \\(\\pi_s\\) are updated using the posterior probability of being in state \\(s\\) at time \\(t=1\\):</p> \\[ \\pi_s \\leftarrow \\gamma_1(s), \\qquad \\sum_s \\pi_s = 1. \\] <ul> <li>Intuition: the best estimate of how often each state is used to start a sequence.  </li> <li>Normalized automatically since \\(\\sum_s \\gamma_1(s)=1\\).</li> </ul>"},{"location":"notes/lec5/#2-objective-2-transition-matrix-a","title":"2. Objective 2 \u2014 Transition Matrix \\(A\\)","text":"<p>We need the posterior of two adjacent states at time \\(t-1\\) and \\(t\\):</p> \\[ p(y_{t-1}=s_i, y_t=s_m \\mid x_{1:T}) \\propto  \\alpha_{t-1}(s_i)\\, a_{s_i,s_m}\\, b_{s_m,x_t}\\, \\beta_t(s_m). \\] <p>Normalizing:</p> \\[ \\xi_t(s_i,s_m) = \\frac{\\alpha_{t-1}(s_i)\\, a_{s_i,s_m}\\, b_{s_m,x_t}\\, \\beta_t(s_m)}{\\sum_{u,v}\\alpha_{t-1}(u)\\, a_{u,v}\\, b_{v,x_t}\\, \\beta_t(v)}. \\] <p>Transition update:</p> \\[ a_{s_i,s_m} \\leftarrow \\frac{\\sum_{t=2}^T \\xi_t(s_i,s_m)}{\\sum_{t=2}^T \\sum_v \\xi_t(s_i,v)}. \\] <ul> <li>Numerator = expected number of transitions \\(s_i \\to s_m\\) </li> <li>Denominator = expected number of transitions leaving \\(s_i\\) </li> <li>Each row of \\(A\\) is renormalized to sum to 1.</li> </ul>"},{"location":"notes/lec5/#3-objective-3-emission-matrix-b","title":"3. Objective 3 \u2014 Emission Matrix \\(B\\)","text":"<p>The state occupancy posterior is:</p> \\[ \\gamma_t(s) = p(y_t=s \\mid x_{1:T}) = \\frac{\\alpha_t(s)\\, \\beta_t(s)}{\\sum_j \\alpha_t(j)\\, \\beta_t(j)}. \\] <p>Emission update (discrete symbols):</p> \\[ b_{s,v_k} \\leftarrow  \\frac{\\sum_{t=1}^T \\mathbf{1}[x_t=v_k]\\, \\gamma_t(s)}{\\sum_{t=1}^T \\gamma_t(s)}. \\] <ul> <li>Numerator = expected number of times in state \\(s\\) emitting symbol \\(v_k\\) </li> <li>Denominator = expected number of times in state \\(s\\) </li> </ul> <p>For continuous features (speech), \\(b_s(x)\\) is parameterized as a PDF (e.g., Gaussian or GMM). Updates use weighted maximum-likelihood with \\(\\gamma_t(s)\\) as responsibilities.</p>"},{"location":"notes/lec5/#4-em-objective-function","title":"4. EM Objective Function","text":"<p>The EM auxiliary function:</p> \\[ Q(\\theta,\\theta_i) = \\mathbb{E}_{y_{1:T}\\sim p(\\cdot\\mid x_{1:T},\\theta_i)}\\Big[ \\log p_\\theta(y_1) + \\sum_{t=2}^T \\log p_\\theta(y_t \\mid y_{t-1}) + \\sum_{t=1}^T \\log p_\\theta(x_t \\mid y_t)\\Big]. \\] <p>This decomposes naturally into: - Initial distribution (\\(\\pi\\)) - Transition matrix (\\(A\\)) - Emission distributions (\\(B\\))</p> <p>Each has a closed-form update (the ones above).</p>"},{"location":"notes/lec5/#5-applications-in-speech-recognition-asr","title":"5. Applications in Speech Recognition (ASR)","text":"<p>The decoding objective:</p> \\[ W^* = \\arg\\max_W\\; p(X \\mid W)\\, p(W), \\] <p>where \\(X = x_{1:T}\\) is the feature sequence (MFCC / FBANK).</p> <ul> <li>Acoustic model (HMM):   States correspond to phones or sub-phones, with self-loops \\(a_{s,s}\\) for duration modeling and forward transitions \\(a_{s,s'}\\) for progression.  </li> <li>Emissions: \\(b_s(x)\\) are PDFs of feature vectors.  </li> <li>Classical: GMM-HMM </li> <li>Modern hybrid: DNN outputs as state posteriors, combined with HMM structure.  </li> <li>Language model (LM): \\(p(W)\\) encodes word sequence probability.  </li> <li>Decoding: Viterbi / beam search over HMM graph + lexicon + LM.</li> </ul>"},{"location":"notes/lec5/#6-mermaid-diagrams","title":"6. Mermaid Diagrams","text":""},{"location":"notes/lec5/#61-speech-recognition-pipeline","title":"6.1 Speech recognition pipeline","text":"<pre><code>flowchart LR\n  Audio[\"Audio waveform\"]\n  Feats[\"Feature extraction (MFCC / FBANK)\"]\n  HMM[\"Acoustic HMM (states, a(u,v))\"]\n  Decode[\"Viterbi / Beam search\"]\n  LM[\"Language model p(W)\"]\n  Words[\"Best hypothesis W*\"]\n\n  Audio --&gt; Feats --&gt; HMM --&gt; Decode --&gt; Words\n  LM --&gt; Decode</code></pre>"},{"location":"notes/lec5/#62-hmm-topology-one-phone-left-to-right-with-self-loops","title":"6.2 HMM topology (one phone, left-to-right with self-loops)","text":"<pre><code>flowchart LR\n  Start([Start]) --&gt; S1\n\n  subgraph Phone_HMM\n    direction LR\n    S1[\"State s1\"] --&gt;|forward| S2[\"State s2\"]\n    S2 --&gt;|forward| S3[\"State s3\"]\n    S1 -- \"self loop a(s1,s1)\" --&gt; S1\n    S2 -- \"self loop a(s2,s2)\" --&gt; S2\n    S3 -- \"self loop a(s3,s3)\" --&gt; S3\n  end\n\n  S3 --&gt; End([End])\n</code></pre>"},{"location":"notes/lec5/#63-forwardbackward-message-passing-per-time-step","title":"6.3 Forward\u2013Backward message passing (per time step)","text":"<pre><code>flowchart LR\n  X1[\"x1\"] --&gt; T1[\"Time 1\"]\n  X2[\"x2\"] --&gt; T2[\"Time 2\"]\n  X3[\"x3\"] --&gt; T3[\"Time 3\"]\n  X4[\"x4\"] --&gt; T4[\"Time 4\"]\n\n  T1 --&gt;|alpha forward| T2 --&gt;|alpha| T3 --&gt;|alpha| T4\n  T4 -.-&gt;|beta backward| T3 -.-&gt;|beta| T2 -.-&gt;|beta| T1</code></pre>"},{"location":"notes/lec5/#64-em-dependency-map-what-feeds-what","title":"6.4 EM dependency map (what feeds what)","text":"<pre><code>flowchart TB\n  X[\"Observations x(1:T)\"] --&gt; Alpha[\"Compute alpha\"]\n  X --&gt; Beta[\"Compute beta\"]\n  Alpha --&gt; Gamma[\"State posteriors gamma(t,s)\"]\n  Beta  --&gt; Gamma\n  Alpha --&gt; Xi[\"Transition posteriors xi(t,i,m)\"]\n  Beta  --&gt; Xi\n\n  Gamma --&gt; Pi[\"Update initial pi\"]\n  Gamma --&gt; B[\"Update emissions B\"]\n  Xi --&gt; A[\"Update transitions A\"]</code></pre>"},{"location":"notes/lec5/#7-quick-reference","title":"7. Quick Reference","text":"<ul> <li> <p>Forward probability: \\(\\alpha_t(s)\\)</p> </li> <li> <p>Backward probability: \\(\\beta_t(s)\\)</p> </li> <li> <p>Two-state posterior: \\(\\xi_t(s_i,s_m)\\)</p> </li> <li> <p>Single-state posterior: \\(\\gamma_t(s)\\)</p> </li> <li> <p>Updates:</p> <ul> <li> <p>\\(\\pi_s \\leftarrow \\gamma_1(s)\\)</p> </li> <li> <p>\\(a_{s_i,s_m} \\leftarrow \\dfrac{\\sum_t \\xi_t(s_i,s_m)}{\\sum_t \\sum_v \\xi_t(s_i,v)}\\)</p> </li> <li> <p>\\(b_{s,v_k} \\leftarrow \\dfrac{\\sum_t \\mathbf{1}[x_t=v_k] \\gamma_t(s)}{\\sum_t \\gamma_t(s)}\\)</p> </li> </ul> </li> <li> <p>Constraints: Row-stochastic normalization, clamp small negatives to \\(0\\).</p> </li> <li> <p>Speech link: HMM + Emission PDFs + LM \u2192 decoding.</p> </li> </ul>"},{"location":"notes/lecture3/","title":"Dimensionality Reduction","text":"<p>Dimensionality reduction is the process of reducing the number of features (dimensions) in a dataset while preserving the most informative and discriminative aspects.</p> <p>It is one of the most critical techniques in machine learning and data science, because:</p> <ul> <li>Models often perform poorly when overwhelmed with too many features (curse of dimensionality).</li> <li>Redundant and irrelevant features introduce noise.</li> <li>High-dimensional spaces are sparse, which makes clustering and classification difficult.</li> <li>Reduced dimensionality leads to better generalization, interpretability, visualization, and faster computation.</li> </ul>"},{"location":"notes/lecture3/#1-the-usual-supervised-learning-approach","title":"1. The Usual Supervised Learning Approach","text":"<pre><code>flowchart LR\n    subgraph Training\n        A[Data: Features] --&gt; M[Learning Algorithm]\n        L[Labels] --&gt; M\n        M --&gt; B[Model]\n    end\n\n    subgraph Testing\n        T[Test Data] --&gt; B\n        B --&gt; P[Predicted Label]\n    end</code></pre> <ul> <li>A dataset consists of features (X) and labels (Y).</li> <li>A supervised learning algorithm learns a mapping X \u2192 Y.</li> <li>With high-dimensional data:</li> <li>The algorithm becomes overwhelmed.</li> <li>Training time is long.</li> <li>The risk of overfitting increases due to sparse data.</li> </ul>"},{"location":"notes/lecture3/#2-feature-types","title":"2. Feature Types","text":"<p>Not all features contribute equally to prediction.</p> <ul> <li> <p>Relevant Features</p> </li> <li> <p>Provide useful predictive information.</p> </li> <li> <p>Example: Diaper, Stroller, Bassinet, Cradle for predicting baby products.</p> </li> <li> <p>Irrelevant Features</p> </li> <li> <p>Provide no predictive signal.</p> </li> <li> <p>Example: Random identifiers.</p> </li> <li> <p>Redundant Features</p> </li> <li> <p>Highly correlated with other features, duplicating information.</p> </li> <li>Example: Stroller and wheels often co-occur.</li> </ul> <pre><code>mindmap\n  root((Features))\n    Relevant\n      \"Strong signal for prediction\"\n    Irrelevant\n      \"No contribution\"\n    Redundant\n      \"Overlaps with others\"</code></pre>"},{"location":"notes/lecture3/#3-approaches-to-dimensionality-reduction","title":"3. Approaches to Dimensionality Reduction","text":"<p>Two broad strategies:</p>"},{"location":"notes/lecture3/#31-feature-selection-downsizing-existing-features","title":"3.1 Feature Selection (downsizing existing features)","text":"<ul> <li>Removes noisy, irrelevant, or redundant features.</li> <li>Keeps original features intact.</li> <li>Preserves interpretability.</li> <li>Useful when:</li> <li>Budget constraints exist (e.g., costly medical tests).</li> <li>Small number of features is required for human interpretability.</li> </ul>"},{"location":"notes/lecture3/#32-low-dimensional-feature-learning-new-derived-features","title":"3.2 Low-Dimensional Feature Learning (new derived features)","text":"<ul> <li>Learns a new representation of the data.</li> <li>May lose interpretability but increases performance.</li> <li>Examples:</li> <li>Linear methods: PCA, MDS</li> <li>Non-linear methods: Kernel PCA, t-SNE</li> <li>Representation learning: Neural embeddings</li> </ul> <pre><code>graph TD\n    X[High-Dimensional Data] --&gt;|Feature Selection| S[Reduced Subset of Features]\n    X --&gt;|Feature Learning| F[New Derived Features]</code></pre>"},{"location":"notes/lecture3/#4-feature-selection-techniques","title":"4. Feature Selection Techniques","text":"Method Idea Pros Cons Examples Wrapper Use ML models to search best subset Captures feature interactions Computationally expensive Sequential Forward Selection, Backward Elimination Filter Rank features by statistical score Fast, scalable Ignores interactions Pearson Correlation, Chi-squared, Mutual Information Embedded Feature selection during training Efficient, task-specific Model-dependent Lasso Regression, Decision Trees"},{"location":"notes/lecture3/#5-wrapper-methods","title":"5. Wrapper Methods","text":""},{"location":"notes/lecture3/#51-exhaustive-search","title":"5.1 Exhaustive Search","text":"<ul> <li>\\(N\\) features \u2192 \\(2^N\\) possible subsets.</li> <li>Quickly infeasible:</li> <li>20 features \u2192 \\~1 million subsets</li> <li>25 features \u2192 \\~33.5 million subsets</li> <li>30 features \u2192 \\~1.1 billion subsets</li> </ul>"},{"location":"notes/lecture3/#52-sequential-forward-selection-sfs","title":"5.2 Sequential Forward Selection (SFS)","text":"<ol> <li>Start with empty set \\(S = \\emptyset\\).</li> <li>While stopping criteria not met:</li> <li>For each feature \\(X_f \\notin S\\):<ul> <li>\\(S' = S \\cup \\{X_f\\}\\)</li> <li>Train model on \\(S'\\).</li> <li>Evaluate accuracy.</li> </ul> </li> <li>Select feature with highest improvement.</li> <li>Return final \\(S\\).</li> </ol> <pre><code>flowchart TD\n    Start[Start with empty set S = empty] --&gt; Loop{Stopping Criteria Met?}\n    Loop -- No --&gt; Add[Add best feature]\n    Add --&gt; Train[Train + Evaluate]\n    Train --&gt; Loop\n    Loop -- Yes --&gt; End[Return Subset S]</code></pre> <ul> <li>Advantage: Captures strong features.</li> <li>Disadvantage: Cannot remove redundant features once added.</li> </ul>"},{"location":"notes/lecture3/#53-sequential-backward-elimination-sbe","title":"5.3 Sequential Backward Elimination (SBE)","text":"<ul> <li>Start with all features.</li> <li>Iteratively remove least useful feature.</li> <li>Stop when removal hurts performance.</li> </ul>"},{"location":"notes/lecture3/#54-heuristic-search","title":"5.4 Heuristic Search","text":"<ul> <li>Use optimization strategies like Genetic Algorithms, Simulated Annealing, Greedy search.</li> </ul>"},{"location":"notes/lecture3/#6-filter-methods","title":"6. Filter Methods","text":"<p>Principle: Replace costly model evaluation with fast statistics \\(J(X_f)\\).</p> <p>Examples:</p> <ul> <li>Mutual Information (MI): Captures dependence.</li> <li>Pearson Correlation: Measures linear relationships.</li> <li>Chi-squared test: For categorical features.</li> </ul>"},{"location":"notes/lecture3/#example-ranking-table","title":"Example Ranking Table","text":"Feature Index Score (\\(J(X_f)\\)) 32 0.98 5501 0.94 101 0.91 345 0.85 1104 0.81"},{"location":"notes/lecture3/#7-pearsons-correlation-coefficient","title":"7. Pearson\u2019s Correlation Coefficient","text":"<p>Captures linear relationships between feature \\(A\\) and target \\(Y\\):</p> \\[ \\rho(A,Y) = \\frac{\\text{cov}(A,Y)}{\\sigma_A \\cdot \\sigma_Y} \\] <p>Expanded:</p> \\[ \\rho(A,Y) = \\frac{\\sum_i (A_i - \\bar{A})(Y_i - \\bar{Y})}{\\sqrt{\\sum_i (A_i - \\bar{A})^2} \\cdot \\sqrt{\\sum_i (Y_i - \\bar{Y})^2}} \\] <ul> <li>\\(A_i, Y_i\\): sample values.</li> <li>\\(\\bar{A}, \\bar{Y}\\): means.</li> <li>Covariance matrix captures pairwise correlations.</li> </ul>"},{"location":"notes/lecture3/#8-embedded-methods","title":"8. Embedded Methods","text":"<ul> <li>Selection happens inside the learning algorithm.</li> <li>Examples:</li> <li>Lasso regression (L1): zeroes out unimportant features.</li> <li>Tree-based methods: select splits only on important features.</li> </ul> <pre><code>flowchart LR\n    Data --&gt; Model[Model with Embedded Selection]\n    Model --&gt; Output[Reduced Feature Set]</code></pre>"},{"location":"notes/lecture3/#extended-notes-on-dimensionality-reduction","title":"Extended Notes on Dimensionality Reduction","text":""},{"location":"notes/lecture3/#9-mutual-information-mi","title":"9. Mutual Information (MI)","text":"<p>Measures reduction in uncertainty:</p> \\[ I(A,Y) = \\sum_{x \\in A}\\sum_{y \\in Y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} \\] <ul> <li>If \\(A\\) and \\(Y\\) are independent \u2192 \\(I(A,Y)=0\\).</li> </ul>"},{"location":"notes/lecture3/#10-feature-interactions","title":"10. Feature Interactions","text":"<p>Individual features may appear weak, but combinations can be strong.</p> <ul> <li>Example: Two independent features become predictive when combined.</li> <li>Highlights the limitation of filter methods which evaluate features individually.</li> </ul>"},{"location":"notes/lecture3/#11-pros-cons-of-filter-methods","title":"11. Pros &amp; Cons of Filter Methods","text":"<p>Advantages:</p> <ul> <li>Simple, scalable.</li> <li>Easily parallelizable.</li> </ul> <p>Disadvantages:</p> <ul> <li>Cannot capture feature interactions.</li> <li>May select redundant features.</li> </ul>"},{"location":"notes/lecture3/#12-embedded-methods-lasso","title":"12. Embedded Methods: LASSO","text":"<p>Optimization problem:</p> \\[ \\min_\\beta \\sum_{i=1}^M (y_i - \\sum_j x_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^N |\\beta_j| \\] <ul> <li>L1 penalty induces sparsity.</li> <li>Contrast with Ridge (L2):</li> </ul> \\[ \\lambda \\sum_{j=1}^N \\beta_j^2 \\] <ul> <li>L2 shrinks weights but rarely eliminates them.</li> </ul>"},{"location":"notes/lecture3/#13-benefits-of-feature-selection","title":"13. Benefits of Feature Selection","text":"<ul> <li>Improves accuracy.</li> <li>Reduces computation.</li> <li>Improves interpretability.</li> <li>Prevents overfitting.</li> </ul>"},{"location":"notes/lecture3/#14-t-sne-motivation","title":"14. t-SNE Motivation","text":"<ul> <li>High-dimensional visualization is difficult.</li> <li>Project data into 2D or 3D for interpretability.</li> <li>t-SNE preserves local structure.</li> </ul>"},{"location":"notes/lecture3/#15-general-problem-statement","title":"15. General Problem Statement","text":"<p>Given high-dimensional \\(X = \\{x_1,...,x_M\\}\\), \\(x_i \\in \\mathbb{R}^N\\):</p> <p>Find \\(Y = \\{y_1,...,y_M\\}\\), \\(y_i \\in \\mathbb{R}^n, n &lt; N\\), minimizing information loss.</p>"},{"location":"notes/lecture3/#16-stochastic-neighbor-embedding-sne","title":"16. Stochastic Neighbor Embedding (SNE)","text":"<ul> <li>Preserves local distances.</li> <li>Converts distances to conditional probabilities:</li> </ul> \\[ p_{j|i} = \\frac{e^{-||x_i-x_j||^2/2\\sigma_i^2}}{\\sum_k e^{-||x_i-x_k||^2/2\\sigma_i^2}} \\] \\[ q_{j|i} = \\frac{e^{-||y_i-y_j||^2}}{\\sum_k e^{-||y_i-y_k||^2}} \\]"},{"location":"notes/lecture3/#17-from-sne-to-t-sne","title":"17. From SNE to t-SNE","text":"<ul> <li>High-dimensional similarities use Gaussian.</li> <li>Low-dimensional similarities use Student\u2019s t-distribution (heavier tails).</li> </ul> \\[ q_{ij} = \\frac{(1+||y_i-y_j||^2)^{-1}}{\\sum_{k \\neq l}(1+||y_k-y_l||^2)^{-1}} \\]"},{"location":"notes/lecture3/#18-kl-divergence-in-t-sne","title":"18. KL Divergence in t-SNE","text":"<p>Objective:</p> \\[ KL(P||Q) = \\sum_i \\sum_j p_{j|i}\\log \\frac{p_{j|i}}{q_{j|i}} \\] <ul> <li>Penalizes mismatches in similarity structure.</li> <li>Large \\(p_{j|i}\\) with small \\(q_{j|i}\\) \u2192 heavy penalty.</li> </ul>"},{"location":"notes/lecture3/#19-gradient-descent-optimization","title":"19. Gradient Descent Optimization","text":"<p>Gradient update rule:</p> \\[ \\frac{\\partial C}{\\partial y_i} = 2\\sum_j (y_j-y_i)(p_{j|i}-q_{j|i}+p_{i|j}-q_{i|j}) \\] <ul> <li>Emphasizes preserving close distances.</li> </ul>"},{"location":"notes/lecture3/#20-visualization-example-mnist","title":"20. Visualization Example (MNIST)","text":"<ul> <li>Applied to 6000 MNIST digits.</li> <li>t-SNE clusters digits cleanly compared to Sammon mapping.</li> </ul>"},{"location":"notes/lecture3/#21-summary-mindmap","title":"21. Summary Mindmap","text":"<pre><code>mindmap\n  root((Dimensionality Reduction))\n    Feature Selection\n      Wrapper\n        Forward Selection\n        Backward Elimination\n        Heuristic Search\n      Filter\n        Correlation\n        Chi-squared\n        Mutual Information\n      Embedded\n        Lasso (L1)\n        Decision Trees\n    Feature Learning\n      PCA\n      Kernel PCA\n      MDS\n      Autoencoders\n      t-SNE\n        SNE\n        KL Divergence\n        Gradient Descent</code></pre>"},{"location":"notes/lecture3/#key-takeaways","title":"\u2705 Key Takeaways","text":"<ul> <li>Dimensionality reduction combats the curse of dimensionality.</li> <li>Two approaches: Feature Selection (simpler, interpretable) vs. Feature Learning (powerful, less interpretable).</li> <li>Methods include Wrapper, Filter, Embedded selection, PCA, t-SNE, autoencoders.</li> <li>t-SNE is best for visualization, capturing local structures.</li> <li>Feature selection methods improve performance and interpretability for predictive modeling.</li> </ul>"},{"location":"notes/lecture3/#dimensionality-reduction-svd-pca-mf-nmf","title":"Dimensionality Reduction (SVD \u2022 PCA \u2022 MF \u2022 NMF)","text":""},{"location":"notes/lecture3/#motivation","title":"\ud83e\udded Motivation","text":"<p>We are deluged with data. Essential information often lives in a much smaller-dimensional subspace. Dimensionality reduction aims to:</p> <ul> <li>compress data (fewer numbers, less storage),</li> <li>denoise (throw away directions dominated by noise),</li> <li>uncover structure (clusters, topics, latent factors),</li> <li>speed up downstream learning.</li> </ul> <pre><code>flowchart LR\n  X[\"High-dim Data: X in R^(m\u00d7n)\"] --&gt;|Decompose / Project| Z[\"Low-dim Representation\"]\n  Z --&gt;|Reconstruct| Xhat[\"Approximate X\"]\n  Xhat --&gt;|Error| Err[\"Reconstruction Error e\"]\n  Z --&gt; Models[\"Downstream models: kNN, SVM, regressors\"]\n\n  subgraph Families\n    A1[\"SVD / PCA (linear, global)\"]\n    A2[\"MF / NMF (parts-based, sparse)\"]\n    A3[\"t-SNE / UMAP (nonlinear, local)\"]\n  end\n\n  X --&gt; A1\n  X --&gt; A2\n  X --&gt; A3\n</code></pre>"},{"location":"notes/lecture3/#1-singular-value-decomposition-svd","title":"1) Singular Value Decomposition (SVD)","text":""},{"location":"notes/lecture3/#definition","title":"Definition","text":"<p>For any real matrix \\(X \\in \\mathbb{R}^{m\\times n}\\):</p> \\[ X = U\\,\\Sigma\\,V^T, \\] <p>where</p> <ul> <li>\\(U\\in\\mathbb{R}^{m\\times m}\\) and \\(V\\in\\mathbb{R}^{n\\times n}\\) are orthogonal (\\(U^TU=I\\), \\(V^TV=I\\)),</li> <li>\\(\\Sigma\\in\\mathbb{R}^{m\\times n}\\) is diagonal with non\u2011negative singular values \\(\\sigma_1\\ge\\sigma_2\\ge\\cdots\\ge0\\).</li> </ul>"},{"location":"notes/lecture3/#intuition","title":"Intuition","text":"<p>A linear map = rotate (V) \u2192 scale (\u03a3) \u2192 rotate (U). Large \\(\\sigma_i\\) indicate energetic directions.</p> <pre><code>graph TD\n  V[Rotate by V^T] --&gt; S[Scale by \u03a3] --&gt; U[Rotate by U]</code></pre>"},{"location":"notes/lecture3/#best-rankk-approximation-eckartyoung","title":"Best rank\u2011k approximation (Eckart\u2013Young)","text":"<p>Let \\(X_k = U_{:,1:k}\\,\\Sigma_{1:k,1:k}\\,V_{:,1:k}^T\\). Then for any matrix \\(Y\\) with \\(\\operatorname{rank}(Y)\\le k\\):</p> \\[ \\|X - X_k\\|_F \\le \\|X - Y\\|_F\\quad\\text{and}\\quad \\|X - X_k\\|_2 = \\sigma_{k+1}. \\]"},{"location":"notes/lecture3/#worked-example-exact-svd-of-a-22-matrix","title":"\ud83d\udd22 Worked Example \u2013 Exact SVD of a 2\u00d72 matrix","text":"<p>Let</p> \\[ X=\\begin{bmatrix}3 &amp; 1\\\\1 &amp; 3\\end{bmatrix}. \\] <ol> <li>Compute \\(X^TX=\\begin{bmatrix}10&amp;6\\\\6&amp;10\\end{bmatrix}\\).</li> <li>Eigenpairs of \\(X^TX\\):</li> <li>\\(\\lambda_1=16\\) with eigenvector \\(v_1=\\tfrac{1}{\\sqrt2}[1,\\,1]^T\\),</li> <li>\\(\\lambda_2=4\\) with eigenvector \\(v_2=\\tfrac{1}{\\sqrt2}[1,\\,-1]^T\\). Hence singular values: \\(\\sigma_1=\\sqrt{16}=4\\), \\(\\sigma_2=\\sqrt{4}=2\\). Let \\(V=[v_1\\ v_2]\\).</li> <li>Compute left vectors: \\(u_i = \\tfrac{1}{\\sigma_i} X v_i\\).</li> <li>\\(u_1 = \\tfrac{1}{4}Xv_1 = \\tfrac{1}{4}\\tfrac{1}{\\sqrt2}[4,\\,4]^T = \\tfrac{1}{\\sqrt2}[1,1]^T\\),</li> <li>\\(u_2 = \\tfrac{1}{2}Xv_2 = \\tfrac{1}{2}\\tfrac{1}{\\sqrt2}[2,\\,-2]^T = \\tfrac{1}{\\sqrt2}[1,-1]^T\\). Let \\(U=[u_1\\ u_2]\\), \\(\\Sigma=\\operatorname{diag}(4,2)\\).</li> <li>Verify: \\(U\\Sigma V^T=X\\).</li> </ol>"},{"location":"notes/lecture3/#worked-example-rank1-truncation-error","title":"\ud83d\udd22 Worked Example \u2013 Rank\u20111 truncation &amp; error","text":"<p>Rank\u20111 approximation \\(X_1=4\\,u_1 v_1^T\\). Error in spectral norm = \\(\\sigma_2=2\\). In Frobenius norm: \\(\\|X-X_1\\|_F=\\sqrt{\\sigma_2^2}=2\\).</p>"},{"location":"notes/lecture3/#power-iteration-computing-top-singular-vector","title":"Power Iteration (computing top singular vector)","text":"<ul> <li>Initialize \\(u^{(0)}\\) randomly.</li> <li>Iterate: \\(v^{(t)}=X^Tu^{(t)};\\ u^{(t+1)} = Xv^{(t)}/\\|Xv^{(t)}\\|\\).</li> <li>Converges to top left singular vector.</li> </ul> <pre><code>sequenceDiagram\n  participant U as u^(t)\n  participant XT as X^T\n  participant V as v^(t)\n  participant X as X\n  U-&gt;&gt;XT: v = X^T u\n  XT--&gt;&gt;V: v^(t)\n  V-&gt;&gt;X: u' = X v\n  X--&gt;&gt;U: normalize(u')</code></pre>"},{"location":"notes/lecture3/#2-principal-component-analysis-pca","title":"2) Principal Component Analysis (PCA)","text":""},{"location":"notes/lecture3/#two-equivalent-views","title":"Two equivalent views","text":"<ol> <li>Eigenview on covariance: with centered data \\(X_c\\), \\(\\tfrac{1}{n}X_cX_c^T = U\\Lambda U^T\\). PCs are columns of \\(U\\).</li> <li>SVD view: \\(X_c=U\\Sigma V^T\\). Then principal directions = columns of \\(U\\); variances = \\(\\sigma_i^2/(n-1)\\).</li> </ol>"},{"location":"notes/lecture3/#algorithm","title":"Algorithm","text":"<ol> <li>Center features: \\(X_c = X - \\mu 1^T\\).</li> <li>Compute SVD: \\(X_c=U\\Sigma V^T\\).</li> <li>Choose \\(k\\) via explained variance ratio \\(\\sum_{i=1}^k \\sigma_i^2 / \\sum_{i} \\sigma_i^2\\).</li> <li>Low\u2011dim representation (scores): \\(Z=U_{:,1:k}^T X_c\\).</li> <li>Reconstruction: \\(\\hat X = U_{:,1:k}Z + \\mu 1^T\\).</li> </ol> <pre><code>flowchart LR\n  A[Raw data X] --&gt; B[Center by mean \u03bc]\n  B --&gt; C[SVD of X_c]\n  C --&gt; D[Pick k by EVR]\n  D --&gt; E[Scores Z = U_k^T X_c]\n  E --&gt; F[Use Z for ML]\n  E --&gt; G[Reconstruct X\u0302 = U_k Z + \u03bc]</code></pre>"},{"location":"notes/lecture3/#worked-example-pca-on-a-2d-toy-dataset","title":"\ud83d\udd22 Worked Example \u2013 PCA on a 2D toy dataset","text":"<p>Data (n=5 points): \\((2,0),(0,2),(3,1),(4,0),(0,3)\\).</p> <ol> <li>Mean: \\(\\mu=[1.8,\\ 1.2]^T\\). Centered matrix</li> </ol> \\[ X_c=\\begin{bmatrix} 0.2 &amp; -1.2\\\\ -1.8 &amp; 0.8\\\\ 1.2 &amp; -0.2\\\\ 2.2 &amp; -1.2\\\\ -1.8 &amp; 1.8 \\end{bmatrix}. \\] <ol> <li>Covariance: \\(C=\\tfrac{1}{n-1}X_c^TX_c=\\begin{bmatrix}3.7 &amp; -2.9\\\\ -2.9 &amp; 2.3\\end{bmatrix}.\\)</li> <li>Eigenpairs (rounded): \\(\\lambda_1\\approx5.89\\) (dir. \\(v_1\\propto[0.82,-0.57]\\)), \\(\\lambda_2\\approx0.11\\).</li> <li>Explained variance of PC1: \\(5.89/(5.89+0.11)\\approx98.2\\%\\).</li> <li>Project onto PC1: scores = \\(Z = X_c v_1\\) (one scalar per point).</li> <li>Reconstruction with \\(k=1\\): \\(\\hat X = (v_1 v_1^T)X_c + \\mu\\).\\    Result: good 1D compression with tiny error (since \\(\\lambda_2\\) small).</li> </ol>"},{"location":"notes/lecture3/#pca-for-images-patch-pca","title":"PCA for images (patch PCA)","text":"<ul> <li>Patch size 12\u00d712 \u2192 144\u2011D.</li> <li>Compute PCs \u2192 basis filters (edges, blobs).</li> <li>Reconstruction error vs k typically decays fast; keep \\(k\\) around 20\u201360 for strong compression on patches.</li> </ul>"},{"location":"notes/lecture3/#3-latent-semantic-indexing-lsi-via-svd-text","title":"3) Latent Semantic Indexing (LSI) via SVD (text)","text":"<p>Given a term\u2013document matrix \\(X\\) (tf or tf\u2013idf). Compute rank\u2011\\(k\\) SVD: \\(X\\approx U_k\\Sigma_kV_k^T\\).</p> <ul> <li>Columns of \\(U_k\\) give term embeddings; columns of \\(V_k\\) give document embeddings; \\(\\Sigma_k\\) rescales.</li> <li>Similarity in this space captures latent topics even if exact words differ.</li> </ul>"},{"location":"notes/lecture3/#worked-example-mini-tdm-3-terms-3-docs","title":"\ud83d\udd22 Worked Example \u2013 Mini TDM (3 terms \u00d7 3 docs)","text":"\\[ X=\\begin{bmatrix} 1&amp;1&amp;0\\\\ 1&amp;0&amp;1\\\\ 0&amp;1&amp;1 \\end{bmatrix}. \\] <ul> <li>Full SVD gives \\(\\sigma\\approx(2,1,0)\\).</li> <li>Rank\u20112 LSI keeps first two singular values/vectors; cosine similarities between docs improve (synonymy effect).</li> </ul>"},{"location":"notes/lecture3/#4-matrix-factorization-mf","title":"4) Matrix Factorization (MF)","text":"<p>We model an (often incomplete) matrix \\(R\\in\\mathbb{R}^{n_u\\times n_i}\\) of user\u2013item interactions as</p> \\[ R \\approx U^T V + b\\,1^T + 1\\,\\tilde b^T, \\] <p>where \\(U\\in\\mathbb{R}^{r\\times n_u}\\) and \\(V\\in\\mathbb{R}^{r\\times n_i}\\) are latent factors, and \\(b,\\tilde b\\) biases.</p>"},{"location":"notes/lecture3/#objective-with-l2-regularization","title":"Objective (with L2 regularization)","text":"\\[ \\min_{U,V,b,\\tilde b} \\sum_{(i,j)\\in\\Omega} (r_{ij}-u_i^T v_j - b_i - \\tilde b_j)^2  + \\lambda(\\|U\\|_F^2+\\|V\\|_F^2 + \\|b\\|_2^2 + \\|\\tilde b\\|_2^2), \\] <p>where \\(\\Omega\\) is the set of observed entries.</p>"},{"location":"notes/lecture3/#algorithms","title":"Algorithms","text":"<ul> <li>ALS (Alternating Least Squares): fix \\(U\\), solve least squares for \\(V\\); swap. Scales well in Spark.</li> <li>SGD: update on each observed triple using gradients; supports online/streaming.</li> </ul> <pre><code>flowchart LR\n  R[Observed ratings R] --&gt; ALS{ALS Loop}\n  ALS --&gt;|fix U| SolveV[Solve for each v_j]\n  ALS --&gt;|fix V| SolveU[Solve for each u_i]\n  SolveU --&gt; ALS\n  SolveV --&gt; ALS\n  ALS --&gt; Pred[Predict r\u0302_ij = u_i^T v_j + b_i + b\u0303_j]</code></pre>"},{"location":"notes/lecture3/#worked-example-tiny-mf-with-als-rank-r2","title":"\ud83d\udd22 Worked Example \u2013 Tiny MF with ALS (rank r=2)","text":"<p>Observed ratings (\"?\" missing):</p> \\[ R=\\begin{array}{c|ccc}  &amp; i_1 &amp; i_2 &amp; i_3 \\\\\\hline u_1 &amp; 5 &amp; 3 &amp; ?\\\\ u_2 &amp; 4 &amp; ? &amp; 1\\\\ \\end{array} \\] <p>Initialize \\(U=\\begin{bmatrix}1&amp;0\\\\0&amp;1\\end{bmatrix}\\), \\(V=\\begin{bmatrix}1&amp;1&amp;1\\\\1&amp;1&amp;1\\end{bmatrix}\\). One ALS sweep (showing item 2):</p> <ul> <li>Users who rated \\(i_2\\): only \\(u_1\\) with rating 3. Solve \\(\\min_{v_2}\\ (3 - u_1^Tv_2)^2 + \\lambda\\|v_2\\|^2\\). With \\(u_1=[1,0]^T\\) and \\(\\lambda=0.1\\), closed form gives \\(v_2 = (U^{(2)T}U^{(2)}+\\lambda I)^{-1}U^{(2)T}r^{(2)} = (1.1I)^{-1}[3,0]^T = [2.727,0]^T\\). Repeat for other items and then solve for user vectors given items. After a few ALS rounds, predict missing: \\(\\hat r_{1,3}=u_1^Tv_3 + b_1 + \\tilde b_3\\) (biases often raise accuracy by 5\u201310%).</li> </ul>"},{"location":"notes/lecture3/#worked-example-onestep-sgd-update","title":"\ud83d\udd22 Worked Example \u2013 One\u2011step SGD update","text":"<p>Loss on observed \\((i,j)=(1,2)\\): \\(\\ell = (r_{12}-u_1^Tv_2)^2 + \\lambda(\\|u_1\\|^2+\\|v_2\\|^2)\\). Gradients: \\(\\nabla_{u_1}=-2(r_{12}-u_1^Tv_2)v_2 + 2\\lambda u_1\\), similarly for \\(v_2\\). Update with step \\(\\eta\\): \\(u_1 \\leftarrow u_1 - \\eta\\nabla_{u_1}\\), \\(v_2 \\leftarrow v_2 - \\eta\\nabla_{v_2}\\).</p>"},{"location":"notes/lecture3/#5-nonnegative-matrix-factorization-nmf","title":"5) Non\u2011negative Matrix Factorization (NMF)","text":"<p>We seek non\u2011negative factors \\(W\\in\\mathbb{R}_+^{m\\times r}\\), \\(H\\in\\mathbb{R}_+^{r\\times n}\\) such that \\(X \\approx WH.\\) This encourages parts\u2011based representations (e.g., eyes, nose, mouth for faces; topics for text).</p>"},{"location":"notes/lecture3/#multiplicative-updates-lee-seung","title":"Multiplicative updates (Lee &amp; Seung)","text":"<p>For objective \\(\\min_{W,H}\\ \\|X-WH\\|_F^2\\):</p> \\[ H \\leftarrow H \\odot \\frac{W^T X}{W^T W H},\\quad W \\leftarrow W \\odot \\frac{X H^T}{W H H^T}\\quad (\\odot: \\text{elementwise}). \\]"},{"location":"notes/lecture3/#worked-example-nmf-on-a-43-matrix-1-iteration","title":"\ud83d\udd22 Worked Example \u2013 NMF on a 4\u00d73 matrix (1 iteration)","text":"\\[ X = \\begin{bmatrix}4&amp;1&amp;0\\\\3&amp;0&amp;1\\\\0&amp;2&amp;3\\\\0&amp;1&amp;4\\end{bmatrix},\\quad r=2, \\ W=\\begin{bmatrix}1&amp;0.5\\\\1&amp;0.5\\\\0.5&amp;1\\\\0.5&amp;1\\end{bmatrix},\\ H=\\begin{bmatrix}1&amp;1&amp;1\\\\1&amp;1&amp;1\\end{bmatrix}. \\] <p>Compute updates (showing \\(H\\) numerator/denominator first column):</p> <ul> <li>\\(W^TX = \\begin{bmatrix}7&amp;2&amp;1\\\\4.5&amp;2&amp;4.5\\end{bmatrix}\\Rightarrow (W^TX)_{:,1}=[7,\\ 4.5]^T\\).</li> <li>\\(W^TWH = (W^TW)H\\) with \\(W^TW=\\begin{bmatrix}2.5&amp;2\\\\2&amp;2.5\\end{bmatrix}\\) gives first col \\([4.5,\\ 4.5]^T\\).</li> <li>New first column of \\(H\\): elementwise multiply by \\([7/4.5,\\ 4.5/4.5]=[1.556,\\ 1]\\). Repeat for other columns; then update \\(W\\) analogously. Observation: factors stay non\u2011negative and begin specializing columns/rows.</li> </ul>"},{"location":"notes/lecture3/#6-pca-vs-autoencoders-ae-encoderdecoder-view","title":"6) PCA vs Autoencoders (AE) \u2013 Encoder\u2013Decoder view","text":"<ul> <li>PCA: linear AE with encoder \\(U_k^T\\), decoder \\(U_k\\), MSE loss, orthogonal columns.</li> <li>AE: can be deep/nonlinear; objective can include sparsity, denoising, contrastive losses.</li> </ul> <pre><code>flowchart LR\n  X[\"X \u2208 R^{m\u00d7n}\"] --&gt; E[\"Encoder f\u03b8\"]\n  E --&gt; Z[\"z \u2208 R^{k\u00d7n}\"]\n  Z --&gt; D[\"Decoder g\u03c6\"]\n  D --&gt; Xhat[\"X\u0302\"]\n\n  X --- PCA[\"PCA\"]\n  PCA --- Xhat\n  X --- AE[\"Autoencoder\"]\n  AE --- Xhat\n\n  %% Style edge labels as separate nodes\n  PCA:::linear\n  AE:::nonlinear\n\n  classDef linear fill:#d9ead3,stroke:#333,stroke-width:1px;\n  classDef nonlinear fill:#fce5cd,stroke:#333,stroke-width:1px;\n</code></pre>"},{"location":"notes/lecture3/#7-practical-guidance-diagnostics","title":"7) Practical guidance &amp; diagnostics","text":"<ul> <li>Centering &amp; Scaling: PCA requires centering; consider standardizing features with different scales.</li> <li>Choosing k: Scree plot (\\(\\sigma_i\\)), cumulative EVR \u2265 0.90\u20130.99 for compression; cross\u2011validate for downstream task.</li> <li>Out\u2011of\u2011sample transforms: Store \\(\\mu\\) and \\(U_k\\) to project new data: \\(z=U_k^T(x-\\mu)\\).</li> <li>Cold start in MF: use content features or priors; biases stabilize.</li> <li>Regularization: \\(\\lambda\\) prevents overfitting in MF; shrink small PCs if noisy (Tikhonov).</li> <li>Sparsity: Prefer MF/NMF when matrices are highly sparse; SVD on dense TDM often uses truncated solvers (Lanczos).</li> </ul>"},{"location":"notes/lecture3/#8-extended-solved-miniproblems","title":"8) Extended solved mini\u2011problems","text":""},{"location":"notes/lecture3/#81-pca-by-hand-32-matrix","title":"8.1 PCA by hand (3\u00d72 matrix)","text":"\\[ X=\\begin{bmatrix}2&amp;0\\\\0&amp;2\\\\3&amp;1\\end{bmatrix},\\ \\mu=\\tfrac{1}{3}[5,\\ 1]^T,\\ X_c=X-\\mu 1^T. \\] <p>Compute SVD of \\(X_c\\) (algebra similar to the earlier 2\u00d72 SVD). Keep largest \\(\\sigma\\) only; reconstruct \\(\\hat X\\); compute error \\(\\|X-\\hat X\\|_F\\).</p>"},{"location":"notes/lecture3/#82-filling-a-missing-rating-with-mf-closed-form-no-bias","title":"8.2 Filling a missing rating with MF (closed form, no bias)","text":"<p>Given two users/two items with observed \\(r_{11}=5, r_{21}=4, r_{12}=3\\); estimate \\(r_{22}\\) with rank\u20111 MF and \\(\\lambda=0\\).\\ Solve \\(\\min_{u_1,u_2,v_1,v_2}\\sum (r_{ij}-u_iv_j)^2\\). Closed form (normal equations) gives \\(u_1=\\tfrac{5}{v_1}, u_2=\\tfrac{4}{v_1}, v_2=\\tfrac{3}{u_1}\\Rightarrow r_{22}=u_2v_2=\\tfrac{4}{v_1}\\cdot\\tfrac{3}{u_1}=\\tfrac{12}{5}\\approx2.4\\) (one of the valid minima). Adding \\(\\lambda&gt;0\\) stabilizes.</p>"},{"location":"notes/lecture3/#83-choosing-k-via-error-bound","title":"8.3 Choosing k via error bound","text":"<p>If singular values (descending) are \\((10, 3, 1, 0.2, \u2026)\\) and we keep \\(k=2\\), then</p> <ul> <li>spectral\u2011norm error = \\(\\sigma_{3}=1\\),</li> <li>relative Frobenius error \\(\\approx\\sqrt{1^2+0.2^2}/\\sqrt{10^2+3^2+1^2+0.2^2}\\approx\\tfrac{1.02}{10.53}\\approx9.7\\%\\).</li> </ul>"},{"location":"notes/lecture3/#84-nmf-topic-sketch-on-tdm-toy","title":"8.4 NMF topic sketch on TDM (toy)","text":"<p>Terms = {color, fabric, size}, Docs = {d1,d2,d3} with \\(X=\\begin{bmatrix}4&amp;1&amp;0\\\\3&amp;0&amp;1\\\\0&amp;2&amp;3\\end{bmatrix}\\). With \\(r=2\\), one topic leans on {color,fabric}, the other on {fabric,size}. Multiplicative updates separate them over iterations.</p>"},{"location":"notes/lecture3/#9-quick-reference-cheatsheet","title":"9) Quick reference (cheatsheet)","text":"<ul> <li>SVD: \\(X=U\\Sigma V^T\\); rank\u2011k truncation is optimal (EYM theorem).</li> <li>PCA: SVD of centered data; scores = \\(U_k^T X_c\\); reconstruction = \\(U_k U_k^T X_c + \\mu\\).</li> <li>MF (ALS): solve \\((U^{(j)T}U^{(j)}+\\lambda I)v_j=U^{(j)T}r^{(j)}\\); symmetric for \\(u_i\\).</li> <li>NMF: multiplicative updates; non\u2011negativity \u21d2 parts\u2011based, interpretable factors.</li> </ul>"},{"location":"notes/lecture3/#appendix-a-notation","title":"Appendix A \u2014 Notation","text":"<ul> <li>\\(m\\): features; \\(n\\): samples; \\(r\\): reduced rank.</li> <li>\\(\\|\\cdot\\|_F\\): Frobenius norm; \\(\\|\\cdot\\|_2\\): spectral norm.</li> <li>\\(1\\): all\u2011ones vector; \\(I\\): identity.</li> </ul>"},{"location":"notes/lecture4/","title":"Unsupervised Learning","text":"<p>Unsupervised learning is a type of machine learning where the model is trained on unlabeled data. The goal is to discover hidden patterns, structures, or representations in the data without explicit supervision.</p>"},{"location":"notes/lecture4/#types-of-unsupervised-learning","title":"Types of Unsupervised Learning","text":"<ol> <li> <p>Dimensionality Reduction    Transformation of data from high-dimensional vector space to low-dimensional space without losing important information.</p> </li> <li> <p>Clustering    Grouping input data points into clusters based on similarity.</p> </li> <li> <p>Generative Modelling    Modeling the underlying data distribution and generating new data points from it.</p> </li> <li> <p>Representation Learning    Learning low-dimensional semantic representations of data.</p> </li> </ol>"},{"location":"notes/lecture4/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>Dimensionality reduction reduces the number of features while retaining essential information.</p> <ul> <li>Common methods: PCA, t\u2011SNE, Matrix factorization (NMF/SVD), Autoencoders</li> <li>PCA (Principal Component Analysis):   Projects data onto new axes that capture maximum variance.</li> <li>In PCA, the projection axes correspond to the eigenvectors of the covariance matrix.</li> </ul>"},{"location":"notes/lecture4/#mathematical-view","title":"Mathematical View","text":"<p>Given zero-mean data matrix \\(X\\in\\mathbb{R}^{n\\times d}\\):</p> <ol> <li>Center the data.</li> <li>Compute covariance matrix:    $$ C = \\frac{1}{n} X^T X $$</li> <li>Compute eigenvalues and eigenvectors of \\(C\\).</li> <li>Project data onto top-\\(k\\) eigenvectors.</li> </ol>"},{"location":"notes/lecture4/#pca-example-concept","title":"PCA Example (concept)","text":"<pre><code>%%{init: {'theme': 'default'}}%%\nflowchart LR\n    A[Raw 2D Data Distribution] --&gt; B[Projection on Primary Eigenvector]\n    A --&gt; C[Projection on Secondary Eigenvector]</code></pre>"},{"location":"notes/lecture4/#pca-minimal-python","title":"PCA: Minimal Python","text":"<pre><code>import numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.datasets import load_iris\n\nX, y = load_iris(return_X_y=True)\nX = X - X.mean(axis=0)  # center\n\npca = PCA(n_components=2, random_state=0)\nZ = pca.fit_transform(X)\n\nprint(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n# Z can be plotted or used downstream (e.g., clustering)\n</code></pre>"},{"location":"notes/lecture4/#clustering","title":"Clustering","text":"<p>Definition: Clustering is the task of grouping a set of objects so that objects within the same group are more similar to each other than to objects in other groups.</p>"},{"location":"notes/lecture4/#types-of-clustering-models","title":"Types of Clustering Models","text":"<ol> <li>Centroid-based </li> <li>Clusters are represented by a central vector (centroid).  </li> <li>Each data point belongs to the cluster with the closest centroid.  </li> <li> <p>Examples: K-means, K-medoids.</p> </li> <li> <p>Connectivity-based (Hierarchical) </p> </li> <li>Builds a hierarchy of clusters based on distances.  </li> <li>Two main strategies:  <ul> <li>Agglomerative: Start with each point as a cluster and merge.</li> <li>Divisive: Start with one big cluster and split recursively.  </li> </ul> </li> <li> <p>Produces a dendrogram.</p> </li> <li> <p>Graph-based </p> </li> <li>Models data as a graph where nodes are data points and edges represent similarity.  </li> <li> <p>Partitions graph into communities using methods like spectral clustering or minimum cut.</p> </li> <li> <p>Distribution-based </p> </li> <li>Assumes data is generated from a mixture of probability distributions.  </li> <li>Learns parameters of distributions using methods like Expectation-Maximization (EM).  </li> <li> <p>Example: Gaussian Mixture Models (GMMs).</p> </li> <li> <p>Density-based </p> </li> <li>Defines clusters as high-density regions separated by low-density regions.  </li> <li>Can discover arbitrarily shaped clusters and handle noise/outliers.  </li> <li> <p>Examples: DBSCAN, OPTICS.</p> </li> <li> <p>Others </p> </li> <li>Grid-based: Divides space into a grid and identifies dense cells.  </li> <li>Neural network\u2013based: e.g., Self-Organizing Maps (SOM).  </li> <li>Soft clustering: Assigns probabilities of belonging to clusters.  </li> <li>Mutual information clustering: Groups data by maximizing shared information.</li> </ol>"},{"location":"notes/lecture4/#clustering-minimal-python-snippets","title":"Clustering: Minimal Python Snippets","text":"<p>K-means / Elbow &amp; Silhouette <pre><code>import numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nX = np.random.RandomState(0).randn(500, 2)\n\ninertias, silhouettes = [], []\nfor k in range(2, 10):\n    km = KMeans(n_clusters=k, n_init=10, random_state=0)\n    labels = km.fit_predict(X)\n    inertias.append(km.inertia_)\n    silhouettes.append(silhouette_score(X, labels))\n\nbest_k = 2 + int(np.argmax(silhouettes))\nprint({\"best_k_by_silhouette\": best_k})\n</code></pre></p> <p>Hierarchical (Agglomerative) &amp; Dendrogram <pre><code>from sklearn.datasets import make_blobs\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nimport matplotlib.pyplot as plt\n\nX, _ = make_blobs(n_samples=80, centers=3, cluster_std=0.60, random_state=0)\nZ = linkage(X, method=\"ward\")\n# In a notebook, you would: dendrogram(Z); plt.show()\n</code></pre></p> <p>DBSCAN (density-based) <pre><code>from sklearn.cluster import DBSCAN\n\nX, _ = make_blobs(n_samples=300, centers=3, cluster_std=(0.3, 0.5, 0.7), random_state=42)\nlabels = DBSCAN(eps=0.5, min_samples=5).fit_predict(X)\nprint(\"Clusters found (including -1 for noise):\", set(labels))\n</code></pre></p>"},{"location":"notes/lecture4/#k-means-clustering","title":"K-means Clustering","text":"<p>Definition: A method of grouping \\(N\\) data points into \\(K\\) clusters, where each data point belongs to the nearest cluster center based on a distance metric.</p>"},{"location":"notes/lecture4/#objective","title":"Objective","text":"<p>Minimize the within-cluster sum of squares (WCSS): \\(\\(\\min_{\\{C_k,\\mu_k\\}} \\sum_{k=1}^K \\sum_{x_i\\in C_k} \\lVert x_i-\\mu_k \\rVert^2\\)\\)</p>"},{"location":"notes/lecture4/#k-means-algorithm","title":"K-means Algorithm","text":"<ol> <li>Choose number of clusters \\(k\\).</li> <li>Initialize \\(k\\) cluster centers (e.g., random or k-means++).</li> <li>Assign each data point to its nearest center (by chosen distance metric).</li> <li>Update each center to be the centroid: \\(\\mu_k=\\frac{1}{|C_k|}\\sum_{x_i\\in C_k} x_i\\).</li> <li>Repeat steps 3\u20134 until convergence (centers stop moving or WCSS improvement \\(&lt;\\varepsilon\\)).</li> </ol> <pre><code>%%{init: {'theme': 'forest'}}%%\nflowchart TD\n    A[Initialize k cluster centers] --&gt; B[Assign points to nearest cluster]\n    B --&gt; C[Update cluster centers]\n    C --&gt; D{Converged?}\n    D -- No --&gt; B\n    D -- Yes --&gt; E[Final Clusters]</code></pre>"},{"location":"notes/lecture4/#k-means-minimal-python","title":"K-means: Minimal Python","text":"<pre><code>from sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\nX, _ = make_blobs(n_samples=400, centers=4, random_state=7)\nkm = KMeans(n_clusters=4, n_init=10, random_state=0)\nlabels = km.fit_predict(X)\ncenters = km.cluster_centers_\nprint(\"WCSS (inertia):\", km.inertia_)\n</code></pre>"},{"location":"notes/lecture4/#k-means-initialization-sketch","title":"K-means++ Initialization (sketch)","text":"<pre><code>import numpy as np\n\nrng = np.random.default_rng(0)\n# X shape: (n_samples, d)\ncenters = []\ncenters.append(X[rng.integers(0, len(X))])\nfor _ in range(1, 4):  # pick until K centers\n    d2 = np.min(((X[:, None, :] - np.array(centers)[None, :, :])**2).sum(axis=2), axis=1)\n    probs = d2 / d2.sum()\n    centers.append(X[rng.choice(len(X), p=probs)])\ncenters = np.array(centers)\n</code></pre>"},{"location":"notes/lecture4/#sample-iteration-illustrated","title":"Sample Iteration (Illustrated)","text":"<p>Below is a step-by-step depiction of one complete iteration of K-means (for \\(K=3\\)). Use it to understand the loop that repeats until convergence.</p>"},{"location":"notes/lecture4/#0-inputs-initialization","title":"0) Inputs &amp; Initialization","text":"<ul> <li>Data points \\(\\{x_1,\\dots,x_N\\}\\) in \\(\\mathbb{R}^d\\) (unlabeled).</li> <li>Number of clusters \\(K\\).</li> <li>Initial centers \\(\\mu^{(0)}_1,\\mu^{(0)}_2,\\mu^{(0)}_3\\) (e.g., random or k-means++).</li> </ul> <pre><code>%%{init: {'theme': 'default'}}%%\nflowchart LR\n    D[Data X: x1..xN] --&gt;|choose K| K[Set K=3]\n    K --&gt; I[Init centers \u03bc1\u207d\u2070\u207e, \u03bc2\u207d\u2070\u207e, \u03bc3\u207d\u2070\u207e]</code></pre>"},{"location":"notes/lecture4/#1-assignment-step-e-step-analogue","title":"1) Assignment Step (E-step analogue)","text":"<p>For each point \\(x_i\\), choose the closest center by distance \\(d(\\cdot,\\cdot)\\) (often Euclidean): $$ a_i^{(t)} = \\arg\\min_{k\\in{1,2,3}} d\\big(x_i,\\mu_k^{(t)}\\big). $$</p> <pre><code>%%{init: {'theme': 'neutral'}}%%\nflowchart TD\n    subgraph Assignment at iteration t\n    X1[x1] --&gt;|\"d(x1, mu1^t) d(x1, mu2^t) d(x1, mu3^t)\"| A1[Assign to closest]\n    X2[x2] --&gt; A1\n    XN[xN] --&gt; A1\n    A1 --&gt; C1[C1]\n    A1 --&gt; C2[C2]\n    A1 --&gt; C3[C3]\n    end\n</code></pre>"},{"location":"notes/lecture4/#2-update-step-m-step-analogue","title":"2) Update Step (M-step analogue)","text":"<p>Recompute each center as the mean of currently assigned points: $$ \\mu_k^{(t+1)} = \\frac{1}{|C_k^{(t)}|} \\sum_{x_i\\in C_k^{(t)}} x_i. $$</p> <pre><code>%%{init: {'theme': 'base'}}%%\nflowchart LR\n    C1[C\u2081 points] --&gt; U1[Compute \u03bc\u2081\u207d\u1d57\u207a\u00b9\u207e]\n    C2[C\u2082 points] --&gt; U2[Compute \u03bc\u2082\u207d\u1d57\u207a\u00b9\u207e]\n    C3[C\u2083 points] --&gt; U3[Compute \u03bc\u2083\u207d\u1d57\u207a\u00b9\u207e]\n    U1 --&gt; M[New centers \u03bc\u207d\u1d57\u207a\u00b9\u207e]\n    U2 --&gt; M\n    U3 --&gt; M</code></pre>"},{"location":"notes/lecture4/#3-convergence-check","title":"3) Convergence Check","text":"<ul> <li>Stop if centers move less than a tolerance \\(\\varepsilon\\) or max iterations reached.</li> <li>Otherwise, set \\(t\\leftarrow t+1\\) and repeat Assignment &amp; Update.</li> </ul> <pre><code>%%{init: {'theme': 'forest'}}%%\nstateDiagram-v2\n    [*] --&gt; Assign: Step 1\n    Assign --&gt; Update: Step 2\n    Update --&gt; Check: Step 3\n    Check --&gt; Assign: shift(\u03bc)&lt;\u03b5\n    Check --&gt; [*]: converged</code></pre>"},{"location":"notes/lecture4/#4-full-mini-run-example-t0-t2","title":"4) Full Mini-Run Example (t=0 \u2192 t=2)","text":"<p>A compact timeline of two iterations:</p> <pre><code>%%{init: {'theme': 'neutral'}}%%\nsequenceDiagram\n    participant X as Data X\n    participant M0 as \u03bc\u207d\u2070\u207e (init)\n    participant A1 as Assign (t=0)\n    participant U1 as Update (t=0\u21921)\n    participant M1 as \u03bc\u207d\u00b9\u207e\n    participant A2 as Assign (t=1)\n    participant U2 as Update (t=1\u21922)\n    participant M2 as \u03bc\u207d\u00b2\u207e\n\n    X-&gt;&gt;M0: Choose K &amp; initialize centers\n    M0-&gt;&gt;A1: Use \u03bc\u207d\u2070\u207e to assign each x\u1d62\n    A1-&gt;&gt;U1: Build clusters C\u2081,C\u2082,C\u2083\n    U1-&gt;&gt;M1: Compute new centers \u03bc\u207d\u00b9\u207e\n    M1-&gt;&gt;A2: Re-assign with \u03bc\u207d\u00b9\u207e\n    A2-&gt;&gt;U2: Update centers again \u2192 \u03bc\u207d\u00b2\u207e\n    U2--&gt;&gt;M2: Check convergence</code></pre> <p>Notes - Distance choice matters (Euclidean vs cosine). Scale features or standardize when needed. - Random init can trap in local minima \u2192 prefer k-means++ and multiple restarts. - Use inertia (WCSS) or silhouette score to pick \\(K\\).</p>"},{"location":"notes/lecture4/#applications-of-clustering","title":"Applications of Clustering","text":"<ul> <li>Exploratory Data Analysis</li> <li>Dimensionality Reduction + Clustering</li> <li>Feature Extraction + Clustering</li> <li>Lossy Image Compression</li> <li>Post-training Model Analysis</li> </ul>"},{"location":"notes/lecture4/#image-compression-using-k-means","title":"Image Compression using K-means","text":"<ul> <li>Represent image pixels as vectors (RGB values).</li> <li>Apply K-means to group similar colors.</li> <li>Replace pixels by their cluster centers \u2192 reduces color space \u2192 compression.</li> </ul> <pre><code>%%{init: {'theme': 'default'}}%%\nflowchart LR\n    A[Original Image] --&gt; B[K-means Clustering]\n    B --&gt; C[Compressed Image]</code></pre>"},{"location":"notes/lecture4/#minimal-python-for-color-quantization","title":"Minimal Python for Color Quantization","text":"<pre><code>import numpy as np\nfrom sklearn.cluster import KMeans\nfrom PIL import Image\n\nimg = Image.open(\"path/to/image.jpg\").convert(\"RGB\")\narr = np.array(img).reshape(-1, 3)\n\nK = 16  # number of colors\nkm = KMeans(n_clusters=K, n_init=5, random_state=0).fit(arr)\npalette = km.cluster_centers_.astype(np.uint8)\nlabels = km.labels_\ncompressed = palette[labels].reshape(np.array(img).shape)\n\nImage.fromarray(compressed).save(\"compressed_k16.jpg\")\n</code></pre>"},{"location":"notes/lecture4/#generative-modelling","title":"Generative Modelling","text":""},{"location":"notes/lecture4/#what-is-generative-modelling","title":"What is Generative Modelling?","text":"<ul> <li>A generative model learns the distribution of the input data.</li> <li>Once you have a model of input data, you can:</li> <li>Generate new examples</li> <li>Perform classification/regression (with small labels)</li> <li>Anomaly detection</li> <li>Fill missing data</li> <li>Examples: Density estimation models, Na\u00efve Bayes, Variational Autoencoders (VAE).</li> </ul>"},{"location":"notes/lecture4/#python-example-naive-bayes-generative-for-classification","title":"Python Example: Na\u00efve Bayes (generative for classification)","text":"<pre><code>from sklearn.naive_bayes import GaussianNB\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nX, y = load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nnb = GaussianNB().fit(X_train, y_train)\ny_pred = nb.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n</code></pre>"},{"location":"notes/lecture4/#generative-adversarial-networks-gans","title":"Generative Adversarial Networks (GANs)","text":"<ul> <li>Deep-learning based generative models.</li> <li>Two components:</li> <li>Generator (G): Produces synthetic data from noise.</li> <li>Discriminator (D): Distinguishes between real and fake samples.</li> <li>GANs are trained as a two-player minimax game:</li> </ul> <p>\\(\\min_G \\max_D V(D,G) = \\mathbb{E}_{x\\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z\\sim p_z(z)}[\\log(1-D(G(z)))]\\)</p>"},{"location":"notes/lecture4/#intuition-two-player-game","title":"Intuition: Two-Player Game","text":"<ul> <li>Criminal (Generator): Tries to produce counterfeit money indistinguishable from real money.</li> <li>Cop (Discriminator): Tries to detect fake money.</li> <li>Over iterations, both improve until the generator produces realistic samples.</li> </ul> <pre><code>%%{init: {'theme': 'forest'}}%%\nflowchart LR\n    Z[Random Noise z] --&gt; G[Generator]\n    G --&gt; X_fake[Fake Sample]\n    X_real[Real Sample] --&gt; D[Discriminator]\n    X_fake --&gt; D\n    D --&gt;|Real or Fake| Out[Training Feedback]\n    Out --&gt; G</code></pre>"},{"location":"notes/lecture4/#gan-training-algorithm-mini-batch-sgd","title":"GAN Training Algorithm (Mini-batch SGD)","text":"<ol> <li> <p>For k steps:</p> </li> <li> <p>Sample minibatch of $m$ noise vectors $z^{(1)},..,z^{(m)}$.</p> </li> <li>Sample minibatch of $m$ real examples $x^{(1)},..,x^{(m)}$.</li> <li> <p>Update discriminator $D$ by ascending: $\\nabla_\\theta \\frac{1}{m}\\sum_{i=1}^m [\\log D(x^{(i)}) + \\log(1-D(G(z^{(i)})))]$</p> </li> <li> <p>Update generator $G$ by descending: $\\nabla_\\theta \\frac{1}{m}\\sum_{i=1}^m [\\log(1-D(G(z^{(i)})))]$</p> </li> </ol>"},{"location":"notes/lecture4/#gan-sample-iteration-visualization","title":"GAN Sample Iteration (Visualization)","text":"<ul> <li>Iteration 0: Generator produces random noise.</li> <li>Iteration 1000: Generated samples start to mimic structure of data.</li> <li>Iteration 2000+: Generated distribution approximates real data.</li> </ul> <pre><code>%%{init: {'theme': 'neutral'}}%%\nsequenceDiagram\n    participant Z as Noise z\n    participant G as Generator\n    participant X_fake as Fake Data\n    participant D as Discriminator\n    participant Real as Real Data\n\n    Z-&gt;&gt;G: Generate G(z)\n    G-&gt;&gt;X_fake: Fake samples\n    Real-&gt;&gt;D: Pass real samples\n    X_fake-&gt;&gt;D: Pass fake samples\n    D-&gt;&gt;G: Gradient feedback</code></pre>"},{"location":"notes/lecture4/#minimal-python-gan-pytorch","title":"Minimal Python GAN (PyTorch)","text":"<pre><code>import torch, torch.nn as nn, torch.optim as optim\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(100, 128), nn.ReLU(),\n            nn.Linear(128, 784), nn.Tanh())\n    def forward(self, z): return self.net(z)\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(784, 128), nn.LeakyReLU(0.2),\n            nn.Linear(128, 1), nn.Sigmoid())\n    def forward(self, x): return self.net(x)\n\nG, D = Generator(), Discriminator()\noptG = optim.Adam(G.parameters(), lr=0.0002)\noptD = optim.Adam(D.parameters(), lr=0.0002)\nloss_fn = nn.BCELoss()\n\nz = torch.randn(16, 100)\nfake = G(z)\nprint(fake.shape)  # (16, 784)\n</code></pre>"},{"location":"notes/lecture4/#conditional-gans-cgans","title":"Conditional GANs (cGANs)","text":"<ul> <li>GANs conditioned on additional input $y$ (labels, class, or attributes).</li> <li>Generator learns mapping $G(z|y)$, Discriminator learns $D(x|y)$.</li> <li>Example: Generate digit images conditioned on label (0\u20139).</li> </ul> <pre><code>%%{init: {'theme': 'forest'}}%%\nflowchart TB\n    subgraph cGAN\n    Z[Noise z] --&gt; G[\"Generator G(z|y)\"]\n    Y1[Condition y] --&gt; G\n    G --&gt; X_fake[Fake Sample]\n    X_real[Real Sample] --&gt; D[\"Discriminator D(x|y)\"]\n    Y2[Condition y] --&gt; D\n    X_fake --&gt; D\n    D --&gt; Out[Real/Fake conditioned]\n    Out --&gt; G\n    end\n</code></pre>"},{"location":"notes/lecture4/#pytorch-snippet-for-cgan","title":"PyTorch Snippet for cGAN","text":"<pre><code>class CondGenerator(nn.Module):\n    def __init__(self, n_classes, z_dim=100):\n        super().__init__()\n        self.embed = nn.Embedding(n_classes, 10)\n        self.net = nn.Sequential(\n            nn.Linear(z_dim+10, 128), nn.ReLU(),\n            nn.Linear(128, 784), nn.Tanh())\n    def forward(self, z, y):\n        y_embed = self.embed(y)\n        return self.net(torch.cat([z, y_embed], dim=1))\n</code></pre>"},{"location":"notes/lecture4/#applications-of-gans","title":"Applications of GANs","text":"<p>GANs have been successfully applied to a wide variety of domains:</p> <ol> <li> <p>Super-Resolution</p> </li> <li> <p>Enhancing low-resolution images to high resolution.</p> </li> <li> <p>Example: SRGAN.</p> </li> <li> <p>Connectivity-based</p> </li> <li> <p>Semantic segmentation and label-to-image translation.</p> </li> <li> <p>Example: GauGAN.</p> </li> <li> <p>Graph-based</p> </li> <li> <p>Filling missing regions in images (image inpainting).</p> </li> <li> <p>Example: Context encoders.</p> </li> <li> <p>Distribution-based</p> </li> <li> <p>Style transfer, domain adaptation, aerial-to-map, map-to-aerial transformations.</p> </li> <li> <p>Density-based</p> </li> <li> <p>Face generation, avatar synthesis, human image generation.</p> </li> <li> <p>Others</p> </li> <li> <p>Image editing</p> </li> <li>Imitation learning</li> <li>Music generation</li> </ol>"},{"location":"notes/lecture4/#representation-learning","title":"Representation Learning","text":"<p>Representation learning is a set of techniques in machine learning that enables a system to automatically discover useful representations of raw data. These representations transform raw inputs into more structured formats that make downstream tasks such as feature detection, classification, or regression easier.</p> <p>Instead of manually engineering features, representation learning techniques automatically identify the right abstractions.</p>"},{"location":"notes/lecture4/#types-of-representation-learning","title":"Types of Representation Learning","text":"<ol> <li>Text Representations</li> <li>Converts raw text into vectors that encode semantic meaning.</li> <li>Captures word similarity, syntactic roles, and semantic context.</li> <li> <p>Examples:</p> <ul> <li>Word2Vec (continuous representations of words)</li> <li>BERT (contextual embeddings)</li> </ul> </li> <li> <p>Graph Representations</p> </li> <li>Encodes nodes and edges into dense vectors.</li> <li>Preserves structural information such as neighborhood, connectivity, and paths.</li> <li> <p>Examples:</p> <ul> <li>DeepWalk (random walks + Skip-gram)</li> <li>node2vec (biased random walks for flexible embeddings)</li> </ul> </li> <li> <p>Image Representations</p> </li> <li>Learns features automatically from pixels.</li> <li>Self-supervised contrastive learning extracts features without labeled data.</li> <li>Enables clustering and transfer learning across visual tasks.</li> </ol> <pre><code>flowchart LR\n    subgraph Text\n      A[\"Raw Text corpus\"] --&gt; B[\"Tokenizer\"]\n      B --&gt; C[\"Context Windows\"]\n      C --&gt; D[\"Embedding Model&lt;br&gt;Word2Vec / BERT\"]\n      D --&gt; E[\"Downstream Tasks\"]\n    end\n    subgraph Graph\n      G1[\"Graph Data&lt;br&gt;nodes &amp; edges\"] --&gt; G2[\"Random Walks\"]\n      G2 --&gt; G3[\"Skip-gram Training\"]\n      G3 --&gt; G4[\"Node Embeddings\"]\n    end\n    subgraph Image\n      I1[\"Unlabeled Images\"] --&gt; I2[\"Augmentations\"]\n      I2 --&gt; I3[\"Contrastive Objective\"]\n      I3 --&gt; I4[\"Image Embeddings\"]\n    end\n</code></pre>"},{"location":"notes/lecture4/#distributed-representations","title":"Distributed Representations","text":"<p>A core idea in representation learning is moving from sparse representations (e.g., one-hot vectors) to dense distributed representations (embeddings).</p> <ul> <li>One-hot representation (Sparse)</li> <li>Each word is represented as a binary vector with a single 1.</li> <li>Does not capture similarity between words.</li> <li> <p>High dimensional and memory inefficient.   <pre><code>banana = [0 0 0 0 0 0 0 1 0 0 0]\nmango  = [0 0 0 0 0 1 0 0 0 0 0]\ndog    = [0 0 1 0 0 0 0 0 0 0 0]\n</code></pre></p> </li> <li> <p>Distributed representation (Dense)</p> </li> <li>Each word is a low-dimensional vector of real numbers.</li> <li>Similar words have similar vectors.</li> <li>Captures semantic and syntactic properties.   <pre><code>banana = [0.23  0.1 -0.1 -0.4 -0.01 -0.121 0.342 0.561]\nmango  = [-0.73 0.0 -0.5  0.4  0.4   0.591 0.732 0.891]\ndog    = [0.61  0.21 0.55 0.45 0.134 0.752 0.525 0.64 ]\n</code></pre></li> </ul> <pre><code>flowchart LR\n    O1[One-hot Vectors\\nHigh-dim, sparse] -- limitations --&gt; L1[No similarity\\ninfo]\n    O1 --&gt; L2[Large memory]\n    D1[Dense Embeddings\\nLow-dim, real-valued] -- benefits --&gt; B1[Capture similarity]\n    D1 --&gt; B2[Generalize across contexts]\n    D1 --&gt; B3[Efficient storage]</code></pre>"},{"location":"notes/lecture4/#word2vec-word-representations","title":"Word2Vec: Word Representations","text":"<p>\u201cYou shall know a word by the company it keeps\u201d \u2013 J.R. Firth (1957)</p> <p>Word2Vec is a neural model that learns word embeddings by leveraging context.</p> <ul> <li>A word\u2019s meaning is inferred from its neighboring words in a large corpus.</li> <li>Example:</li> <li>Target word: banking </li> <li>Context words: <code>w_{t-1}, w_{t+1}, ...</code></li> </ul> <p>Word2Vec comes in two variants: CBOW and Skip-gram.</p>"},{"location":"notes/lecture4/#basic-idea-of-word2vec","title":"Basic Idea of Word2Vec","text":"<p>Input assumptions: - A huge corpus of text \\(C\\) - A pre-defined vocabulary \\(V\\)</p> <p>Representation: - Each word has two vectors:   - \\( v_{w_t} \\): word as a target   - \\( v_{w_c} \\): word as a context</p>"},{"location":"notes/lecture4/#models","title":"Models","text":"<ol> <li>Continuous Bag of Words (CBOW)</li> <li>Predicts the target word given surrounding context words.</li> <li> <p>Uses softmax over similarity between the target vector and the sum of context vectors.</p> </li> <li> <p>Skip-gram (SG)</p> </li> <li>Predicts the context words given a target word.</li> <li>Uses softmax over similarity between target and context vectors.</li> </ol> <pre><code>flowchart LR\n    subgraph CBOW\n        w1((\"w_{t-2}\")) --&gt; SUM((\"\u03a3\"))\n        w2((\"w_{t-1}\")) --&gt; SUM\n        w3((\"w_{t+1}\")) --&gt; SUM\n        w4((\"w_{t+2}\")) --&gt; SUM\n        SUM --&gt; target((\"w_t\"))\n    end\n\n    subgraph SkipGram\n        target2((\"w_t\")) --&gt; c1((\"w_{t-2}\"))\n        target2 --&gt; c2((\"w_{t-1}\"))\n        target2 --&gt; c3((\"w_{t+1}\"))\n        target2 --&gt; c4((\"w_{t+2}\"))\n    end\n</code></pre>"},{"location":"notes/lecture4/#skip-gram-with-window","title":"Skip-gram with Window","text":"<p>Skip-gram works by predicting context words within a fixed window size around the target word.</p> <p>For example, with a window size of 2:</p> \\[ P(w_{t-2} | w_t), P(w_{t-1} | w_t), P(w_{t+1} | w_t), P(w_{t+2} | w_t) \\] <pre><code>flowchart LR\n    T((\"banking&lt;br&gt;w_t\")) --&gt; L1((\"problems&lt;br&gt;w_{t-2}\"))\n    T --&gt; L2((\"turning&lt;br&gt;w_{t-1}\"))\n    T --&gt; R1((\"crises&lt;br&gt;w_{t+1}\"))\n    T --&gt; R2((\"as&lt;br&gt;w_{t+2}\"))\n\n    classDef ctx fill:#fff,stroke:#999,stroke-width:1px;\n    class L1,L2,R1,R2 ctx;\n</code></pre> <p>This ensures both left and right contexts contribute to training.</p>"},{"location":"notes/lecture4/#skip-gram-objective-function","title":"Skip-gram Objective Function","text":"<p>The optimization goal is:</p> \\[ \\theta^* = \\arg\\max_{\\theta} \\prod_{i=1}^{T} \\prod_{-m \\leq j \\leq m, j \\neq 0} P(w_{i+j} | w_i; \\theta) \\] <p>Equivalent minimization:</p> \\[ \\theta^* = \\arg\\min_{\\theta} -\\frac{1}{T}\\sum_{i=1}^T \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log P(w_{i+j} | w_i; \\theta) \\] <p>Where probability is defined as:</p> \\[ P(w_{i+j} | w_i) = \\frac{\\exp(v_{w_i}^T v_{w_{i+j}})}{\\sum_{w \\in V} \\exp(v_{w_i}^T v_{w_c})} \\] <ul> <li>Similarity: computed as dot product of target and context vectors.</li> <li>Normalization: softmax ensures probabilities sum to 1 over the entire vocabulary.</li> </ul> <pre><code>flowchart TB\n    subgraph \"Training Step\"\n        A[\"Target word&lt;br&gt;v_{w_t}\"] --&gt; D[\"Dot Product\"]\n        C[\"Context word&lt;br&gt;v_{w_c}\"] --&gt; D\n        D --&gt; S[\"Score\"]\n        S --&gt; SM[\"Softmax /&lt;br&gt;Neg. Sampling\"]\n        SM --&gt; L[\"Loss\"]\n        L --&gt;|backprop| A\n        L --&gt;|backprop| C\n    end\n</code></pre>"},{"location":"notes/lecture4/#word2vec-properties","title":"Word2Vec Properties","text":"<ul> <li>Embeddings place similar words close in vector space.</li> <li>Captures rich semantic relationships, for example:</li> <li>Gender relationship: <code>king - man + woman \u2248 queen</code></li> <li>Verb tense: <code>walked - walking \u2248 swam - swimming</code></li> <li>Geographical relation: <code>Paris - France \u2248 Berlin - Germany</code></li> </ul> <pre><code>flowchart LR\n    K[king] --- M[man]\n    Q[queen] --- W[woman]\n    subgraph Analogies\n      direction LR\n      K --&gt;| - man + woman | Q\n    end\n    subgraph Tense\n      direction LR\n      Walked[walked] --&gt;| - walking | Delta1[\u0394]\n      Swam[swam] --&gt;| - swimming | Delta2[\u0394]\n      Delta1 --- Delta2\n    end\n    subgraph Capitals\n      direction LR\n      France --- Paris\n      Germany --- Berlin\n      France --&gt;|offset \u2248| Germany\n      Paris --&gt;|offset \u2248| Berlin\n    end</code></pre> <p>These relationships emerge naturally without explicit supervision.</p>"},{"location":"notes/lecture4/#negative-sampling","title":"Negative Sampling","text":"<p>A challenge in Word2Vec is that the softmax denominator involves summing over the entire vocabulary, which is computationally expensive for large vocabularies.</p> <p>Solution: Negative Sampling (a form of NCE) - Instead of updating all weights, only update a few sampled negative examples at each step. - The model learns to discriminate between:   - Positive (real context) pairs   - Negative (random noise) pairs</p> <p>New simplified objective:</p> \\[ \\log \\sigma(v_{w_t}^T v_{w_{c}}) + \\sum_{l=1}^k \\mathbb{E}_{w_l \\sim P_n(w)} [\\log \\sigma(-v_{w_t}^T v_{w_l})] \\] <p>Where the noise distribution is:</p> \\[ P_n(w) \\propto U(w)^{3/4} \\] <pre><code>flowchart LR\n    subgraph Sampling\n      T((\"Target&lt;br&gt;w_t\")) --&gt; P1((\"Pos Context&lt;br&gt;w_c\"))\n      T --&gt; N1((\"Neg 1\"))\n      T --&gt; N2((\"Neg 2\"))\n      T --&gt; Nk((\"Neg k\"))\n    end\n\n    P1 --&gt; L1[\"log \u03c3(v_t^T v_c)\"]\n    N1 --&gt; L2[\"log \u03c3(-v_t^T v_{n1})\"]\n    N2 --&gt; L3[\"log \u03c3(-v_t^T v_{n2})\"]\n    Nk --&gt; Lk[\"log \u03c3(-v_t^T v_{nk})\"]\n\n    L1 --&gt; SUM((\"Sum Loss\"))\n    L2 --&gt; SUM\n    L3 --&gt; SUM\n    Lk --&gt; SUM\n</code></pre> <p>This reduces training complexity while preserving embedding quality.</p>"},{"location":"notes/lecture4/#how-deepwalk-node2vec-connect-to-skip-gram","title":"How DeepWalk / node2vec Connect to Skip-gram","text":"<p>Both methods generate node sequences via random walks and then train a Skip-gram model on those sequences\u2014treating walks like sentences.</p> <pre><code>flowchart LR\n    G[Graph] --&gt; RW[\"Random Walks&lt;br&gt;(sequences of nodes)\"]\n    RW --&gt; SG[\"Skip-gram Training\"]\n    SG --&gt; Z[\"Node Embeddings\"]\n\n    style G stroke-width:2px\n</code></pre>"},{"location":"notes/lecture4/#summary","title":"Summary","text":"<ul> <li>Representation learning eliminates the need for manual feature engineering.</li> <li>Dense embeddings capture semantic, syntactic, and relational properties of data.</li> <li>Word2Vec is a landmark method for learning text representations using CBOW and Skip-gram.</li> <li>Techniques like negative sampling make large-scale training feasible.</li> <li>Graph and image modalities use analogous pipelines to learn meaningful embeddings without heavy labeling.</li> </ul> <p>Representation learning underpins many modern AI systems, from NLP to computer vision and graph learning.</p>"},{"location":"notes/lecture4/#advanced-topics-in-representation-learning","title":"Advanced Topics in Representation Learning","text":""},{"location":"notes/lecture4/#contextual-word-embeddings","title":"Contextual Word Embeddings","text":"<p>Traditional embeddings like Word2Vec assign a single vector per word, regardless of context. However, the same word can mean different things in different contexts:</p> <ul> <li>open a bank account</li> <li>on the river bank**</li> </ul> <p>This motivates contextual embeddings, which adapt to surrounding words.</p>"},{"location":"notes/lecture4/#elmo-embeddings-from-language-models","title":"ELMo (Embeddings from Language Models)","text":"<ul> <li>ELMo uses a 2-layer bidirectional LSTM (biLM) trained as a language model.</li> <li>Unlike Word2Vec, which gives static embeddings, ELMo embeddings are dynamic and depend on the entire sentence context.</li> <li>Produces better representations for polysemous words.</li> </ul> <p>Training Objective:</p> \\[ \\sum_{k=1}^{N} \\Big( \\log p(t_k | t_1, \u2026, t_{k-1}; \\theta_{LSTM}^{\u2192}) + \\log p(t_k | t_{k+1}, \u2026, t_N; \\theta_{LSTM}^{\u2190}) \\Big) \\] <pre><code>flowchart TB\n    E1[Input E1] --&gt; L1[LSTM Forward]\n    E2[Input E2] --&gt; L1\n    E3[Input E3] --&gt; L1\n    E1 --&gt; L2[LSTM Backward]\n    E2 --&gt; L2\n    E3 --&gt; L2\n    L1 --&gt; O1[Contextual Embeddings]\n    L2 --&gt; O1</code></pre>"},{"location":"notes/lecture4/#bert-bidirectional-encoder-representations-from-transformers","title":"BERT (Bidirectional Encoder Representations from Transformers)","text":"<ul> <li>Uses the Transformer architecture with multi-head self-attention.</li> <li>Generates bidirectional contextual embeddings, allowing words to \"see\" both left and right contexts simultaneously.</li> <li>Pretrained on two tasks:</li> <li>Masked Language Modeling (MLM): randomly mask tokens and predict them.</li> <li>Next Sentence Prediction (NSP): predict if sentence B follows sentence A.</li> </ul> <p>Advantages: - Deep bidirectional attention captures richer context than LSTMs. - Strong transfer learning: pretrained weights can be fine-tuned on many downstream NLP tasks.</p> <pre><code>flowchart TB\n    subgraph BERT Pretraining\n      direction TB\n      A[Input Tokens] --&gt; M1[Masked Tokens]\n      M1 --&gt; T[Transformer Layers]\n      T --&gt; MLM[Masked LM Prediction]\n      A2[Sentence A] --&gt; T\n      B2[Sentence B] --&gt; T\n      T --&gt; NSP[Next Sentence Prediction]\n    end</code></pre>"},{"location":"notes/lecture4/#bert-pretraining-tasks","title":"BERT Pretraining Tasks","text":"<ol> <li>Masked Language Model (MLM):</li> <li>Randomly mask 15% of tokens.</li> <li>Of these: 80% replaced with [MASK], 10% replaced with random tokens, 10% unchanged.</li> <li> <p>Example:</p> <ul> <li>Input: the man went to the [MASK] to buy a [MASK] of milk</li> <li>Predictions: <code>store</code>, <code>gallon</code></li> </ul> </li> <li> <p>Next Sentence Prediction (NSP):</p> </li> <li>Sentence A: The man went to the store.</li> <li>Sentence B: He bought a gallon of milk. \u2192 Label = IsNextSentence</li> <li>Sentence B: Penguins are flightless. \u2192 Label = NotNextSentence</li> </ol>"},{"location":"notes/lecture4/#graph-embeddings-why","title":"Graph Embeddings \u2013 Why?","text":"<p>Graph data is ubiquitous: - Knowledge Graphs (search engines, question answering) - Social Graphs (friend/follow recommendations) - Recommendation Systems (products, items, users)</p> <pre><code>flowchart LR\n    A((Alice)) --&gt;|visits| B((Paris))\n    A --&gt;|is interested in| C((Mona Lisa))\n    C --&gt;|is created by| D((Leonardo da Vinci))</code></pre>"},{"location":"notes/lecture4/#graph-basics","title":"Graph Basics","text":"<p>A graph is defined as:</p> <p>[ G = (V, E) ] - \\( V \\): set of vertices (nodes) - \\( E \\subset V \\times V \\): edges (connections)</p> <p>Modalities: - Node embeddings - Edge embeddings - Subgraph embeddings - Whole-graph embeddings</p> <p>Goal: learn embeddings that preserve graph structure while being useful for downstream tasks.</p>"},{"location":"notes/lecture4/#deepwalk","title":"DeepWalk","text":"<ul> <li>Generates random walks on the graph.</li> <li>Treats walks as sentences and applies Skip-gram for training node embeddings.</li> <li>Preserves both local and global graph structures.</li> </ul> <pre><code>flowchart LR\n    subgraph DeepWalk\n      N1((v1)) --&gt; N2((v5)) --&gt; N3((v7)) --&gt; N4((v2))\n    end\n    DeepWalk --&gt; SG[Skip-gram Training]\n    SG --&gt; Emb[Node Embeddings]</code></pre> <p>Algorithm (simplified): 1. Perform multiple random walks per node. 2. Train Skip-gram on sequences of visited nodes. 3. Output low-dimensional node embeddings.</p>"},{"location":"notes/lecture4/#node2vec","title":"Node2Vec","text":"<p>Node2Vec extends DeepWalk with a flexible biased random walk strategy: - Parameters:   - \\(p\\): return parameter (exploration vs revisiting)   - \\(q\\): in-out parameter (BFS vs DFS bias)</p> <ul> <li>If \\(p &gt; 1 &gt; q\\): encourages global exploration.</li> <li>If \\(p &lt; 1 &lt; q\\): encourages local exploration.</li> </ul> <pre><code>flowchart LR\n    U((u)) --&gt;|BFS| S1((s1))\n    U --&gt;|BFS| S2((s2))\n    U --&gt;|DFS| S3((s3))\n    U --&gt;|DFS| S4((s4))</code></pre>"},{"location":"notes/lecture4/#node2vec-community-vs-structural-roles","title":"Node2Vec Community vs Structural Roles","text":"<ul> <li>By tuning \\(p, q\\), Node2Vec can:</li> <li>Capture community structure (nodes in the same cluster).</li> <li>Capture structural equivalence (nodes with similar roles, even in different communities).</li> </ul> <pre><code>flowchart TB\n    subgraph Community Structure\n      A1((Node A)) --- A2((Node B)) --- A3((Node C))\n      A2 --- A4((Node D))\n    end\n    subgraph Structural Equivalence\n      B1((Hub 1)) --- X((Peripheral))\n      B2((Hub 2)) --- X\n    end</code></pre>"},{"location":"notes/lecture4/#image-representations","title":"Image Representations","text":""},{"location":"notes/lecture4/#contrastive-learning","title":"Contrastive Learning","text":"<ul> <li>Goal: learn an embedding space where similar pairs are close and dissimilar pairs are far apart.</li> <li>Given anchor \\(x\\), positive \\(x^+\\), and negative \\(x^-\\):</li> </ul> <p>Contrastive Loss: \\(\\lVert x - x^+ \\rVert^2 + \\max(0, m - \\lVert x - x^- \\rVert^2)\\)</p> <p>Triplet Loss: \\(\\max(0, \\lVert x - x^+ \\rVert^2 - \\lVert x - x^- \\rVert^2 + m)\\)</p> <pre><code>flowchart LR\n    A[Anchor x] --&gt; P[Positive x\u207a]\n    A --&gt; N[Negative x\u207b]\n    P --&gt;|close in embedding| Z[Embedding Space]\n    N --&gt;|far apart| Z</code></pre>"},{"location":"notes/lecture4/#simclr-simple-framework-for-contrastive-learning-of-representations","title":"SimCLR (Simple Framework for Contrastive Learning of Representations)","text":"<ul> <li>Framework for self-supervised learning (SSL) with contrastive loss.</li> <li>Given \\(n\\) images, generate \\(2n\\) augmented samples.</li> <li>For each positive pair, there are \\(2(n-1)\\) negative pairs.</li> </ul> <p>NT-Xent (Normalized Temperature-scaled Cross Entropy) Loss: \\(\\(\\mathcal{L}_{i,j} = - \\log \\frac{\\exp(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k=1}^{2n} 1_{[k \\neq i]} \\exp(\\text{sim}(z_i, z_k)/\\tau)}\\)\\)</p> <ul> <li>Key idea: maximize agreement between positive pairs.</li> <li>Uses strong data augmentations (cropping, flipping, color distortion, Gaussian blur, etc.).</li> </ul> <pre><code>flowchart TB\n    X[\"Input Image\"] --&gt; Aug1[\"Augmentation t\"]\n    X --&gt; Aug2[\"Augmentation t'\"]\n    Aug1 --&gt; F1[\"Encoder f(\u00b7)\"] --&gt; H1[\"Projection Head h(\u00b7)\"] --&gt; Z1[\"Embedding z_i\"]\n    Aug2 --&gt; F2[\"Encoder f(\u00b7)\"] --&gt; H2[\"Projection Head h(\u00b7)\"] --&gt; Z2[\"Embedding z_j\"]\n    Z1 --&gt; Loss[\"Contrastive Loss\"]\n    Z2 --&gt; Loss\n</code></pre> <p>Impact: - Achieved state-of-the-art in SSL, surpassing supervised ResNet-50 on ImageNet Top-1 accuracy.</p>"},{"location":"notes/lecture4/#summary-of-advanced-topics","title":"Summary of Advanced Topics","text":"<ul> <li>ELMo: Contextual embeddings using BiLSTM.</li> <li>BERT: Transformer-based contextual embeddings with MLM and NSP.</li> <li>Graph embeddings: Learn node/edge/subgraph representations preserving structure.</li> <li>DeepWalk: Uses random walks + Skip-gram.</li> <li>Node2Vec: Extends DeepWalk with biased random walks for better control of context.</li> <li>Contrastive Learning &amp; SimCLR: Powerful SSL methods for images, learning embeddings without labels.</li> </ul> <p>These advances form the foundation of modern NLP, graph, and image representation learning.</p>"},{"location":"notes/supervised/","title":"Supervised Learning","text":""}]}