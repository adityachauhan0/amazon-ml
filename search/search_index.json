{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome \ud83d\udc4b","text":"<p>This is my ML notes site.</p>"},{"location":"cheatsheets/","title":"Cheatsheets","text":""},{"location":"notes/lec5/","title":"Sequential Learning \u2014 Sequence Tagging with Hidden Markov Models","text":"<p>About these notes</p> <p>These MkDocs\u2011ready notes were produced from your slide snapshots. They include Mermaid diagrams and LaTeX. We can expand or rearrange sections any time.</p>"},{"location":"notes/lec5/#1-parts-of-speech-pos-tagging","title":"1. Parts of Speech (POS) Tagging","text":"<p>Goal: classify each token in a sentence into a syntactic category (e.g., noun, verb, adjective).</p> <ul> <li>Words are ambiguous and can take multiple POS tags depending on context.</li> <li>Example: the word play</li> <li>As verb: \u201cI like to play cricket.\u201d</li> <li>As noun: \u201cI would like to act in that play.\u201d</li> <li>Disambiguation requires sequence context.</li> </ul>"},{"location":"notes/lec5/#2-problem-formulation-sentence-tagging","title":"2. Problem Formulation (Sentence Tagging)","text":"<p>Given an input token sequence \\(x_{1:N} = (x_1, \\ldots, x_N)\\), predict a label sequence \\(y_{1:N} = (y_1, \\ldots, y_N)\\).</p> <p>Two broad modeling paradigms:</p> <ul> <li>Generative: model the joint \\(p_\\theta(x_{1:N}, y_{1:N})\\) and decode   \\(\\(\\hat{y}_{1:N} = \\arg\\max_{y_{1:N}} p_\\theta(x_{1:N}, y_{1:N}).\\)\\)</li> <li>Discriminative: model the conditional \\(p_\\phi(y_{1:N}\\mid x_{1:N})\\) (or a direct structured predictor) and decode   \\(\\(\\hat{y}_{1:N} = \\arg\\max_{y_{1:N}} p_\\phi(y_{1:N}\\mid x_{1:N}).\\)\\)</li> </ul> <p>Choosing a paradigm</p> <p>Generative models (like HMMs) are simple, often data\u2011efficient, and interpretable; discriminative models (CRF, BiLSTM\u2011CRF, Transformers) can capture richer features.</p>"},{"location":"notes/lec5/#3-generative-models","title":"3. Generative Models","text":"<p>Given training pairs \\((x^{(k)}, y^{(k)})\\) for \\(k = 1, \\dots, K\\), fit a probabilistic model for the joint \\(p(X,Y)\\). At inference, choose the label sequence maximizing the joint probability:</p> \\[\\hat{y} = \\arg\\max_{y} p(x, y).\\] <p>HMMs are the canonical generative model for sequence labeling.</p>"},{"location":"notes/lec5/#4-hidden-markov-models-hmms","title":"4. Hidden Markov Models (HMMs)","text":""},{"location":"notes/lec5/#41-graphical-view","title":"4.1 Graphical view","text":"<pre><code>flowchart LR\n    subgraph Hidden_States\n      y1((y\u2081)) --&gt; y2((y\u2082)) --&gt; y3((y\u2083)) --&gt; y4((\u2026))\n    end\n    x1([x\u2081]) --- y1\n    x2([x\u2082]) --- y2\n    x3([x\u2083]) --- y3\n    x4([x\u2084]) --- y4</code></pre> <ul> <li>Hidden states \\(y_t\\) are POS tags (e.g., det, noun, verb).</li> <li>Observations \\(x_t\\) are words (the, dog, barks).</li> <li>We assume:</li> <li>Markov property: \\(p(y_t\\mid y_{1:t-1}) = p(y_t\\mid y_{t-1})\\).</li> <li>Emission independence: \\(p(x_t\\mid x_{1:t-1}, y_{1:t}) = p(x_t\\mid y_t)\\).</li> </ul>"},{"location":"notes/lec5/#42-generative-story-toy-pos-tagger","title":"4.2 Generative story (toy POS tagger)","text":"<pre><code>flowchart LR\n    Start([Start]) --&gt; det((det))\n    Start --&gt; noun((noun))\n    Start --&gt; verb((verb))\n\n    det --&gt; det\n    det --&gt; noun\n    det --&gt; verb\n\n    noun --&gt; det\n    noun --&gt; noun\n    noun --&gt; verb\n\n    verb --&gt; det\n    verb --&gt; noun\n    verb --&gt; verb\n\n    det -.emit.-&gt; the([\"the\"]) \n    noun -.emit.-&gt; dog([\"dog\"]) \n    verb -.emit.-&gt; barks([\"barks\"]) \n\n    classDef default fill:none,stroke-width:1px</code></pre>"},{"location":"notes/lec5/#43-notation","title":"4.3 Notation","text":"<ul> <li>Vocabulary \\(\\mathcal{V}\\): set of all words.</li> <li>Tag set \\(\\mathcal{S}\\): e.g., \\(\\{\\text{det}, \\text{noun}, \\text{verb}, \\dots\\}\\).</li> <li>Sentence \\(X = x_{1:N}\\) with \\(x_i \\in \\mathcal{V}\\).</li> <li>Tag sequence \\(Y = y_{1:N}\\) with \\(y_i \\in \\mathcal{S}\\).</li> </ul>"},{"location":"notes/lec5/#44-parameters","title":"4.4 Parameters","text":"<p>HMM parameters \\(\\theta = (\\boldsymbol{\\pi}, \\mathbf{A}, \\mathbf{B})\\):</p> <ul> <li>Initial distribution \\(\\boldsymbol{\\pi}\\) with \\(\\pi_{s} = p(y_1 = s)\\).</li> <li>Transition matrix \\(\\mathbf{A}\\) with \\(a_{uv} = p(y_t = v \\mid y_{t-1} = u)\\).</li> <li>Emission probabilities \\(\\mathbf{B}\\) with \\(b_{s,w} = p(x_t = w \\mid y_t = s)\\).</li> </ul> <p>Example (toy numbers from slides):</p> \\(\\pi\\) value det 0.50 noun 0.40 verb 0.10 \\(\\mathbf{A}\\) det noun verb det 0.01 0.99 0.00 noun 0.30 0.30 0.40 verb 0.40 0.40 0.20 \\(\\mathbf{B}\\) the dog barks det 0.40 0.00 0.00 noun 0.00 0.015 0.0031 verb 0.00 0.0004 0.020 <p>Examples: \\(a_{\\text{det},\\text{noun}}=0.99\\), \\(a_{\\text{noun},\\text{verb}}=0.40\\), \\(b_{\\text{det},\\text{the}}=0.40\\), \\(b_{\\text{noun},\\text{barks}}=0.0031\\).</p>"},{"location":"notes/lec5/#45-likelihood-and-factorization","title":"4.5 Likelihood and factorization","text":"<p>For a first\u2011order HMM,</p> \\[ p_\\theta(x_{1:N}, y_{1:N}) = p(y_1)\\, p(x_1\\mid y_1) \\prod_{t=2}^N p(y_t\\mid y_{t-1})\\, p(x_t\\mid y_t). \\] <p>The marginal likelihood over observations is</p> \\[ p_\\theta(x_{1:N}) = \\sum_{y_{1:N}} p_\\theta(x_{1:N}, y_{1:N}). \\]"},{"location":"notes/lec5/#46-inference-tasks","title":"4.6 Inference tasks","text":"<ul> <li>Decoding (Viterbi): \\(\\hat{y}_{1:N} = \\arg\\max_{y_{1:N}} p_\\theta(y_{1:N}\\mid x_{1:N})\\).   Recurrence for the best path score \\(\\delta_t(s)\\) and backpointers \\(\\psi_t(s)\\):   $$   \\begin{aligned}   \\delta_1(s) &amp;= \\log \\pi_s + \\log b_{s,x_1},\\   \\delta_t(s) &amp;= \\max_{u}\\; \\delta_{t-1}(u) + \\log a_{u,s} + \\log b_{s,x_t}.    \\end{aligned}   $$</li> <li>Sequence likelihood (Forward): \\(\\alpha_t(s)\\)   $$   \\alpha_1(s) = \\pi_s\\, b_{s,x_1},\\qquad   \\alpha_t(s) = b_{s,x_t} \\sum_{u} \\alpha_{t-1}(u) a_{u,s}.   $$</li> <li>Posterior decoding (Forward\u2011Backward): \\(\\gamma_t(s) = p(y_t=s\\mid x_{1:N})\\) using \\(\\alpha\\) and \\(\\beta\\).</li> </ul>"},{"location":"notes/lec5/#47-learning-brief","title":"4.7 Learning (brief)","text":"<p>With labeled data, estimate \\(\\pi, A, B\\) by count normalization (MLE) with smoothing (e.g., add\u2011\\(\\lambda\\)).</p> <p>With unlabeled data, use EM/Baum\u2013Welch:</p> <ul> <li>E\u2011step: compute expected counts via forward\u2011backward (\\(\\gamma, \\xi\\)).</li> <li>M\u2011step: renormalize to update \\(\\pi, A, B\\).</li> </ul> <p>Smoothing &amp; OOV</p> <p>Emission smoothing and an UNK token are essential for robust tagging.</p>"},{"location":"notes/lec5/#5-worked-toy-example","title":"5. Worked toy example","text":"<p>Sentence: \u201cthe dog barks\u201d.</p> <p>We illustrate the decoding objective:</p> \\[ \\hat{y}_{1:3} = \\arg\\max_{y_1,y_2,y_3} \\; p(y_1)\\,p(x_1\\mid y_1)\\,p(y_2\\mid y_1)\\,p(x_2\\mid y_2)\\,p(y_3\\mid y_2)\\,p(x_3\\mid y_3). \\] <pre><code>flowchart LR\n    y1((y\u2081)) --&gt; y2((y\u2082)) --&gt; y3((y\u2083))\n    y1 -.-&gt; x1([the])\n    y2 -.-&gt; x2([dog])\n    y3 -.-&gt; x3([barks])\n    classDef default fill:none,stroke-width:1px</code></pre> <p>A plausible best path is det \u2192 noun \u2192 verb, matching the POS intuition.</p>"},{"location":"notes/lec5/#6-summary-next-steps","title":"6. Summary &amp; Next Steps","text":"<ul> <li>Sequence tagging maps \\(x_{1:N} \\to y_{1:N}\\); context resolves ambiguity.</li> <li>HMMs provide a clean generative baseline with efficient dynamic programming for likelihood and decoding.</li> <li>Next, we can add: CRFs, BiLSTM\u2011CRF, transformer\u2011based taggers, and comparisons (accuracy, data efficiency).</li> </ul>"},{"location":"notes/lec5/#appendix-quick-reference","title":"Appendix: Quick reference","text":"<p>Key equations</p> <ul> <li>Joint factorization for HMM:   \\(\\(p(x_{1:N},y_{1:N}) = p(y_1) p(x_1\\mid y_1) \\prod_{t=2}^N p(y_t\\mid y_{t-1}) p(x_t\\mid y_t).\\)\\)</li> <li>Marginal likelihood:   \\(\\(p(x_{1:N}) = \\sum_{y_{1:N}} p(x_{1:N},y_{1:N}).\\)\\)</li> <li>Viterbi recurrence:   \\(\\(\\delta_t(s) = \\max_u \\{\\delta_{t-1}(u) + \\log a_{u,s}\\} + \\log b_{s,x_t}.\\)\\)</li> <li>Forward recurrence:   \\(\\(\\alpha_t(s) = b_{s,x_t} \\sum_u \\alpha_{t-1}(u) a_{u,s}.\\)\\)</li> </ul> <p>Glossary</p> <ul> <li>\\(\\mathcal{V}\\): vocabulary of tokens.</li> <li>\\(\\mathcal{S}\\): set of tags/states.</li> <li>\\(\\pi\\): initial distribution over tags.</li> <li>\\(A\\): transition matrix.</li> <li>\\(B\\): emission probabilities.</li> </ul>"},{"location":"notes/lec5/#hidden-markov-models-ultradetailed-addendum-mkdocs-mermaid-latex","title":"Hidden Markov Models \u2014 Ultra\u2011Detailed Addendum (MkDocs + Mermaid + LaTeX)","text":"<p>Scope</p> <p>This addendum expands your slides on HMM core tasks, Viterbi inference, Forward/Backward, likelihood, and training. It is MkDocs\u2011ready (Material admonitions, Mermaid diagrams, LaTeX math). Fully modular so we can grow it chapter by chapter.</p>"},{"location":"notes/lec5/#1-hmm-three-problems-to-solve","title":"1. HMM: Three Problems to Solve","text":"<p>Working with HMMs generally involves three canonical problems:</p> <ol> <li>Inference / Decoding    Given observations \\(x_{1:T}\\) and HMM parameters \\((\\pi, A, B)\\), return the most probable hidden state sequence \\(\\hat{y}_{1:T}\\).</li> <li>Likelihood    Compute the overall likelihood of the observation sequence: \\(\\(p_\\theta(x_{1:T}) = \\sum_{y_{1:T}} p_\\theta(x_{1:T}, y_{1:T}).\\)\\)</li> <li>Training (Learning)    Given data (labeled or unlabeled), learn \\(\\theta=(\\pi, A, B)\\) that best explains the data.</li> </ol> <p>Notation mapping</p> <p>Slide convention: \\(N=\\text{#states}\\), \\(T=\\text{sequence length}\\). Elsewhere in these notes we sometimes write \\(|\\mathcal{S}|\\) for #states.</p>"},{"location":"notes/lec5/#2-lattice-view-paths-and-complexity","title":"2. Lattice View, Paths, and Complexity","text":"<p>For a length\u2011\\(T\\) sentence and \\(N\\) tags/states, the state lattice has \\(T\\) columns and \\(N\\) nodes per column; edges represent transitions.</p> <pre><code>flowchart LR\n  subgraph t1[ t=1 ]\n    d1((det))\n    n1((noun))\n    v1((verb))\n  end\n  subgraph t2[ t=2 ]\n    d2((det))\n    n2((noun))\n    v2((verb))\n  end\n  subgraph t3[ t=3 ]\n    d3((det))\n    n3((noun))\n    v3((verb))\n  end\n  d1--&gt;d2; d1--&gt;n2; d1--&gt;v2\n  n1--&gt;d2; n1--&gt;n2; n1--&gt;v2\n  v1--&gt;d2; v1--&gt;n2; v1--&gt;v2\n  d2--&gt;d3; d2--&gt;n3; d2--&gt;v3\n  n2--&gt;d3; n2--&gt;n3; n2--&gt;v3\n  v2--&gt;d3; v2--&gt;n3; v2--&gt;v3</code></pre> <ul> <li>#Sequences: \\(N^T\\) (e.g., 3 tags over 3 tokens \u21d2 \\(3^3=27\\) sequences).  </li> <li>Dynamic programming avoids enumeration: inference/likelihood in \\(\\mathcal{O}(TN^2)\\).</li> </ul>"},{"location":"notes/lec5/#3-viterbi-algorithm-map-path","title":"3. Viterbi Algorithm \u2014 MAP Path","text":"<p>We seek \\(\\hat{y}_{1:T} = \\arg\\max_{y_{1:T}} p(y_{1:T}\\mid x_{1:T})\\); since \\(p(x_{1:T})\\) is constant wrt \\(y_{1:T}\\), equivalently maximize the joint \\(p(y_{1:T},x_{1:T})\\).</p>"},{"location":"notes/lec5/#31-recurrence-probability-domain","title":"3.1 Recurrence (probability domain)","text":"<p>Let \\(\\delta_t(s)\\) be the best path probability ending at state \\(s\\) at time \\(t\\), and \\(\\psi_t(s)\\) the backpointer:</p> \\[ \\begin{aligned} \\delta_1(s) &amp;= \\pi_s\\, b_{s,x_1},\\\\ \\delta_t(s) &amp;= b_{s,x_t}\\, \\max_{u\\in\\mathcal{S}} \\delta_{t-1}(u)\\, a_{u,s},\\\\ \\psi_t(s) &amp;= \\arg\\max_{u\\in\\mathcal{S}} \\delta_{t-1}(u)\\, a_{u,s}. \\end{aligned} \\] <p>Termination: \\(\\hat{y}_T = \\arg\\max_s \\delta_T(s)\\) Backtrace: \\(\\hat{y}_{t-1} = \\psi_t(\\hat{y}_t)\\).</p>"},{"location":"notes/lec5/#32-logdomain-numerically-stable","title":"3.2 Log\u2011domain (numerically stable)","text":"<p>Define \\(\\Delta_t(s) = \\log \\delta_t(s)\\):</p> \\[ \\Delta_1(s)=\\log\\pi_s+\\log b_{s,x_1},\\qquad \\Delta_t(s)=\\log b_{s,x_t}+\\max_u\\{\\Delta_{t-1}(u)+\\log a_{u,s}\\}. \\]"},{"location":"notes/lec5/#33-worked-example-pos-the-dog-barks","title":"3.3 Worked example (POS: the dog barks)","text":"<p>Parameters from your slide tables (\\(\\pi, A, B\\) over tags det/noun/verb and words the/dog/barks):</p> <ul> <li>Init (t=1, \"the\") \\(\\delta_1(\\text{det})=0.5\\cdot0.40=0.20\\), \\(\\delta_1(\\text{noun})=0\\), \\(\\delta_1(\\text{verb})=0\\).</li> <li>t=2 (\"dog\") \\(\\delta_2(\\text{noun})=0.015\\cdot\\max\\{0.20\\cdot0.99,0,0\\}=\\mathbf{0.00297}\\); others \\(\\approx0\\).   Backpointer: \\(\\psi_2(\\text{noun})=\\text{det}\\).</li> <li>t=3 (\"barks\") \\(\\delta_3(\\text{verb})=0.020\\cdot\\max\\{0,\\;0.00297\\cdot0.40,\\;0\\}=\\mathbf{2.376\\times10^{-5}}\\); \\(\\delta_3(\\text{noun})=0.0031\\cdot\\max\\{0,\\;0.00297\\cdot0.30,\\;0\\}=2.7621\\times10^{-6}\\).</li> </ul> <p>Decoded path: det \u2192 noun \u2192 verb.</p> <pre><code>flowchart LR\n  det1((det)) --&gt; noun2((noun)) --&gt; verb3((verb))\n  the([the]) -.-&gt; det1\n  dog([dog]) -.-&gt; noun2\n  barks([barks]) -.-&gt; verb3\n  classDef default fill:none,stroke-width:1px</code></pre> <p>Viterbi vs. marginal argmax</p> <p>Viterbi gives the single best sequence. Tagging each position with \\(\\arg\\max_s p(y_t=s\\mid x_{1:T})\\) can yield a different (and suboptimal) global path.</p>"},{"location":"notes/lec5/#4-likelihood-via-the-forward-algorithm","title":"4. Likelihood via the Forward Algorithm","text":"<p>We want \\(p_\\theta(x_{1:T})\\) without enumerating \\(N^T\\) sequences.</p>"},{"location":"notes/lec5/#41-forward-recurrences","title":"4.1 Forward recurrences","text":"<p>Let \\(\\alpha_t(s) = p(x_{1:t}, y_t=s)\\):</p> \\[ \\alpha_1(s)=\\pi_s\\,b_{s,x_1},\\qquad \\alpha_t(s)=b_{s,x_t}\\sum_{u}\\alpha_{t-1}(u) a_{u,s}. \\] <p>Likelihood: \\(p_\\theta(x_{1:T}) = \\sum_s \\alpha_T(s)\\).</p>"},{"location":"notes/lec5/#42-complexity","title":"4.2 Complexity","text":"<ul> <li>Na\u00efve: \\(\\mathcal{O}(N^T)\\) (impractical).  </li> <li>Forward DP: \\(\\mathcal{O}(TN^2)\\) time; memory \\(\\mathcal{O}(TN)\\) (or \\(\\mathcal{O}(N)\\) streaming if no backpointers are needed).</li> </ul>"},{"location":"notes/lec5/#43-numeric-check-same-toy-example","title":"4.3 Numeric check (same toy example)","text":"<p>\\(\\alpha_1=(0.20,0,0)\\); \\(\\alpha_2(\\text{noun})=0.00297\\). \\(\\alpha_3(\\text{verb})=2.376\\times10^{-5}\\); \\(\\alpha_3(\\text{noun})=2.7621\\times10^{-6}\\); hence \\(\\boxed{p(x)=2.652\\times10^{-5}}\\) and \\(\\log p(x)\\approx-10.538\\).</p>"},{"location":"notes/lec5/#44-scaling-stability","title":"4.4 Scaling / stability","text":"<ul> <li>Scaled forward: define \\(c_t = \\big(\\sum_s \\tilde{\\alpha}_t(s)\\big)^{-1}\\) and set \\(\\alpha_t(s)=c_t\\tilde{\\alpha}_t(s)\\) so \\(\\sum_s \\alpha_t(s)=1\\). Then \\(\\log p(x_{1:T})=-\\sum_{t=1}^T \\log c_t\\).</li> <li>Log\u2011space: maintain \\(\\log\\alpha_t(s)\\) using <code>logsumexp</code>.</li> </ul>"},{"location":"notes/lec5/#5-backward-algorithm-and-posteriors","title":"5. Backward Algorithm and Posteriors","text":"<p>Define \\(\\beta_t(s)=p(x_{t+1:T}\\mid y_t=s)\\) with</p> \\[ \\beta_T(s)=1,\\qquad \\beta_t(s)=\\sum_{v} a_{s,v}\\, b_{v,x_{t+1}}\\, \\beta_{t+1}(v). \\] <p>Posterior quantities:</p> \\[ \\gamma_t(s)=\\frac{\\alpha_t(s)\\,\\beta_t(s)}{\\sum_j \\alpha_t(j)\\,\\beta_t(j)},\\qquad \\xi_t(u,v)=\\frac{\\alpha_t(u)\\,a_{u,v}\\,b_{v,x_{t+1}}\\,\\beta_{t+1}(v)}{\\sum_{i,j} \\alpha_t(i)\\,a_{i,j}\\,b_{j,x_{t+1}}\\,\\beta_{t+1}(j)}. \\] <p>These enable posterior decoding and Baum\u2013Welch (EM).</p>"},{"location":"notes/lec5/#6-training-supervised-em","title":"6. Training (Supervised &amp; EM)","text":""},{"location":"notes/lec5/#61-supervised-mle-labeled-sequences","title":"6.1 Supervised MLE (labeled sequences)","text":"<p>Counts \u2192 normalized probabilities (with smoothing):</p> \\[ \\hat{\\pi}_s = \\operatorname{norm}_1\\big(\\text{count}(y_1=s)\\big),\\quad \\hat{a}_{u,v} = \\operatorname{norm}_1\\big(\\text{count}(y_{t-1}=u, y_t=v)\\big),\\quad \\hat{b}_{s,w} = \\operatorname{norm}_1\\big(\\text{count}(y_t=s, x_t=w)\\big). \\]"},{"location":"notes/lec5/#62-unlabeled-data-baumwelch-em","title":"6.2 Unlabeled data \u2014 Baum\u2013Welch (EM)","text":"<ul> <li>E\u2011step: compute expected counts with forward\u2013backward: \\(\\bar{\\pi}_s\\leftarrow\\gamma_1(s)\\), \\(\\bar{a}_{u,v}\\leftarrow\\sum_{t=1}^{T-1}\\xi_t(u,v)\\), \\(\\bar{b}_{s,w}\\leftarrow\\sum_{t=1}^{T}\\mathbb{1}[x_t=w]\\,\\gamma_t(s)\\).</li> <li>M\u2011step: renormalize rows/PMFs to update \\(\\pi, A, B\\) (plus add\u2011\\(\\lambda\\) smoothing).</li> </ul> <p>Smoothing &amp; OOV</p> <p>Use add\u2011\\(\\lambda\\) smoothing and an UNK token (or character/feature\u2011based emissions) for robustness.</p>"},{"location":"notes/lec5/#7-pseudocode-copypaste-ready","title":"7. Pseudocode (copy\u2011paste ready)","text":"<pre><code>Viterbi(x[1..T], \u03c0, A, B):\n  for s in S: \u03b4[1,s] = \u03c0[s]*B[s,x1]; \u03c8[1,s]=NULL\n  for t=2..T:\n    for s in S:\n      (best, arg)=max_u (\u03b4[t-1,u]*A[u,s])\n      \u03b4[t,s] = B[s,xt]*best\n      \u03c8[t,s] = arg\n  y\u0302_T = argmax_s \u03b4[T,s]\n  for t=T..2: y\u0302_{t-1} = \u03c8[t, y\u0302_t]\n  return y\u0302\n\nForward(x[1..T], \u03c0, A, B):\n  for s in S: \u03b1[1,s] = \u03c0[s]*B[s,x1]\n  for t=2..T:\n    for s in S: \u03b1[t,s] = B[s,xt] * \u03a3_u \u03b1[t-1,u]*A[u,s]\n  return \u03a3_s \u03b1[T,s]\n\nBackward(x[1..T], \u03c0, A, B):\n  for s in S: \u03b2[T,s] = 1\n  for t=T-1..1:\n    for s in S: \u03b2[t,s] = \u03a3_v A[s,v]*B[v,x_{t+1}]*\u03b2[t+1,v]\n</code></pre>"},{"location":"notes/lec5/#8-implementation-notes-edge-cases","title":"8. Implementation Notes &amp; Edge Cases","text":"<ul> <li>Start/End states: optionally include explicit START/END; easiest is to fold START into \\(\\pi\\) and END into a final termination step.</li> <li>Forbidden transitions: set \\(a_{u,v}=0\\) (or \\(-\\infty\\) in log\u2011space); Viterbi will ignore them.</li> <li>Beam search: prune states with low partial scores to speed decoding at a small accuracy cost.</li> <li>Higher\u2011order HMMs: 2nd\u2011order uses \\(p(y_t\\mid y_{t-1},y_{t-2})\\); cost \\(\\mathcal{O}(TN^3)\\).</li> <li>HSMMs (semi\u2011Markov): model explicit durations; useful for long spans.</li> <li>Evaluation: token accuracy, sentence accuracy, per\u2011tag precision/recall, confusion matrices. Compare to trivial baselines (majority tag, suffix memorization).</li> </ul>"},{"location":"notes/lec5/#9-quick-reference","title":"9. Quick Reference","text":"<ul> <li>Joint factorization \\(\\(p(x_{1:T},y_{1:T}) = p(y_1) p(x_1\\mid y_1) \\prod_{t=2}^{T} p(y_t\\mid y_{t-1}) p(x_t\\mid y_t).\\)\\)</li> <li>Likelihood \\(\\(p(x_{1:T}) = \\sum_{y_{1:T}} p(x_{1:T},y_{1:T}).\\)\\)</li> <li>Viterbi \\(\\(\\delta_t(s) = b_{s,x_t} \\max_u \\delta_{t-1}(u) a_{u,s},\\quad \\psi_t(s)=\\arg\\max_u \\delta_{t-1}(u) a_{u,s}.\\)\\)</li> <li>Forward \\(\\(\\alpha_t(s) = b_{s,x_t} \\sum_u \\alpha_{t-1}(u) a_{u,s}.\\)\\)</li> <li>Backward \\(\\(\\beta_t(s) = \\sum_v a_{s,v} b_{v,x_{t+1}} \\beta_{t+1}(v).\\)\\)</li> <li>Posteriors \\(\\(\\gamma_t(s) = \\frac{\\alpha_t(s)\\beta_t(s)}{\\sum_j \\alpha_t(j)\\beta_t(j)},\\quad \\xi_t(u,v) = \\frac{\\alpha_t(u)a_{u,v}b_{v,x_{t+1}}\\beta_{t+1}(v)}{\\sum_{i,j} \\alpha_t(i)a_{i,j}b_{j,x_{t+1}}\\beta_{t+1}(j)}.\\)\\)</li> </ul>"},{"location":"notes/lec5/#hidden-markov-models-hmm-em-posteriors-applications-in-speech","title":"Hidden Markov Models (HMM) \u2014 EM Posteriors &amp; Applications in Speech","text":"<p>This document explains the Expectation\u2013Maximization (EM, Baum\u2013Welch) training procedure for HMMs, with posterior definitions, update rules, and applications in speech recognition (ASR). Includes multiple Mermaid diagrams for structure and intuition.</p>"},{"location":"notes/lec5/#1-objective-1-initial-distribution","title":"1. Objective 1 \u2014 Initial Distribution","text":"<p>The initial state probabilities \\(\\pi_s\\) are updated using the posterior probability of being in state \\(s\\) at time \\(t=1\\):</p> \\[ \\pi_s \\leftarrow \\gamma_1(s), \\qquad \\sum_s \\pi_s = 1. \\] <ul> <li>Intuition: the best estimate of how often each state is used to start a sequence.  </li> <li>Normalized automatically since \\(\\sum_s \\gamma_1(s)=1\\).</li> </ul>"},{"location":"notes/lec5/#2-objective-2-transition-matrix-a","title":"2. Objective 2 \u2014 Transition Matrix \\(A\\)","text":"<p>We need the posterior of two adjacent states at time \\(t-1\\) and \\(t\\):</p> \\[ p(y_{t-1}=s_i, y_t=s_m \\mid x_{1:T}) \\propto  \\alpha_{t-1}(s_i)\\, a_{s_i,s_m}\\, b_{s_m,x_t}\\, \\beta_t(s_m). \\] <p>Normalizing:</p> \\[ \\xi_t(s_i,s_m) = \\frac{\\alpha_{t-1}(s_i)\\, a_{s_i,s_m}\\, b_{s_m,x_t}\\, \\beta_t(s_m)}{\\sum_{u,v}\\alpha_{t-1}(u)\\, a_{u,v}\\, b_{v,x_t}\\, \\beta_t(v)}. \\] <p>Transition update:</p> \\[ a_{s_i,s_m} \\leftarrow \\frac{\\sum_{t=2}^T \\xi_t(s_i,s_m)}{\\sum_{t=2}^T \\sum_v \\xi_t(s_i,v)}. \\] <ul> <li>Numerator = expected number of transitions \\(s_i \\to s_m\\) </li> <li>Denominator = expected number of transitions leaving \\(s_i\\) </li> <li>Each row of \\(A\\) is renormalized to sum to 1.</li> </ul>"},{"location":"notes/lec5/#3-objective-3-emission-matrix-b","title":"3. Objective 3 \u2014 Emission Matrix \\(B\\)","text":"<p>The state occupancy posterior is:</p> \\[ \\gamma_t(s) = p(y_t=s \\mid x_{1:T}) = \\frac{\\alpha_t(s)\\, \\beta_t(s)}{\\sum_j \\alpha_t(j)\\, \\beta_t(j)}. \\] <p>Emission update (discrete symbols):</p> \\[ b_{s,v_k} \\leftarrow  \\frac{\\sum_{t=1}^T \\mathbf{1}[x_t=v_k]\\, \\gamma_t(s)}{\\sum_{t=1}^T \\gamma_t(s)}. \\] <ul> <li>Numerator = expected number of times in state \\(s\\) emitting symbol \\(v_k\\) </li> <li>Denominator = expected number of times in state \\(s\\) </li> </ul> <p>For continuous features (speech), \\(b_s(x)\\) is parameterized as a PDF (e.g., Gaussian or GMM). Updates use weighted maximum-likelihood with \\(\\gamma_t(s)\\) as responsibilities.</p>"},{"location":"notes/lec5/#4-em-objective-function","title":"4. EM Objective Function","text":"<p>The EM auxiliary function:</p> \\[ Q(\\theta,\\theta_i) = \\mathbb{E}_{y_{1:T}\\sim p(\\cdot\\mid x_{1:T},\\theta_i)}\\Big[ \\log p_\\theta(y_1) + \\sum_{t=2}^T \\log p_\\theta(y_t \\mid y_{t-1}) + \\sum_{t=1}^T \\log p_\\theta(x_t \\mid y_t)\\Big]. \\] <p>This decomposes naturally into: - Initial distribution (\\(\\pi\\)) - Transition matrix (\\(A\\)) - Emission distributions (\\(B\\))</p> <p>Each has a closed-form update (the ones above).</p>"},{"location":"notes/lec5/#5-applications-in-speech-recognition-asr","title":"5. Applications in Speech Recognition (ASR)","text":"<p>The decoding objective:</p> \\[ W^* = \\arg\\max_W\\; p(X \\mid W)\\, p(W), \\] <p>where \\(X = x_{1:T}\\) is the feature sequence (MFCC / FBANK).</p> <ul> <li>Acoustic model (HMM):   States correspond to phones or sub-phones, with self-loops \\(a_{s,s}\\) for duration modeling and forward transitions \\(a_{s,s'}\\) for progression.  </li> <li>Emissions: \\(b_s(x)\\) are PDFs of feature vectors.  </li> <li>Classical: GMM-HMM </li> <li>Modern hybrid: DNN outputs as state posteriors, combined with HMM structure.  </li> <li>Language model (LM): \\(p(W)\\) encodes word sequence probability.  </li> <li>Decoding: Viterbi / beam search over HMM graph + lexicon + LM.</li> </ul>"},{"location":"notes/lec5/#6-mermaid-diagrams","title":"6. Mermaid Diagrams","text":""},{"location":"notes/lec5/#61-speech-recognition-pipeline","title":"6.1 Speech recognition pipeline","text":"<pre><code>flowchart LR\n  Audio[\"Audio waveform\"]\n  Feats[\"Feature extraction (MFCC / FBANK)\"]\n  HMM[\"Acoustic HMM (states, a(u,v))\"]\n  Decode[\"Viterbi / Beam search\"]\n  LM[\"Language model p(W)\"]\n  Words[\"Best hypothesis W*\"]\n\n  Audio --&gt; Feats --&gt; HMM --&gt; Decode --&gt; Words\n  LM --&gt; Decode</code></pre>"},{"location":"notes/lec5/#62-hmm-topology-one-phone-left-to-right-with-self-loops","title":"6.2 HMM topology (one phone, left-to-right with self-loops)","text":"<pre><code>flowchart LR\n  Start([Start]) --&gt; S1\n\n  subgraph Phone_HMM\n    direction LR\n    S1[\"State s1\"] --&gt;|forward| S2[\"State s2\"]\n    S2 --&gt;|forward| S3[\"State s3\"]\n    S1 -- \"self loop a(s1,s1)\" --&gt; S1\n    S2 -- \"self loop a(s2,s2)\" --&gt; S2\n    S3 -- \"self loop a(s3,s3)\" --&gt; S3\n  end\n\n  S3 --&gt; End([End])\n</code></pre>"},{"location":"notes/lec5/#63-forwardbackward-message-passing-per-time-step","title":"6.3 Forward\u2013Backward message passing (per time step)","text":"<pre><code>flowchart LR\n  X1[\"x1\"] --&gt; T1[\"Time 1\"]\n  X2[\"x2\"] --&gt; T2[\"Time 2\"]\n  X3[\"x3\"] --&gt; T3[\"Time 3\"]\n  X4[\"x4\"] --&gt; T4[\"Time 4\"]\n\n  T1 --&gt;|alpha forward| T2 --&gt;|alpha| T3 --&gt;|alpha| T4\n  T4 -.-&gt;|beta backward| T3 -.-&gt;|beta| T2 -.-&gt;|beta| T1</code></pre>"},{"location":"notes/lec5/#64-em-dependency-map-what-feeds-what","title":"6.4 EM dependency map (what feeds what)","text":"<pre><code>flowchart TB\n  X[\"Observations x(1:T)\"] --&gt; Alpha[\"Compute alpha\"]\n  X --&gt; Beta[\"Compute beta\"]\n  Alpha --&gt; Gamma[\"State posteriors gamma(t,s)\"]\n  Beta  --&gt; Gamma\n  Alpha --&gt; Xi[\"Transition posteriors xi(t,i,m)\"]\n  Beta  --&gt; Xi\n\n  Gamma --&gt; Pi[\"Update initial pi\"]\n  Gamma --&gt; B[\"Update emissions B\"]\n  Xi --&gt; A[\"Update transitions A\"]</code></pre>"},{"location":"notes/lec5/#7-quick-reference","title":"7. Quick Reference","text":"<ul> <li> <p>Forward probability: \\(\\alpha_t(s)\\)</p> </li> <li> <p>Backward probability: \\(\\beta_t(s)\\)</p> </li> <li> <p>Two-state posterior: \\(\\xi_t(s_i,s_m)\\)</p> </li> <li> <p>Single-state posterior: \\(\\gamma_t(s)\\)</p> </li> <li> <p>Updates:</p> <ul> <li> <p>\\(\\pi_s \\leftarrow \\gamma_1(s)\\)</p> </li> <li> <p>\\(a_{s_i,s_m} \\leftarrow \\dfrac{\\sum_t \\xi_t(s_i,s_m)}{\\sum_t \\sum_v \\xi_t(s_i,v)}\\)</p> </li> <li> <p>\\(b_{s,v_k} \\leftarrow \\dfrac{\\sum_t \\mathbf{1}[x_t=v_k] \\gamma_t(s)}{\\sum_t \\gamma_t(s)}\\)</p> </li> </ul> </li> <li> <p>Constraints: Row-stochastic normalization, clamp small negatives to \\(0\\).</p> </li> <li> <p>Speech link: HMM + Emission PDFs + LM \u2192 decoding.</p> </li> </ul> <p>TL;DR</p> <p>Goal. Move from recurrent sequence\u2011to\u2011sequence models to attention and finally to Transformers.</p> <p>Key ideas. - Seq2Seq = Encoder\u2013Decoder + teacher forcing during training. - Attention learns alignment scores to build a context vector from all encoder states. - Bahdanau (additive) vs Luong (multiplicative) attention differ in the scoring function. - RNNs suffer from long interaction distance and poor parallelism. - Transformer replaces recurrence with self\u2011attention + positional encoding and parallelizable blocks.</p>"},{"location":"notes/lec5/#1-sequencetosequence-seq2seq","title":"1) Sequence\u2011to\u2011Sequence (Seq2Seq)","text":"<p>Architecture. An encoder RNN consumes the source sequence and produces hidden states {h\u2081,\u2026,h_T}. A decoder RNN generates the target tokens step by step.</p> <p>Teacher Forcing</p> <p>During training, the decoder receives the ground\u2011truth token y_{t\u22121} as input at step t instead of its own previous prediction \u0177_{t\u22121}. This stabilizes and accelerates learning but creates exposure bias at inference.</p> <p>Autoregressive decoding. \\(p(\\mathbf{y}|\\mathbf{x})=\\prod_{t=1}^{U} p\\big(y_t\\mid y_{&lt;t},\\mathbf{x}\\big)\\) (cross\u2011entropy loss).</p> <pre><code>flowchart LR\n    subgraph ENC[Encoder RNN]\n      x1((x1))--&gt;h1[h1]; h1--&gt;h2[h2]; h2--&gt;h3[h3]; h3--&gt;hT[hT]\n    end\n    subgraph DEC[Decoder RNN]\n      y0((&lt;Start&gt;))--&gt;s1[s1]; s1--&gt;s2[s2]; s2--&gt;s3[s3]; s3--&gt;sU[sU]\n    end\n    hT-- context --&gt;s1\n    s1--&gt;y1((y1)); s2--&gt;y2((y2)); s3--&gt;y3((y3))</code></pre> <p>Slide example: \u201cI speak French\u201d \u2192 \u201cJe parle fran\u00e7ais\u201d.</p>"},{"location":"notes/lec5/#2-why-attention","title":"2) Why Attention?","text":"<p>Issues with recurrent models</p> <p>Linear interaction distance. Dependencies must flow left\u2192right (or right\u2192left), making long\u2011range reference hard to capture (e.g., pronoun \u2194 antecedent across many tokens).</p> <p>Lack of parallelizability. The time dimension is inherently serial: step t waits for t\u22121.</p> <p>Core idea. Compute a context for each decoding step as a weighted sum of all encoder states, with weights indicating relevance to the current decoder state.</p> \\[ \\mathbf{c}_t = \\sum_{i=1}^{T} \\alpha_{t,i}\\,\\mathbf{h}_i, \\qquad \\alpha_{t,i} = \\operatorname{softmax}_i\\big( e(\\mathbf{s}_{t-1},\\mathbf{h}_i) \\big) \\] <pre><code>sequenceDiagram\n    autonumber\n    participant Enc as Encoder states h1..hT\n    participant Dec as Decoder state s(t-1)\n    participant Att as Attention\n    Enc-&gt;&gt;Att: all hidden states\n    Dec-&gt;&gt;Att: query s(t-1)\n    Att--&gt;&gt;Dec: context c_t = \u03a3 \u03b1_{t,i} h_i\n    Dec--&gt;&gt;Dec: produce y_t</code></pre>"},{"location":"notes/lec5/#3-bahdanau-additive-attention","title":"3) Bahdanau (Additive) Attention","text":"<p>Scoring (additive MLP).</p> \\[  e_{t,i} = \\mathbf{v}_a^{\\top} \\tanh\\!\\big( \\mathbf{W}_s\\,\\mathbf{s}_{t-1} + \\mathbf{W}_h\\,\\mathbf{h}_i \\big),\\quad  \\alpha_{t,i} = \\operatorname{softmax}_i(e_{t,i}),\\quad  \\mathbf{c}_t = \\sum_i \\alpha_{t,i}\\,\\mathbf{h}_i. \\] <p>The decoder then consumes \\([\\mathbf{s}_{t-1};\\mathbf{c}_t]\\) to produce \\(\\mathbf{s}_t\\) and output \\(y_t\\).</p> <p>When additive helps</p> <p>Useful when query/key dimensions differ or when an extra learned nonlinearity improves alignment.</p>"},{"location":"notes/lec5/#4-luong-multiplicative-attention","title":"4) Luong (Multiplicative) Attention","text":"<p>Let \\(\\mathbf{s}_t\\) be the decoder state and \\(\\mathbf{h}_i\\) an encoder state. Three alignment scoring functions from the slide:</p> <ul> <li>Dot: \\(\\;\\text{score}(\\mathbf{s}_t,\\mathbf{h}_i) = \\mathbf{s}_t^{\\top}\\mathbf{h}_i\\)</li> <li>General: \\(\\;\\text{score}(\\mathbf{s}_t,\\mathbf{h}_i) = \\mathbf{s}_t^{\\top}\\mathbf{W}\\,\\mathbf{h}_i\\)</li> <li>Concat: \\(\\;\\text{score}(\\mathbf{s}_t,\\mathbf{h}_i) = \\mathbf{v}^{\\top}\\tanh\\!\\big(\\mathbf{W}[\\mathbf{s}_t;\\mathbf{h}_i]\\big)\\)</li> </ul> <p>Then \\(\\alpha_{t,i}=\\operatorname{softmax}_i(\\text{score})\\) and \\(\\mathbf{c}_t = \\sum_i \\alpha_{t,i}\\,\\mathbf{h}_i\\).</p> <p>Bahdanau vs. Luong</p> <p>Additive \u2248 small MLP; flexible but slightly slower. Multiplicative \u2248 dot\u2011products; fast, especially when state sizes match.</p>"},{"location":"notes/lec5/#5-from-rnnattention-to-the-transformer","title":"5) From RNN+Attention to the Transformer","text":"<p>Even with attention, RNNs decode sequentially. Transformers remove recurrence and apply self\u2011attention in parallel.</p> <pre><code>flowchart LR\n    subgraph Transformer\n      direction LR\n      PE[Positional Encoding]--&gt;E[Embedding]\n      E--&gt;EncBlock1[Encoder Block \u00d7 N]\n      EncBlock1--&gt;EncBlockN\n      EncBlockN-- cross-attn --&gt;DecBlock1[Decoder Block \u00d7 N]\n      DecBlock1--&gt;DecBlockN\n      DecBlockN--&gt;LM[Linear + Softmax]\n    end</code></pre> <p>Encoder block (per layer). 1. Multi\u2011Head Self\u2011Attention on input X  \u2192 Add &amp; LayerNorm 2. Feed\u2011Forward (two linear layers with activation) \u2192 Add &amp; LayerNorm</p> <p>Decoder block adds Masked Self\u2011Attention before Encoder\u2013Decoder Attention to prevent peeking ahead.</p>"},{"location":"notes/lec5/#6-selfattention-mechanics","title":"6) Self\u2011Attention Mechanics","text":"<p>Given input matrix \\(X\\in\\mathbb{R}^{T\\times d_{model}}\\):</p> \\[ Q = X\\,W^Q,\\quad K = X\\,W^K,\\quad V = X\\,W^V,\\qquad \\operatorname{Attention}(Q,K,V) = \\operatorname{softmax}\\!\\left( \\frac{QK^{\\top}}{\\sqrt{d_k}} \\right) V. \\] <p>Multi\u2011Head. Split into \\(h\\) heads and concatenate results: \\(\\operatorname{MHA}(X)=\\operatorname{Concat}(\\text{head}_1,\\ldots,\\text{head}_h)W^O\\).</p> <p>Positional encoding. (sinusoidal)</p> \\[ \\mathrm{PE}_{(pos,2i)}=\\sin\\!\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right),\\quad \\mathrm{PE}_{(pos,2i+1)}=\\cos\\!\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right). \\] <p>Masks. Decoder uses a causal mask so position t cannot attend to &gt; t.</p>"},{"location":"notes/lec5/#7-tabs-alignment-context-output","title":"7) Tabs: Alignment \u2192 Context \u2192 Output","text":"Additive (Bahdanau)Multiplicative (Luong)Decoder step \\[ e_{t,i} = \\mathbf{v}_a^{\\top}\\tanh(\\mathbf{W}_s\\mathbf{s}_{t-1}+\\mathbf{W}_h\\mathbf{h}_i),\\qquad \\alpha_{t,i}=\\operatorname{softmax}_i(e_{t,i}). \\] \\[ \\alpha_{t,i}=\\operatorname{softmax}_i\\big(\\mathbf{s}_t^{\\top}\\mathbf{W}\\,\\mathbf{h}_i\\big),\\qquad \\mathbf{c}_t=\\sum_i\\alpha_{t,i}\\,\\mathbf{h}_i. \\] \\[ \\tilde{\\mathbf{s}}_t=\\tanh(\\mathbf{W}_c[\\mathbf{c}_t;\\mathbf{s}_t]),\\quad p(y_t\\mid\\cdot)=\\operatorname{softmax}(\\mathbf{W}_o\\tilde{\\mathbf{s}}_t). \\]"},{"location":"notes/lec5/#8-mermaid-visuals","title":"8) Mermaid Visuals","text":"<pre><code>flowchart LR\n  E1[\"h1\"]\n  E2[\"h2\"]\n  E3[\"h3\"]\n  E4[\"hT\"]\n  S0[\"decoder s(t-1)\"]\n  ATT((Attention))\n  S1[\"s_t\"]\n  Y[\"y_t\"]\n\n  E1 --&gt; E2 --&gt; E3 --&gt; E4\n  S0 --|query|--&gt; ATT\n  E1 --|key/value|--&gt; ATT\n  E2 --|key/value|--&gt; ATT\n  E3 --|key/value|--&gt; ATT\n  E4 --|key/value|--&gt; ATT\n  ATT --|context c_t|--&gt; S1 --&gt; Y\n</code></pre> <pre><code>flowchart LR\n  subgraph EncoderBlock\n    X[Input X]--&gt;MHA[Multi-Head Self-Attn]\n    MHA--&gt;Add1[+ Residual]--&gt;LN1[LayerNorm]\n    LN1--&gt;FFN[Feed-Forward]\n    FFN--&gt;Add2[+ Residual]--&gt;LN2[LayerNorm]\n  end</code></pre>"},{"location":"notes/lec5/#9-practical-notes","title":"9) Practical Notes","text":"<p>Training</p> <ul> <li>Teacher forcing with scheduled sampling can mitigate exposure bias.</li> <li>Label smoothing (e.g., \\(\\epsilon=0.1\\)) improves generalization for Transformers.</li> </ul> <p>Scaling</p> <ul> <li>Choose head sizes so \\(d_k=d_v=\\tfrac{d_{model}}{h}\\).</li> <li>Use warmup LR schedules: \\(\\text{lr} \\propto d_{model}^{-0.5}\\min(t^{-0.5}, t\\,\\text{warmup}^{-1.5})\\).</li> </ul> <p>Pitfalls</p> <ul> <li>Mismatched query/key dimensions.</li> <li>Forgetting causal masks in the decoder (information leakage).</li> </ul>"},{"location":"notes/lec5/#10-keywords-no-term-left-behind","title":"10) Keywords (no term left behind)","text":"<p><code>seq2seq, encoder, decoder, teacher forcing, context vector, attention weights, alignment scores, Bahdanau attention, additive attention, Luong attention, multiplicative attention, dot score, general score, concat score, exposure bias, RNN bottleneck, linear interaction distance, parallelizability, self-attention, multi-head attention, queries, keys, values, scaled dot-product, positional encoding, causal mask, encoder\u2013decoder attention, feed-forward network, residual connection, layer normalization, cross-entropy, beam search</code></p>"},{"location":"notes/lec5/#11-references-attributions","title":"11) References &amp; Attributions","text":"<ul> <li>Diagrams inspired by: Attention Mechanism (FloydHub blog).  </li> <li>Transformer visuals inspired by: The Illustrated Transformer by Jay Alammar.</li> </ul>"},{"location":"notes/lec5/#appendix-alignment-scoring-from-slides","title":"Appendix \u2014 Alignment Scoring (from slides)","text":"<ul> <li>Dot: \\(\\;\\text{score}=\\mathbf{H}_{enc}^{\\top}\\mathbf{H}_{dec}\\)</li> <li>General: \\(\\;\\text{score}=\\mathbf{H}_{dec}^{\\top}\\mathbf{W}\\,\\mathbf{H}_{enc}\\)</li> <li>Concat: \\(\\;\\text{score}=\\mathbf{v}^{\\top}\\tanh\\big(\\mathbf{W}\\,[\\mathbf{H}_{enc};\\mathbf{H}_{dec}]\\big)\\)</li> </ul> <p>These match the \u201cAlignment Scoring functions in Luong Attention\u201d slide.</p> <p>title: \"Sequential Learning \u2014 Part 3: Transformer Self-Attention Deep Dive\" description: \"Extended MkDocs-ready notes on Transformer internals: self-attention analogy, step-by-step derivations, worked numerical examples, multi-head attention, positional encodings, residual connections, encoder-decoder structure, and subword modeling.\" tags: [transformer, self-attention, multi-head, positional-encoding, residual, subword, nlp, deep-learning] date: 2025-08-24</p> <p>TL;DR</p> <p>Topic: Deep dive into Transformer self-attention and architectural innovations.</p> <p>Big ideas: - Self-attention \u2248 database retrieval with Query\u2013Key\u2013Value. - Softmax scaling ensures stable gradients. - Multi-head attention provides multiple perspectives. - Positional encodings inject sequence order. - Residual + LayerNorm enable deep stacking. - Subword modeling allows open-vocabulary handling.</p> <p>Applications: Machine translation, summarization, dialogue, protein folding, code generation.</p>"},{"location":"notes/lec5/#1-self-attention-as-database-lookup","title":"1. Self-Attention as Database Lookup","text":"<ul> <li>Analogy:</li> <li>Query = question asked of the database.</li> <li>Keys = index entries.</li> <li>Values = stored records.</li> <li>The system retrieves values based on similarity between Query and Keys.</li> </ul> <pre><code>flowchart LR\n    Q[Query Vector] --&gt; DB[(Keys + Values)] --&gt; V[Retrieved Value]</code></pre> <p>Slide analogy</p> <ul> <li>Query Q1 \u2192 Database index \u2192 Key\u2013Value pairs \u2192 Answer.</li> <li>Each token embedding simultaneously plays all three roles (Q, K, V).</li> </ul>"},{"location":"notes/lec5/#2-mathematical-self-attention","title":"2. Mathematical Self-Attention","text":"<p>Given input embeddings \\(X \\in \\mathbb{R}^{T \\times d_{model}}\\):</p> \\[ Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V \\] <p>where \\(W^Q, W^K, W^V\\) are learned parameter matrices.</p> <p>Scaled Dot-Product Attention:</p> \\[ \\text{Attention}(Q,K,V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V \\]"},{"location":"notes/lec5/#steps","title":"Steps","text":"<ol> <li>Dot product of queries with keys: relevance score.</li> <li>Scaling by \\(\\sqrt{d_k}\\) prevents extreme values and vanishing gradients.</li> <li>Softmax converts scores \u2192 probabilities \\(\\alpha_{t,i}\\).</li> <li>Weighted sum of values: contextualized embedding for each position.</li> </ol> <p>Numerical stability</p> <p>Without scaling, large \\(d_k\\) \u2192 extremely large dot products \u2192 softmax saturates.</p>"},{"location":"notes/lec5/#3-worked-example-numerical","title":"3. Worked Example (Numerical)","text":"<ul> <li>Suppose \\(q_1 \\cdot k_1 = 112\\), \\(q_1 \\cdot k_2 = 96\\).</li> <li>Scale by \\(\\sqrt{d_k} = 8\\): \\(14, 12\\).</li> <li>Softmax \u2192 weights \\([0.88, 0.12]\\).</li> <li>Weighted sum \u2192 \\(0.88 v_1 + 0.12 v_2\\).</li> </ul> <pre><code>flowchart LR\n    q1[\"Query q1\"] --&gt;|dot| k1\n    q1 --&gt;|dot| k2\n\n    k1 --&gt; Score[\"Raw Scores\"]\n    k2 --&gt; Score\n\n    Score --&gt; Scale[\"Divide by sqrt(d_k)\"]\n    Scale --&gt; Softmax[\"Softmax\"]\n    Softmax --&gt; Weights[\"Attention Weights\"]\n\n    Weights --&gt; Sum[\"Weighted Values \u2192 Context Vector\"]\n</code></pre> <p>Interpretation</p> <p>Token Thinking attends 88% to itself, 12% to Machines.</p>"},{"location":"notes/lec5/#4-multi-head-attention","title":"4. Multi-Head Attention","text":"<ul> <li>Instead of one Q\u2013K\u2013V projection, we use h heads.</li> <li>Each head projects into \\(d_k = d_{model}/h\\).</li> <li>Results concatenated \u2192 projected via \\(W^O\\).</li> </ul> <p>\\(\\(\\text{MHA}(X) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O\\)\\) Advantages: - Captures multiple types of relationships (syntax, semantics, long vs short range). - Prevents overfitting to a single attention pattern.</p> <pre><code>flowchart TB\n    X[Input Embedding] --&gt; Proj1[W^Q0, W^K0, W^V0] --&gt; Head0[Head 0]\n    X --&gt; Proj2[W^Q1, W^K1, W^V1] --&gt; Head1[Head 1]\n    Head0 &amp; Head1 --&gt; Concat[Concatenate Heads]\n    Concat --&gt; WO[Linear W^O]</code></pre> <p>Scaling</p> <p>In the original Transformer, \\(d_{model}=512\\), \\(h=8\\), so \\(d_k=64\\).</p>"},{"location":"notes/lec5/#5-positional-encoding","title":"5. Positional Encoding","text":"<p>Why needed?</p> <p>Self-attention itself is order-invariant. Without positional info, sentences like \u201cdog bites man\u201d vs \u201cman bites dog\u201d look identical.</p> <p>Sinusoidal Encoding:</p> \\[ PE_{(pos,2i)} = \\sin\\left( \\frac{pos}{10000^{2i/d_{model}}} \\right), \\quad PE_{(pos,2i+1)} = \\cos\\left( \\frac{pos}{10000^{2i/d_{model}}} \\right) \\] <ul> <li>Allows extrapolation to unseen sequence lengths.</li> <li>Captures relative distances via sinusoidal patterns.</li> </ul> <pre><code>graph LR\n  Emb[\"Word Embedding\"] --&gt; Sum[\"Final Input Embedding\"]\n  PE[\"Positional Encoding\"] --&gt; Sum\n</code></pre>"},{"location":"notes/lec5/#6-residual-connections-layer-normalization","title":"6. Residual Connections &amp; Layer Normalization","text":"<ul> <li>Residual connections: \\(\\text{Output} = f(x) + x\\).</li> <li>LayerNorm: normalize across features for stability.</li> </ul> <pre><code>graph TB\n  X[Input] --&gt; SA[Self-Attention]\n  SA --&gt; Add[Add Residual]\n  Add --&gt; LN[LayerNorm]</code></pre> <p>Effect</p> <ul> <li>Enables training very deep networks.</li> <li>Helps gradients flow back through many layers.</li> </ul>"},{"location":"notes/lec5/#7-encoder-decoder-structure","title":"7. Encoder &amp; Decoder Structure","text":"<p>Encoder Layer: 1. Self-Attention \u2192 Add + Norm. 2. Feed-Forward Network \u2192 Add + Norm.</p> <p>Decoder Layer: 1. Masked Self-Attention (prevents looking ahead). 2. Encoder\u2013Decoder Attention (queries = decoder states, keys/values = encoder outputs). 3. Feed-Forward Network.</p> <pre><code>flowchart TB\n    EncInput[Encoder Embeddings] --&gt; SA[Self-Attn]\n    SA --&gt; Add1[Add+Norm] --&gt; FF[Feed Forward]\n    FF --&gt; Add2[Add+Norm]\n\n    DecInput[Target Embeddings] --&gt; MaskSA[Masked Self-Attn]\n    MaskSA --&gt; Add3[Add+Norm] --&gt; EncDec[Encoder-Decoder Attention]\n    EncDec --&gt; Add4[Add+Norm] --&gt; DecFF[Feed Forward]\n    DecFF --&gt; Add5[Add+Norm]</code></pre> <p>Causal Masking</p> <p>Ensures autoregressive decoding \u2014 prediction of token t cannot access future tokens.</p>"},{"location":"notes/lec5/#8-subword-modeling","title":"8. Subword Modeling","text":"<p>Traditional assumptions: - Fixed vocabulary. - OOV (out-of-vocabulary) tokens mapped to <code>&lt;UNK&gt;</code>.</p> <p>Challenges: - Morphologically rich languages \u2192 huge vocab. - Rare words \u2192 inefficient training.</p> <p>Subword methods: - Break words into frequent subword units. - Preserve readability and coverage.</p> <p>Example:</p> <p>\u201cunfortunately\u201d \u2192 <code>un + for + tun + ate + ly</code></p> <p>Popular techniques</p> <ul> <li>Byte Pair Encoding (BPE)</li> <li>WordPiece (used in BERT)</li> <li>SentencePiece (used in T5, GPT\u20112 training data)</li> </ul>"},{"location":"notes/lec5/#9-advanced-notes-variants","title":"9. Advanced Notes &amp; Variants","text":"<ul> <li>Relative Positional Encoding: Improves generalization to long sequences.</li> <li>Rotary Embeddings (RoPE): Used in GPT\u2011NeoX, LLaMA.</li> <li>FlashAttention: Optimized implementation reducing memory overhead.</li> <li>ALiBi (Attention with Linear Biases): Improves extrapolation to longer contexts.</li> </ul>"},{"location":"notes/lec5/#10-keywords","title":"10. Keywords","text":"<p><code>Self-attention, Query, Key, Value, Attention Weights, Softmax, Contextual Representation, Multi-Head Attention, Positional Encoding, Residual Connection, Layer Normalization, Encoder-Decoder Attention, Masked Attention, Subword Modeling, Byte Pair Encoding, WordPiece, SentencePiece, Relative Position, Rotary Embeddings, FlashAttention, ALiBi</code></p>"},{"location":"notes/lec5/#11-references","title":"11. References","text":"<ul> <li>Vaswani et al. (2017), Attention Is All You Need </li> <li>Jay Alammar, The Illustrated Transformer </li> <li>Press et al. (2021), Train Short, Test Long: Attention with Linear Biases </li> <li>Dao et al. (2022), FlashAttention: Fast and Memory-Efficient Exact Attention </li> </ul> <p>TL;DR</p> <p>Topic: Pretraining language models with Transformers.</p> <p>Key steps: - Tokenization: Subword modeling (BPE, WordPiece, SentencePiece). - Pretraining tasks: autoregressive LM (GPT), masked LM (BERT), span corruption (T5). - Fine-tuning: task-specific adaptation. - Results: Large pretrained models transfer effectively across many NLP tasks.</p>"},{"location":"notes/lec5/#1-subword-modeling","title":"1. Subword Modeling","text":""},{"location":"notes/lec5/#11-byte-pair-encoding-bpe","title":"1.1 Byte Pair Encoding (BPE)","text":"<p>Algorithm (Sennrich et al. 2016; Wu et al. 2016):</p> <ol> <li>Initialize vocabulary with all characters.</li> <li>Tokenize words into characters + end-of-word symbol <code>&lt;/w&gt;</code>.</li> <li>Count pairs of consecutive symbols across corpus.</li> <li>Merge most frequent pair into new symbol.</li> <li>Repeat until reaching desired vocab size / merges.</li> </ol> <p>Example: - \u201cunfortunately\u201d \u2192 <code>un for tun ate ly</code></p> <p>Effect</p> <p>Reduces OOV, balances vocab size vs efficiency.</p>"},{"location":"notes/lec5/#12-other-subword-methods","title":"1.2 Other Subword Methods","text":"<ul> <li>Unigram LM Tokenization (Kudo, 2018): probabilistic model chooses best segmentation.</li> <li>WordPiece: merges based on likelihood rather than frequency (used in BERT).</li> <li>SentencePiece: language-independent, treats input as raw stream.</li> </ul> <p>Reference: https://arxiv.org/pdf/1804.10959.pdf</p>"},{"location":"notes/lec5/#2-pretraining-through-language-modeling","title":"2. Pretraining through Language Modeling","text":"<p>Autoregressive LM objective:</p> \\[ \\max_\\theta \\sum_t \\log p_\\theta(w_t \\mid w_{1:t-1}) \\] <ul> <li>Model predicts next token given context.</li> <li>Decoder architectures (RNN, Transformer) are typical.</li> </ul> <pre><code>flowchart LR\n    w1[w1] --&gt; Dec[Decoder]\n    w2[w2] --&gt; Dec\n    w3[w3] --&gt; Dec\n    Dec --&gt; wt[Predict next token]</code></pre>"},{"location":"notes/lec5/#3-generative-pretrained-transformer-gpt","title":"3. Generative Pretrained Transformer (GPT)","text":"<ul> <li>Architecture: Transformer Decoder-only.</li> <li>GPT-1: 12 layers, 768 hidden dim, 12 heads, 40k BPE vocab.</li> <li>Pretraining dataset: BookCorpus.</li> </ul> <p>Training task: Language modeling (left-to-right).</p> <p>Example input format (translation): <pre><code>[START] what is your name [DELIM] \u0924\u0941\u092e\u094d\u0939\u093e\u0930\u093e \u0928\u093e\u092e \u0915\u094d\u092f\u093e \u0939\u0948 [EXTRACT]\n</code></pre></p>"},{"location":"notes/lec5/#fine-tuning-gpt","title":"Fine-tuning GPT","text":"<ul> <li>Add linear layer on final hidden state.</li> <li>Task-specific supervision.</li> <li>Works for classification, QA, etc.</li> </ul> <p>GLUE tasks</p> <p>GPT achieved strong results on 6/8 GLUE benchmarks.</p>"},{"location":"notes/lec5/#4-bert-bidirectional-encoder-representations","title":"4. BERT (Bidirectional Encoder Representations)","text":"<ul> <li>Architecture: Transformer Encoder-only.</li> <li>Uses WordPiece tokenizer.</li> <li>Masked Language Model (MLM): randomly replace tokens with <code>[MASK]</code>, predict them.</li> <li>Next Sentence Prediction (NSP): binary classification for sentence pairs.</li> </ul> <p>BERT-Base: 12 layers, 768-dim hidden, 12 heads. BERT-Large: 24 layers, 1024-dim hidden, 16 heads.</p> <pre><code>flowchart LR\n    I1[I went to the] --&gt; M[[MASK]]\n    I2[to the store] --&gt; E[Encoder]\n    E --&gt; Predict[Predict missing word]</code></pre> <p>Bi-directionality</p> <p>Unlike GPT, BERT sees both left and right context simultaneously.</p>"},{"location":"notes/lec5/#5-t5-text-to-text-transfer-transformer","title":"5. T5: Text-to-Text Transfer Transformer","text":"<ul> <li>Idea: Frame every NLP task as text-to-text.</li> <li>Same model, same loss, same decoding.</li> </ul> <p>Examples: - Translation: <code>translate English to German: That is good.</code> \u2192 <code>Das ist gut.</code> - Classification: <code>cola sentence: The course is jumping well.</code> \u2192 <code>not acceptable</code> - QA: <code>When was Roosevelt born?</code> \u2192 <code>1882</code></p>"},{"location":"notes/lec5/#pretraining-task-span-corruption","title":"Pretraining Task: Span Corruption","text":"<ul> <li>Mask random spans (not just tokens).</li> <li>Replace with <code>&lt;X&gt;</code> markers.</li> <li>Train model to reconstruct missing spans.</li> </ul> <pre><code>Input: Thank you &lt;X&gt; me to your party &lt;Y&gt; week.\nTarget: &lt;X&gt; for inviting &lt;Y&gt; last &lt;Z&gt;\n</code></pre>"},{"location":"notes/lec5/#6-results-impact","title":"6. Results &amp; Impact","text":"<ul> <li>GPT: Improved over ELMo &amp; BiLSTMs.</li> <li>BERT: Huge boost across GLUE, SQuAD.</li> <li>T5: Unified pretraining + fine-tuning across many tasks.</li> </ul> <pre><code>graph TB\n  GPT[GPT Pretraining] --&gt; FineTune[Task Fine-tuning]\n  BERT[BERT Pretraining] --&gt; FineTune\n  T5[T5 Pretraining] --&gt; FineTune</code></pre> <p>Lesson</p> <p>Pretraining + fine-tuning is the foundation of modern NLP.</p>"},{"location":"notes/lec5/#7-keywords","title":"7. Keywords","text":"<p><code>Byte Pair Encoding, BPE, WordPiece, SentencePiece, Unigram LM, Pretraining, Language Modeling, GPT, Decoder-only Transformer, BookCorpus, Fine-tuning, GLUE, BERT, Encoder-only Transformer, MLM, NSP, Bi-directional Context, T5, Text-to-Text, Span Corruption, Transfer Learning</code></p>"},{"location":"notes/lec5/#8-references","title":"8. References","text":"<ul> <li>Sennrich et al. (2016). Neural Machine Translation of Rare Words with Subword Units. </li> <li>Wu et al. (2016). Google\u2019s Neural Machine Translation System. </li> <li>Radford et al. (2018). Improving Language Understanding by Generative Pre-Training. </li> <li>Devlin et al. (2019). BERT: Pre-training of Deep Bidirectional Transformers. </li> <li>Raffel et al. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5). </li> </ul> <ol> <li> <p>https://blog.floydhub.com/attention-mechanism/\u00a0\u21a9</p> </li> <li> <p>https://jalammar.github.io/illustrated-transformer/\u00a0\u21a9</p> </li> </ol>"},{"location":"notes/lec6/","title":"Reinforcement Learning (RL) Notes","text":""},{"location":"notes/lec6/#what-is-reinforcement-learning-rl","title":"\ud83d\udccc What is Reinforcement Learning (RL)?","text":"<p>Reinforcement Learning (RL) is a computational framework for sequential decision making. The central idea is that an agent learns to interact with an environment in order to maximize some notion of cumulative reward.</p> <ul> <li>Key Characteristics:</li> <li>Sequential: Decisions happen over time, not independently.</li> <li>Trial-and-error learning: The agent must try actions and learn from feedback.</li> <li>Delayed consequences: Rewards may not be immediate, requiring planning.</li> <li>Exploration vs exploitation: Balance between trying new actions vs. leveraging known good ones.</li> </ul> <pre><code>flowchart TD\n    subgraph RL [Reinforcement Learning Framework]\n        A[Agent] -- Action/Control --&gt; E[Environment]\n        E -- State/Observation --&gt; A\n        E -- Reward/Cost --&gt; A\n    end</code></pre>"},{"location":"notes/lec6/#difference-between-supervised-learning-and-rl","title":"\ud83d\udccc Difference Between Supervised Learning and RL","text":"Aspect Supervised Learning Reinforcement Learning Training Data Pre-collected, labeled No initial dataset; learns through interaction Labels Always known Rewards are unknown, possibly stochastic Assumptions IID data Data is sequential, non-IID Goal Minimize prediction error Maximize cumulative reward over time Example Spam filtering, regression Robot navigation, recommendation systems <pre><code>flowchart LR\n    SL[Supervised Learning] --&gt;|Pre-collected Data| Model\n    RL[Reinforcement Learning] --&gt;|Interaction| Env\n    Env --&gt;|Reward/Next State| RL</code></pre>"},{"location":"notes/lec6/#agentenvironment-loop","title":"\ud83d\udccc Agent\u2013Environment Loop","text":"<p>At each discrete time step \\(t\\): 1. Agent observes state \\(S_t\\). 2. Agent selects an action \\(A_t\\). 3. Environment responds with a reward \\(R_t\\) and a new state \\(S_{t+1}\\).</p> <pre><code>sequenceDiagram\n    participant Agent\n    participant Environment\n\n    Agent-&gt;&gt;Environment: Selects Action $A_t$\n    Environment--&gt;&gt;Agent: Returns Reward $R_t$\n    Environment--&gt;&gt;Agent: Provides Next State $S_{t+1}$</code></pre> <p>Mathematically: $$ S_t \\xrightarrow{A_t} S_{t+1}, \\quad R_t = R(S_t, A_t) $$</p>"},{"location":"notes/lec6/#markov-decision-process-mdp","title":"\ud83d\udccc Markov Decision Process (MDP)","text":"<p>An MDP provides the mathematical foundation for RL. Defined as a tuple: $$ (S, A, R, P, \\gamma) $$</p> <ul> <li>States (S): The set of all possible configurations of the environment.</li> <li>Actions (A): The set of possible moves the agent can make.</li> <li>Reward function (R): Maps \\((s, a)\\) to a scalar signal.</li> <li>Transition probabilities (P):   $$ P_a(s,s') = Pr(S_{t+1}=s'|S_t=s, A_t=a) $$</li> <li>Discount factor (\\(\\gamma\\)): Weighs immediate vs. future rewards.</li> </ul> <p>Objective: Maximize the expected discounted return: $$ G_t = E\\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\right] $$</p> <pre><code>flowchart LR\n    S0((S0)) --&gt;|A0| S1((S1))\n    S1 --&gt;|A1| S2((S2))\n    S2 --&gt;|A2| S3((S3))\n    S3 --&gt;|...| S4((...))</code></pre>"},{"location":"notes/lec6/#discount-factor-gamma","title":"\ud83d\udccc Discount Factor (\\(\\gamma\\))","text":"<p>The discount factor \\(\\gamma \\in [0,1]\\) represents how much future rewards are valued relative to immediate ones.</p> <ul> <li>If \\(\\gamma = 0\\) \u2192 Only immediate rewards matter.</li> <li>If \\(\\gamma = 1\\) \u2192 Future rewards are valued equally as present ones.</li> <li>Typically: \\(0 &lt; \\gamma &lt; 1\\) for balance.</li> </ul> <p>Example: - Predicting stock prices 5 years from now is uncertain \u2192 lower weight. - Receiving $10 today vs. $10 after 5 years \u2192 prefer immediate.</p> \\[ V(s) = E\\left[ R_0 + \\gamma R_1 + \\gamma^2 R_2 + \\dots \\mid S_0=s \\right] \\] <pre><code>flowchart TD\n    Now[Immediate Reward] -. High Weight .-&gt; Utility\n    Future[Future Reward] -. Discounted by \u03b3 .-&gt; Utility</code></pre>"},{"location":"notes/lec6/#exploration-vs-exploitation-dilemma","title":"\ud83d\udccc Exploration vs Exploitation Dilemma","text":"<p>The agent must explore new actions to learn but also exploit known good ones to maximize reward.</p> <ul> <li>Exploration: Try less-known actions \u2192 gather information.</li> <li>Exploitation: Choose best-known action \u2192 maximize immediate reward.</li> </ul> <p>Examples: - Restaurant choice: try new vs. go to favorite. - Music: listen to fav vs. discover new.</p> <pre><code>flowchart TD\n    Start --&gt;|Exploration| TryNew[Try a New Action]\n    Start --&gt;|Exploitation| Best[Choose Best Known Action]</code></pre>"},{"location":"notes/lec6/#multi-armed-bandit-mab-problem","title":"\ud83d\udccc Multi-Armed Bandit (MAB) Problem","text":"<p>Simplest RL setting: - One state only. - Multiple actions (arms). - Each arm yields a random reward.</p> <ul> <li>Tuple: \\((A, R)\\)</li> <li>Actions (A): Finite set of arms \\(\\{a_1, a_2, ..., a_m\\}\\)</li> <li>Rewards (R): Unknown distributions.</li> </ul> <p>At each step \\(t\\): - Agent picks \\(A_t\\). - Environment provides \\(R_t\\).</p> <p>Goal: $$ \\max E \\left[ \\sum_{t=1}^T R_t \\right] $$</p> <pre><code>flowchart LR\n    Agent --&gt;|Pull Arm| Slot[Slot Machine Arms]\n    Slot --&gt;|Reward| Agent</code></pre>"},{"location":"notes/lec6/#regret-in-bandits","title":"\ud83d\udccc Regret in Bandits","text":"<p>Regret quantifies how much reward was lost compared to the optimal policy.</p> <ul> <li>Mean reward of action \\(A\\): $$ V_A = E_{R \\sim R_A}[R|A] $$</li> <li>Optimal action: $$ A^* = \\arg\\max_A V_A $$</li> <li>Regret after \\(T\\) rounds: $$ Regret(T) = T \\cdot V_{A^*} - \\sum_{t=1}^T V_{A_t} $$</li> </ul> <pre><code>flowchart LR\n    Opt[Optimal Arm A*] --&gt; HighReward\n    Agent[Chosen Arms] --&gt; ActualReward\n    HighReward -. difference .-&gt; Regret</code></pre>"},{"location":"notes/lec6/#epsilon-greedy-strategy","title":"\ud83d\udccc Epsilon-Greedy Strategy","text":"<ul> <li>With probability \\(\\epsilon\\): pick a random arm (exploration).</li> <li>With probability \\(1-\\epsilon\\): pick the best-known arm (exploitation).</li> </ul> <pre><code>flowchart TD\n    Start --&gt;|\u03b5| Rand[Choose Random Arm]\n    Start --&gt;|1-\u03b5| Best[Choose Best Arm]\n    Rand --&gt; Arms\n    Best --&gt; Arms\n    Arms[Reward Observed]</code></pre> <p>Variants: - \u03b5-first: Explore first few rounds, then exploit. - \u03b5-decreasing: Reduce \\(\u03b5\\) over time.</p>"},{"location":"notes/lec6/#upper-confidence-bound-ucb","title":"\ud83d\udccc Upper Confidence Bound (UCB)","text":"<p>Balances exploration and exploitation via confidence intervals.</p> <ul> <li>Estimate mean reward \\(\\bar V_a\\).</li> <li>Add exploration bonus: $$ UCB_a(t) = \\bar V_a + \\sqrt{\\frac{2 \\ln t}{N_a(t)}} $$</li> </ul> <pre><code>flowchart LR\n    Est[Estimate Mean Reward] --&gt; Bonus[Add Confidence Bonus]\n    Bonus --&gt; UCB[UCB Score]\n    UCB --&gt; Select[Choose Best Arm]</code></pre>"},{"location":"notes/lec6/#thompson-sampling","title":"\ud83d\udccc Thompson Sampling","text":"<p>Bayesian method: sample parameters from posterior and act greedily.</p> <p>Steps: 1. Assume prior distribution \\(p(\\theta)\\). 2. Update posterior \\(p(\\theta|D_t)\\) using Bayes\u2019 theorem. 3. Sample parameter from posterior. 4. Choose action maximizing expected reward.</p> <p>For Bernoulli rewards: - Prior: Beta(\\(\\alpha, \\beta\\)). - Posterior after \\(N\\) trials: $$ p(\\theta|D_t) = Beta(\\alpha + \\text{successes}, \\beta + \\text{failures}) $$</p> <pre><code>flowchart TD\n    Prior[\"Prior Beta(\u03b1,\u03b2)\"] --&gt; Posterior[\"Update with Data\"]\n    Posterior --&gt; Sample[\"Sample \u03b8 from Posterior\"]\n    Sample --&gt; Action[\"Choose Arm with Max \u03b8\"]\n</code></pre>"},{"location":"notes/lec6/#contextual-bandits","title":"\ud83d\udccc Contextual Bandits","text":"<p>Unlike MAB, contextual bandits incorporate features of states and actions.</p> <ul> <li>State (context): user demographics, time, history.</li> <li>Action: item/arm to show.</li> <li>Reward: observed click/purchase.</li> </ul> <p>Tuple: \\((S,A,R)\\)</p> <p>At each step: 1. Environment provides context \\(s_t\\). 2. Agent selects action \\(a_t\\). 3. Environment returns reward \\(r_t\\).</p> <p>Objective: $$ \\max \\sum_{t=1}^T E[r_t | s_t, a_t] $$</p> <pre><code>flowchart LR\n    Context[Context Features] --&gt; Agent\n    Agent --&gt;|Action a_t| Item[Chosen Item]\n    Item --&gt; Reward[r_t]</code></pre>"},{"location":"notes/lec6/#contextual-bandit-algorithms","title":"\ud83d\udccc Contextual Bandit Algorithms","text":"<ul> <li>\u03b5-greedy with context: Use predictive model \\(f(s,a)\\) for reward estimation.</li> <li>Linear UCB:   $$ q_\\theta(s,a) = \\phi(s,a)^T \\theta $$</li> <li>Contextual Thompson Sampling: Posterior inference over contextual parameters.</li> </ul>"},{"location":"notes/lec6/#summary-of-contextual-bandits","title":"\ud83d\udccc Summary of Contextual Bandits","text":"Algorithm Strengths Weaknesses \u03b5-Greedy Easy, simple Sensitive to \u03b5 tuning UCB Theoretically strong Computationally heavy Thompson Sampling Handles uncertainty Posterior inference hard"},{"location":"notes/lec6/#big-picture","title":"\ud83d\udccc Big Picture","text":"<ul> <li>Multi-armed bandits: No context.</li> <li>Contextual bandits: Context-aware recommendations.</li> <li>Full RL (MDP): Sequential, long-term decision-making.</li> </ul> <pre><code>flowchart LR\n    Bandit[Multi-Armed Bandit] --&gt; Contextual[Contextual Bandit]\n    Contextual --&gt; RL[Full Reinforcement Learning]</code></pre> <p>Reinforcement Learning spans from simple bandits to complex long-horizon planning problems like robotics, healthcare, and recommendation systems.</p>"},{"location":"notes/lec6/#reinforcement-learning-advanced-nodetailleftbehind-notes","title":"Reinforcement Learning \u2014 Advanced, No\u2011Detail\u2011Left\u2011Behind Notes","text":"<p>These notes expand your earlier summary with deep, implementation\u2011ready detail that covers tabular Q\u2011learning, policy gradients / REINFORCE, and Deep Q\u2011Learning (DQN) with the practical tricks used in Atari\u2011style setups.</p>"},{"location":"notes/lec6/#1-qlearning-update-from-bellman-optimality-to-stochastic-updates","title":"1) Q\u2011Learning Update \u2014 From Bellman Optimality to Stochastic Updates","text":""},{"location":"notes/lec6/#bellman-optimality-actionvalue-form","title":"Bellman Optimality (action\u2011value form)","text":"<p>For any MDP with optimal \\(Q^*\\), the fixed point satisfies $$ Q^(s,a) = \\mathbb E\\left[ r_{t+1} + \\gamma \\max_{a'} Q^(s_{t+1},a') \\mid s_t=s, a_t=a \\right]. $$ The operator \\(\\mathcal T^*\\) defined by \\((\\mathcal T^*Q)(s,a) = \\mathbb E\\left[ r + \\gamma \\max_{a'} Q(s',a')\\right]\\) is a \\(\\gamma\\)\u2011contraction in the sup\u2011norm; therefore it has a unique fixed point \\(Q^*\\).</p>"},{"location":"notes/lec6/#samplebased-target-and-td-error","title":"Sample\u2011based target and TD error","text":"<p>Given a single transition \\((s_t,a_t,r_{t+1},s_{t+1})\\), form a sample target $$ \\underbrace{y_t}{\\text{target}} \\;=\\; r,a') $$ and the } + \\gamma \\max_{a'} Q(s_{t+1TD error $$ \\delta_t \\;=\\; y_t - Q(s_t,a_t). $$</p>"},{"location":"notes/lec6/#decayed-exponentialmovingaverage-update","title":"Decayed (exponential\u2011moving\u2011average) update","text":"<p>Blend the old estimate with the new target using a learning rate \\(\\alpha_t\\): $$ Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha_t\\,\\delta_t \\;=\\; (1-\\alpha_t)\\,Q(s_t,a_t) + \\alpha_t\\,\\big(r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1},a')\\big). $$ This is exactly the final line on your slide rewritten as an incremental update.</p>"},{"location":"notes/lec6/#convergence-tabular-case","title":"Convergence (tabular case)","text":"<p>Under standard conditions (finite MDP, GLIE exploration so every \\((s,a)\\) is visited infinitely often; step sizes satisfy \\(\\sum_t \\alpha_t=\\infty\\), \\(\\sum_t \\alpha_t^2&lt;\\infty\\)), tabular Q\u2011learning converges to \\(Q^*\\) with probability 1. In practice, a constant \\(\\alpha\\) is common; then you trade strict convergence guarantees for faster tracking.</p>"},{"location":"notes/lec6/#onpolicy-vs-offpolicy-and-sarsa","title":"On\u2011policy vs Off\u2011policy and SARSA","text":"<ul> <li>Q\u2011learning (off\u2011policy): learns \\(Q^*\\) using the max over \\(a'\\) irrespective of the behavior policy (often \\(\\epsilon\\)\u2011greedy).</li> <li>SARSA (on\u2011policy): target uses the actual next action \\(a_{t+1}\\) sampled by the behavior policy:   \\(y_t = r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1})\\) \u2014 safer under stochastic dynamics or when function approximation is unstable.</li> </ul>"},{"location":"notes/lec6/#recovering-the-greedy-policy","title":"Recovering the greedy policy","text":"<p>Given a learned \\(Q\\), the greedy policy is $$ \\pi^(s) \\;=\\; \\arg\\max_{a\\in\\mathcal A} Q(s,a). $$ During learning, use \\(\\epsilon\\)\u2011greedy* (or softmax) to ensure exploration: pick a random action w.p. \\(\\epsilon\\), otherwise the argmax.</p>"},{"location":"notes/lec6/#practical-hyperparameters-tabular","title":"Practical hyperparameters (tabular)","text":"<ul> <li>\\(\\epsilon\\) schedule: start high (0.9\u20131.0), linearly decay to 0.05\u20130.1; or cosine/exp schedules.</li> <li>Learning rate: \\(\\alpha\\in[0.05,0.5]\\) depending on reward scale.</li> <li>Discount: \\(\\gamma\\in[0.95,0.999]\\) for continuing tasks; smaller if horizons are short.</li> </ul>"},{"location":"notes/lec6/#common-failure-modes","title":"Common failure modes","text":"<ul> <li>Insufficient exploration: Q values overfit early experience.</li> <li>Non\u2011stationarity: changing dynamics reward requires non\u2011decaying or adaptive \\(\\alpha\\).</li> <li>Deadly triad (with function approximation + bootstrapping + off\u2011policy): divergence. Use target networks, double estimators, or on\u2011policy control.</li> </ul>"},{"location":"notes/lec6/#2-policybased-rl-reinforce-the-policy-gradient-theorem","title":"2) Policy\u2011based RL (REINFORCE &amp; the Policy Gradient Theorem)","text":""},{"location":"notes/lec6/#episodic-objective","title":"Episodic objective","text":"<p>Let a trajectory be \\(\\tau = (s_0,a_0,r_1,\\dots,s_T)\\) generated by a differentiable policy \\(\\pi_\\theta(a\\mid s)\\). Define the expected return of an episode $$ J(\\pi_\\theta) = \\mathbb E_{\\tau\\sim p_\\theta(\\tau)}[R(\\tau)],\\quad R(\\tau)=\\sum_{t=0}^{T-1} r_{t+1}. $$ The trajectory density factorizes as $$ p_\\theta(\\tau) = p(s_0)\\prod_{t=0}^{T-1} \\pi_\\theta(a_t\\mid s_t)\\,p(s_{t+1}\\mid s_t,a_t). $$</p>"},{"location":"notes/lec6/#policy-gradient-theorem-logderivative-trick","title":"Policy Gradient Theorem (log\u2011derivative trick)","text":"<p>Bring the gradient under the integral and use \\(\\nabla_\\theta \\log \\pi_\\theta\\): $$ \\nabla_\\theta J(\\pi_\\theta) = \\mathbb E_{\\tau}\\Bigg[\\sum_{t=0}^{T-1} \\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\, G_t\\Bigg], $$ where \\(G_t=\\sum_{k=t}^{T-1} r_{k+1}\\) is the return\u2011to\u2011go. This result holds because \\(\\nabla_\\theta \\log p(s_{t+1}\\mid s_t,a_t)=0\\) (dynamics do not depend on \\(\\theta\\) in model\u2011free settings).</p>"},{"location":"notes/lec6/#reinforce-algorithm-monte-carlo-policy-gradient","title":"REINFORCE algorithm (Monte Carlo policy gradient)","text":"<p>Vanilla form (without baseline): 1. Roll out episodes with current \\(\\theta\\). 2. For each time step, compute \\(G_t\\). 3. Ascend the gradient: \\(\\theta \\leftarrow \\theta + \\alpha\\,\\sum_t \\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\,G_t\\).</p> <p>With a baseline (variance reduction): replace \\(G_t\\) by advantage \\(A_t = G_t - b(s_t)\\); unbiased if \\(b\\) does not depend on \\(a\\). Common \\(b\\): state\u2011value \\(V_\\phi(s_t)\\) learned by regression. Update becomes $$ \\theta \\leftarrow \\theta + \\alpha\\,\\sum_t \\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\,\\underbrace{(G_t - V_\\phi(s_t))}_{A_t}. $$</p>"},{"location":"notes/lec6/#practical-improvements","title":"Practical improvements","text":"<ul> <li>Reward\u2011to\u2011go: use \\(G_t\\) not full \\(R(\\tau)\\) to reduce variance.</li> <li>Normalize advantages within a batch (zero mean / unit variance).</li> <li>Entropy regularization: add \\(+\\beta\\,\\mathbb E[\\mathcal H(\\pi_\\theta(\\cdot\\mid s))]\\) to encourage exploration; equivalent to a soft\u2011maximization of reward.</li> <li>Generalized Advantage Estimation (GAE): exponentially\u2011weighted TD(\\(\\lambda\\)) advantages: \\(A_t^{\\text{GAE}(\\lambda)}\\) balances bias/variance.</li> <li>Trust regions / clipping: TRPO/PPO stabilize large updates; PPO\u2019s clipped surrogate is widely used in practice.</li> </ul>"},{"location":"notes/lec6/#strengths-vs-weaknesses-policy-gradients","title":"Strengths vs weaknesses (policy gradients)","text":"<ul> <li>Pros: works in continuous action spaces, directly optimizes stochastic policies, easy to combine with differentiable nets.</li> <li>Cons: high variance, sample\u2011inefficient, typically on\u2011policy; mitigated by baselines, variance reduction, and richer critics (actor\u2011critic).</li> </ul>"},{"location":"notes/lec6/#3-qlearning-vs-reinforce-biasvariance-onoff-policy","title":"3) Q\u2011Learning vs REINFORCE \u2014 Bias/Variance, On/Off Policy","text":"Aspect Q\u2011Learning REINFORCE Learning type Value\u2011based, bootstrapped Policy\u2011based, Monte\u2011Carlo Policy Greedy/\\(\\epsilon\\)-greedy derived from \\(Q\\) Directly parameterized \\(\\pi_\\theta\\) On/Off policy Off\u2011policy (can learn from any behavior) On\u2011policy (data must match current policy) Bias/Variance High bias (bootstrapping) but low variance Unbiased gradient; high variance Action spaces Discrete (tabular/approx.) Naturally handles continuous actions Sample efficiency Good with replay Poor without replay; PPO/TRPO/A2C improve Stability Sensitive with function approx. (deadly triad) Sensitive to step size; stabilized by baselines/critics"},{"location":"notes/lec6/#4-deep-qlearning-dqn-why-deep-learning","title":"4) Deep Q\u2011Learning (DQN) \u2014 Why Deep Learning?","text":""},{"location":"notes/lec6/#curse-of-dimensionality-from-pixels","title":"Curse of dimensionality from pixels","text":"<p>If a state is a stack of the last 4 grayscale frames at 84\u00d784 resolution (256 intensities), the raw state space size is roughly \\(256^{84\\times84\\times4} \\approx 10^{67970}\\) \u2014 tabular methods are impossible. We approximate \\(Q(s,a)\\) with a convolutional neural network.</p>"},{"location":"notes/lec6/#qnetwork-parameterization","title":"Q\u2011Network parameterization","text":"<ul> <li>Single\u2011head form: one forward pass outputs a vector \\(Q_\\mathbf w(s,\\cdot)\\in\\mathbb R^{|\\mathcal A|}\\); the entry for index \\(a\\) is \\(Q(s,a;\\mathbf w)\\).</li> <li>Train by minimizing a temporal\u2011difference regression loss over mini\u2011batches of replayed transitions.</li> </ul>"},{"location":"notes/lec6/#experience-replay-break-temporal-correlations","title":"Experience Replay (break temporal correlations)","text":"<p>Maintain a buffer \\(\\mathcal D\\) of tuples \\((s,a,r,s',\\text{done})\\). Periodically sample i.i.d. minibatches to compute the TD loss $$ \\mathcal L(\\mathbf w) = \\mathbb E_{(s,a,r,s')\\sim\\mathcal D} \\Big[\\;\\underbrace{\\big(r + \\gamma\\,\\mathbf 1_{\\neg\\text{done}}\\,\\max_{a'} Q_{\\mathbf w^-}(s',a')\\; -\\; Q_{\\mathbf w}(s,a)\\big)^2}_{\\text{TD error squared (or Huber)}}\\;\\Big]. $$ Here \\(\\mathbf w^-\\) are target network parameters, copied from \\(\\mathbf w\\) every \\(C\\) steps (or updated by Polyak averaging). Targets computed with \\(\\mathbf w^-\\) stabilize training.</p>"},{"location":"notes/lec6/#dqn-algorithm-pseudocode","title":"DQN Algorithm (pseudocode)","text":"<pre><code>Initialize Q-network w with random weights\nInitialize target network w^- \u2190 w\nInitialize replay buffer D\nfor episode = 1..M:\n  s \u2190 env.reset();  \u03b5 \u2190 schedule(t)\n  for t = 1..T:\n    with prob \u03b5 choose a random action; else a \u2190 argmax_a Q_w(s,a)\n    s', r, done \u2190 env.step(a)\n    D.add(s,a,r,s',done)\n    if |D| \u2265 batch_size:\n      B \u2190 sample_minibatch(D)\n      y \u2190 r + \u03b3\u00b71_{\u00acdone}\u00b7max_{a'} Q_{w^-}(s',a')\n      L \u2190 Huber(y - Q_w(s,a))\n      w \u2190 w - \u03b7\u00b7\u2207_w L\n    every C steps: w^- \u2190 w (or soft update)\n    s \u2190 s'\n    if done: break\n</code></pre>"},{"location":"notes/lec6/#ataristyle-preprocessing-typical-choices","title":"Atari\u2011style Preprocessing (typical choices)","text":"<ul> <li>Frame preprocessing: convert to grayscale; downsample to 84\u00d784; max\u2011pool over 2 frames to remove flicker.</li> <li>Stack the last 4 frames to encode velocity.</li> <li>Action repeat / frame skip (e.g., 4) to reduce computation.</li> <li>Reward clipping to \\([-1, 1]\\) for scale\u2011invariance; gradient clipping in optimizer.</li> <li>Exploration schedule: \\(\\epsilon\\) linearly decays from 1.0 to 0.1 over 1e6 steps (or 0.01 for evaluation).</li> <li>Optimizer: RMSProp or Adam with learning rate around \\(10^{-4}\\)\u2013\\(10^{-5}\\); minibatch sizes 32\u201364.</li> <li>Target update period \\(C\\): 10^3\u201310^4 env steps; replay buffer size 1e5\u20131e6.</li> </ul>"},{"location":"notes/lec6/#key-stability-tricks","title":"Key stability tricks","text":"<ul> <li>Target networks (fixed \\(\\mathbf w^-\\) for targets).</li> <li>Huber loss instead of MSE (less sensitive to outliers):   \\(\\text{Huber}_\\kappa(x)=\\begin{cases} \\tfrac12x^2 &amp; |x|\\le\\kappa \\\\ \\kappa(|x|-\\tfrac12\\kappa) &amp; |x|&gt;\\kappa.\\end{cases}\\)</li> <li>Gradient clipping (e.g., global norm to 10).</li> <li>Replay buffer warmup before learning starts.</li> </ul>"},{"location":"notes/lec6/#reducing-overestimation-double-dqn","title":"Reducing overestimation: Double DQN","text":"<p>Use the online network to select the action and the target network to evaluate it: $$  y = r + \\gamma\\,Q_{\\mathbf w^-}!\\Big(s',\\arg\\max_{a'} Q_{\\mathbf w}(s',a')\\Big). $$ This curbs the positive bias from \\(\\max\\) over noisy estimates.</p>"},{"location":"notes/lec6/#prioritized-replay","title":"Prioritized Replay","text":"<p>Sample transitions with probability \\(p_i \\propto (|\\delta_i| + \\epsilon)^\\alpha\\); correct the bias with importance sampling weights \\(w_i = (N\\,p_i)^{-\\beta}\\) (normalized) inside the loss. Typical \\(\\alpha\\in[0.5,0.7]\\), \\(\\beta\\) annealed to 1.0.</p>"},{"location":"notes/lec6/#dueling-networks","title":"Dueling Networks","text":"<p>Decompose Q into state value and advantage: \\(Q(s,a)=V(s)+A(s,a)-\\tfrac1{|\\mathcal A|}\\sum_{a'}A(s,a')\\). Helps when many actions share similar value.</p>"},{"location":"notes/lec6/#multistep-returns-rainbow","title":"Multi\u2011step returns &amp; Rainbow","text":"<p>Use \\(n\\)\u2011step targets: \\(y = \\sum_{i=0}^{n-1} \\gamma^i r_{t+i+1} + \\gamma^n \\max_{a'} Q_{\\mathbf w^-}(s_{t+n},a')\\). Combine Double DQN, Prioritized Replay, Dueling, Noisy Nets, Distributional RL (Categorical/Quantile) \u2192 Rainbow DQN gains large performance boosts.</p>"},{"location":"notes/lec6/#5-dqn-in-atari-endtoend-from-pixels","title":"5) DQN in Atari \u2014 End\u2011to\u2011End from Pixels","text":"<ul> <li>Input: stack of 4 processed frames \\(\\in\\mathbb R^{84\\times84\\times4}\\).</li> <li>Conv net: e.g., conv(32,8\u00d78,stride4) \u2192 conv(64,4\u00d74,stride2) \u2192 conv(64,3\u00d73,stride1) \u2192 FC(512) \u2192 FC(\\(|\\mathcal A|\\)).</li> <li>Output: one scalar per discrete joystick/button combination (\u224818 in ALE).</li> <li>Reward: per\u2011step score change; often clipped.</li> <li>Evaluation: average over multiple seeds and sticky actions for robustness.</li> </ul> <p>Implementation notes: - Maintain separate training and evaluation \\(\\epsilon\\). - Reset lives handling consistently (some works treat loss of life as terminal for training only). - Monitor: average return, TD error histogram, Q\u2011values magnitude, buffer age, action visitation.</p>"},{"location":"notes/lec6/#6-deep-rl-breakthroughs-context-intuition","title":"6) Deep RL Breakthroughs (context &amp; intuition)","text":"<ul> <li>Atari (2013/2015): A single DQN architecture reached human\u2011level performance on many Atari 2600 games using only pixels and score as reward. Key ideas: replay, target network, convnets.</li> <li>AlphaZero (2017): Self\u2011play with Monte Carlo Tree Search guided by a shared policy\u2013value network; no human data, exceeded prior programs in chess, shogi, Go.</li> <li>AlphaStar (2019): Multi\u2011agent RL + imitation learning + league training reached Grandmaster level in StarCraft II (partial observability, macro/micro decisions, long horizons).</li> </ul> <p>These illustrate how value learning, policy learning, and planning can be combined at scale.</p>"},{"location":"notes/lec6/#7-putting-it-all-together-practical-recipes","title":"7) Putting It All Together \u2014 Practical Recipes","text":""},{"location":"notes/lec6/#choosing-an-algorithm","title":"Choosing an algorithm","text":"<ul> <li>Small discrete spaces: tabular Q\u2011learning/SARSA.</li> <li>High\u2011dimensional discrete (pixels): DQN (Double + Dueling + PER + n\u2011step = Rainbow).</li> <li>Continuous actions: Policy gradients with actor\u2011critic: PPO/SAC/TD3 (not covered in slides but essential in practice).</li> </ul>"},{"location":"notes/lec6/#hyperparameter-starter-kit-dqn","title":"Hyperparameter starter kit (DQN)","text":"<ul> <li>Buffer 1e6; batch 32/64; learning rate \\(1\\times10^{-4}\\); target update 1e4 steps; \\(\\gamma=0.99\\); warmup 5e4 steps; total env steps 2e7+ for difficult games.</li> </ul>"},{"location":"notes/lec6/#debugging-checklist","title":"Debugging checklist","text":"<ul> <li>Verify reward scaling/clipping and done flags.</li> <li>Ensure no bootstrapping at terminal (mask with \\((1-\\text{done})\\)).</li> <li>Plot \\(\\epsilon\\) vs steps, replay age distribution, Q\u2011value ranges, loss curves.</li> <li>Sanity check: on a tiny MDP, overfit with a tiny network to confirm learning signal.</li> </ul>"},{"location":"notes/lec6/#8-mathematics-why-the-updates-work","title":"8) Mathematics: Why the Updates Work","text":""},{"location":"notes/lec6/#contraction-and-fixed-point","title":"Contraction and fixed point","text":"<p>The Bellman optimality operator \\(\\mathcal T^*\\) is a \\(\\gamma\\)\u2011contraction: for any \\(Q_1,Q_2\\), \\(\\|\\mathcal T^*Q_1-\\mathcal T^*Q_2\\|_\\infty \\le \\gamma\\,\\|Q_1-Q_2\\|_\\infty\\). Banach\u2019s fixed\u2011point theorem \u21d2 unique \\(Q^*\\).</p>"},{"location":"notes/lec6/#stochastic-approximation-view","title":"Stochastic Approximation view","text":"<p>Q\u2011learning is Robbins\u2013Monro SA tracking the root of \\(\\mathbb E[\\delta_t\\mid s_t,a_t]=0\\). Conditions on \\(\\alpha_t\\) and visitation ensure almost\u2011sure convergence in tabular settings.</p>"},{"location":"notes/lec6/#policy-gradient-derivation-sketch","title":"Policy gradient derivation sketch","text":"<p>Use \\(\\nabla_\\theta J = \\int R(\\tau)\\,\\nabla_\\theta p_\\theta(\\tau)\\,d\\tau = \\int R(\\tau)\\,p_\\theta(\\tau)\\,\\nabla_\\theta\\log p_\\theta(\\tau)\\,d\\tau\\). Since \\(\\log p_\\theta(\\tau)\\) sums only \\(\\log\\pi_\\theta\\) terms, the gradient becomes an expectation of \\(\\sum_t \\nabla\\log\\pi_\\theta(a_t\\mid s_t)\\,G_t\\).</p>"},{"location":"notes/lec6/#9-algorithm-boxes-copypaste-friendly","title":"9) Algorithm Boxes (copy\u2011paste friendly)","text":""},{"location":"notes/lec6/#tabular-qlearning-episodic","title":"Tabular Q\u2011learning (episodic)","text":"<pre><code>Q = zeros(|S|, |A|)\nfor episode in range(M):\n    s = env.reset()\n    for t in range(T):\n        a = \u03b5_greedy(Q[s])\n        s2, r, done, _ = env.step(a)\n        td_target = r + \u03b3 * (0 if done else max(Q[s2]))\n        td_error  = td_target - Q[s,a]\n        Q[s,a]   += \u03b1 * td_error\n        s = s2\n        if done: break\n</code></pre>"},{"location":"notes/lec6/#reinforce-with-baseline","title":"REINFORCE (with baseline)","text":"<pre><code>for update in range(U):\n    traj = collect_episodes(\u03c0_\u03b8)\n    Gt = returns_to_go(traj.rewards, \u03b3)\n    b  = V_\u03c6(traj.states).detach()\n    adv = (Gt - b).normalize()\n    loss_actor = -(log\u03c0_\u03b8(traj.actions|traj.states) * adv).mean()\n    loss_critic = ((V_\u03c6(traj.states) - Gt)**2).mean()\n    optimize(loss_actor + c_v*loss_critic - c_ent*entropy(\u03c0_\u03b8))\n</code></pre>"},{"location":"notes/lec6/#dqn-double-huber-target-net","title":"DQN (Double + Huber + target net)","text":"<pre><code>batch = sample(D)\ns,a,r,s2,d = batch\nwith torch.no_grad():\n    a2 = Q_w(s2).argmax(dim=1)\n    y  = r + \u03b3*(1-d)*Q_wminus(s2).gather(1, a2.unsqueeze(1)).squeeze(1)\nqsa = Q_w(s).gather(1, a.unsqueeze(1)).squeeze(1)\nloss = huber(y - qsa)\nopt.zero_grad(); loss.backward(); clip_grad_norm_(Q_w.parameters(), 10); opt.step()\n</code></pre>"},{"location":"notes/lec6/#10-glossary-of-symbols","title":"10) Glossary of Symbols","text":"<ul> <li>\\(s_t,a_t,r_{t+1},s_{t+1}\\) \u2014 state, action, reward, next state at time \\(t\\).</li> <li>\\(\\gamma\\) \u2014 discount factor.</li> <li>\\(Q(s,a)\\) \u2014 action\u2011value; \\(V(s)\\) \u2014 state\u2011value.</li> <li>\\(\\pi_\\theta(a\\mid s)\\) \u2014 (stochastic) policy parameterized by \\(\\theta\\).</li> <li>\\(G_t\\) \u2014 return\u2011to\u2011go; \\(A_t\\) \u2014 advantage (e.g., \\(G_t - V(s_t)\\)).</li> <li>\\(\\mathbf w,\\mathbf w^-\\) \u2014 online and target Q\u2011network parameters.</li> <li>PER hyperparams: \\(\\alpha\\) (priority exponent), \\(\\beta\\) (IS correction exponent).</li> </ul>"},{"location":"notes/lec6/#11-extended-topics-quick-references","title":"11) Extended Topics (quick references)","text":"<ul> <li>Expected\u2011SARSA: target uses expectation over behavior policy at \\(s'\\); less variance than SARSA.</li> <li>Distributional RL: learn full return distribution \\(Z(s,a)\\) (categorical/quantile). Targets use distributional Bellman operator.</li> <li>Noisy Nets: parameterized noise in linear layers for exploration.</li> <li>n\u2011step Q\u2011learning: blend Monte\u2011Carlo and bootstrapping; better credit assignment.</li> <li>Actor\u2011Critic family (beyond slides): A2C/A3C (synchronous/asynchronous), PPO (clipped surrogate), SAC/TD3/DDPG (continuous control, entropy regularization or twin critics).</li> </ul>"},{"location":"notes/lec6/#closing","title":"Closing","text":"<p>You now have slide\u2011level theory and practitioner\u2011level knobs for Q\u2011learning, policy gradients, and DQN\u2014from the Bellman equations and policy gradient theorem all the way to Double/Dueling/PR replay and Atari\u2011grade pipelines. If you want, we can add worked examples (gridworld, bandit, CartPole, or Pong) with step\u2011by\u2011step calculations and reference hyperparameters.</p>"},{"location":"notes/lecture3/","title":"Dimensionality Reduction","text":"<p>Dimensionality reduction is the process of reducing the number of features (dimensions) in a dataset while preserving the most informative and discriminative aspects.</p> <p>It is one of the most critical techniques in machine learning and data science, because:</p> <ul> <li>Models often perform poorly when overwhelmed with too many features (curse of dimensionality).</li> <li>Redundant and irrelevant features introduce noise.</li> <li>High-dimensional spaces are sparse, which makes clustering and classification difficult.</li> <li>Reduced dimensionality leads to better generalization, interpretability, visualization, and faster computation.</li> </ul>"},{"location":"notes/lecture3/#1-the-usual-supervised-learning-approach","title":"1. The Usual Supervised Learning Approach","text":"<pre><code>flowchart LR\n    subgraph Training\n        A[Data: Features] --&gt; M[Learning Algorithm]\n        L[Labels] --&gt; M\n        M --&gt; B[Model]\n    end\n\n    subgraph Testing\n        T[Test Data] --&gt; B\n        B --&gt; P[Predicted Label]\n    end</code></pre> <ul> <li>A dataset consists of features (X) and labels (Y).</li> <li>A supervised learning algorithm learns a mapping X \u2192 Y.</li> <li>With high-dimensional data:</li> <li>The algorithm becomes overwhelmed.</li> <li>Training time is long.</li> <li>The risk of overfitting increases due to sparse data.</li> </ul>"},{"location":"notes/lecture3/#2-feature-types","title":"2. Feature Types","text":"<p>Not all features contribute equally to prediction.</p> <ul> <li> <p>Relevant Features</p> </li> <li> <p>Provide useful predictive information.</p> </li> <li> <p>Example: Diaper, Stroller, Bassinet, Cradle for predicting baby products.</p> </li> <li> <p>Irrelevant Features</p> </li> <li> <p>Provide no predictive signal.</p> </li> <li> <p>Example: Random identifiers.</p> </li> <li> <p>Redundant Features</p> </li> <li> <p>Highly correlated with other features, duplicating information.</p> </li> <li>Example: Stroller and wheels often co-occur.</li> </ul> <pre><code>mindmap\n  root((Features))\n    Relevant\n      \"Strong signal for prediction\"\n    Irrelevant\n      \"No contribution\"\n    Redundant\n      \"Overlaps with others\"</code></pre>"},{"location":"notes/lecture3/#3-approaches-to-dimensionality-reduction","title":"3. Approaches to Dimensionality Reduction","text":"<p>Two broad strategies:</p>"},{"location":"notes/lecture3/#31-feature-selection-downsizing-existing-features","title":"3.1 Feature Selection (downsizing existing features)","text":"<ul> <li>Removes noisy, irrelevant, or redundant features.</li> <li>Keeps original features intact.</li> <li>Preserves interpretability.</li> <li>Useful when:</li> <li>Budget constraints exist (e.g., costly medical tests).</li> <li>Small number of features is required for human interpretability.</li> </ul>"},{"location":"notes/lecture3/#32-low-dimensional-feature-learning-new-derived-features","title":"3.2 Low-Dimensional Feature Learning (new derived features)","text":"<ul> <li>Learns a new representation of the data.</li> <li>May lose interpretability but increases performance.</li> <li>Examples:</li> <li>Linear methods: PCA, MDS</li> <li>Non-linear methods: Kernel PCA, t-SNE</li> <li>Representation learning: Neural embeddings</li> </ul> <pre><code>graph TD\n    X[High-Dimensional Data] --&gt;|Feature Selection| S[Reduced Subset of Features]\n    X --&gt;|Feature Learning| F[New Derived Features]</code></pre>"},{"location":"notes/lecture3/#4-feature-selection-techniques","title":"4. Feature Selection Techniques","text":"Method Idea Pros Cons Examples Wrapper Use ML models to search best subset Captures feature interactions Computationally expensive Sequential Forward Selection, Backward Elimination Filter Rank features by statistical score Fast, scalable Ignores interactions Pearson Correlation, Chi-squared, Mutual Information Embedded Feature selection during training Efficient, task-specific Model-dependent Lasso Regression, Decision Trees"},{"location":"notes/lecture3/#5-wrapper-methods","title":"5. Wrapper Methods","text":""},{"location":"notes/lecture3/#51-exhaustive-search","title":"5.1 Exhaustive Search","text":"<ul> <li>\\(N\\) features \u2192 \\(2^N\\) possible subsets.</li> <li>Quickly infeasible:</li> <li>20 features \u2192 \\~1 million subsets</li> <li>25 features \u2192 \\~33.5 million subsets</li> <li>30 features \u2192 \\~1.1 billion subsets</li> </ul>"},{"location":"notes/lecture3/#52-sequential-forward-selection-sfs","title":"5.2 Sequential Forward Selection (SFS)","text":"<ol> <li>Start with empty set \\(S = \\emptyset\\).</li> <li>While stopping criteria not met:</li> <li>For each feature \\(X_f \\notin S\\):<ul> <li>\\(S' = S \\cup \\{X_f\\}\\)</li> <li>Train model on \\(S'\\).</li> <li>Evaluate accuracy.</li> </ul> </li> <li>Select feature with highest improvement.</li> <li>Return final \\(S\\).</li> </ol> <pre><code>flowchart TD\n    Start[Start with empty set S = empty] --&gt; Loop{Stopping Criteria Met?}\n    Loop -- No --&gt; Add[Add best feature]\n    Add --&gt; Train[Train + Evaluate]\n    Train --&gt; Loop\n    Loop -- Yes --&gt; End[Return Subset S]</code></pre> <ul> <li>Advantage: Captures strong features.</li> <li>Disadvantage: Cannot remove redundant features once added.</li> </ul>"},{"location":"notes/lecture3/#53-sequential-backward-elimination-sbe","title":"5.3 Sequential Backward Elimination (SBE)","text":"<ul> <li>Start with all features.</li> <li>Iteratively remove least useful feature.</li> <li>Stop when removal hurts performance.</li> </ul>"},{"location":"notes/lecture3/#54-heuristic-search","title":"5.4 Heuristic Search","text":"<ul> <li>Use optimization strategies like Genetic Algorithms, Simulated Annealing, Greedy search.</li> </ul>"},{"location":"notes/lecture3/#6-filter-methods","title":"6. Filter Methods","text":"<p>Principle: Replace costly model evaluation with fast statistics \\(J(X_f)\\).</p> <p>Examples:</p> <ul> <li>Mutual Information (MI): Captures dependence.</li> <li>Pearson Correlation: Measures linear relationships.</li> <li>Chi-squared test: For categorical features.</li> </ul>"},{"location":"notes/lecture3/#example-ranking-table","title":"Example Ranking Table","text":"Feature Index Score (\\(J(X_f)\\)) 32 0.98 5501 0.94 101 0.91 345 0.85 1104 0.81"},{"location":"notes/lecture3/#7-pearsons-correlation-coefficient","title":"7. Pearson\u2019s Correlation Coefficient","text":"<p>Captures linear relationships between feature \\(A\\) and target \\(Y\\):</p> \\[ \\rho(A,Y) = \\frac{\\text{cov}(A,Y)}{\\sigma_A \\cdot \\sigma_Y} \\] <p>Expanded:</p> \\[ \\rho(A,Y) = \\frac{\\sum_i (A_i - \\bar{A})(Y_i - \\bar{Y})}{\\sqrt{\\sum_i (A_i - \\bar{A})^2} \\cdot \\sqrt{\\sum_i (Y_i - \\bar{Y})^2}} \\] <ul> <li>\\(A_i, Y_i\\): sample values.</li> <li>\\(\\bar{A}, \\bar{Y}\\): means.</li> <li>Covariance matrix captures pairwise correlations.</li> </ul>"},{"location":"notes/lecture3/#8-embedded-methods","title":"8. Embedded Methods","text":"<ul> <li>Selection happens inside the learning algorithm.</li> <li>Examples:</li> <li>Lasso regression (L1): zeroes out unimportant features.</li> <li>Tree-based methods: select splits only on important features.</li> </ul> <pre><code>flowchart LR\n    Data --&gt; Model[Model with Embedded Selection]\n    Model --&gt; Output[Reduced Feature Set]</code></pre>"},{"location":"notes/lecture3/#extended-notes-on-dimensionality-reduction","title":"Extended Notes on Dimensionality Reduction","text":""},{"location":"notes/lecture3/#9-mutual-information-mi","title":"9. Mutual Information (MI)","text":"<p>Measures reduction in uncertainty:</p> \\[ I(A,Y) = \\sum_{x \\in A}\\sum_{y \\in Y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} \\] <ul> <li>If \\(A\\) and \\(Y\\) are independent \u2192 \\(I(A,Y)=0\\).</li> </ul>"},{"location":"notes/lecture3/#10-feature-interactions","title":"10. Feature Interactions","text":"<p>Individual features may appear weak, but combinations can be strong.</p> <ul> <li>Example: Two independent features become predictive when combined.</li> <li>Highlights the limitation of filter methods which evaluate features individually.</li> </ul>"},{"location":"notes/lecture3/#11-pros-cons-of-filter-methods","title":"11. Pros &amp; Cons of Filter Methods","text":"<p>Advantages:</p> <ul> <li>Simple, scalable.</li> <li>Easily parallelizable.</li> </ul> <p>Disadvantages:</p> <ul> <li>Cannot capture feature interactions.</li> <li>May select redundant features.</li> </ul>"},{"location":"notes/lecture3/#12-embedded-methods-lasso","title":"12. Embedded Methods: LASSO","text":"<p>Optimization problem:</p> \\[ \\min_\\beta \\sum_{i=1}^M (y_i - \\sum_j x_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^N |\\beta_j| \\] <ul> <li>L1 penalty induces sparsity.</li> <li>Contrast with Ridge (L2):</li> </ul> \\[ \\lambda \\sum_{j=1}^N \\beta_j^2 \\] <ul> <li>L2 shrinks weights but rarely eliminates them.</li> </ul>"},{"location":"notes/lecture3/#13-benefits-of-feature-selection","title":"13. Benefits of Feature Selection","text":"<ul> <li>Improves accuracy.</li> <li>Reduces computation.</li> <li>Improves interpretability.</li> <li>Prevents overfitting.</li> </ul>"},{"location":"notes/lecture3/#14-t-sne-motivation","title":"14. t-SNE Motivation","text":"<ul> <li>High-dimensional visualization is difficult.</li> <li>Project data into 2D or 3D for interpretability.</li> <li>t-SNE preserves local structure.</li> </ul>"},{"location":"notes/lecture3/#15-general-problem-statement","title":"15. General Problem Statement","text":"<p>Given high-dimensional \\(X = \\{x_1,...,x_M\\}\\), \\(x_i \\in \\mathbb{R}^N\\):</p> <p>Find \\(Y = \\{y_1,...,y_M\\}\\), \\(y_i \\in \\mathbb{R}^n, n &lt; N\\), minimizing information loss.</p>"},{"location":"notes/lecture3/#16-stochastic-neighbor-embedding-sne","title":"16. Stochastic Neighbor Embedding (SNE)","text":"<ul> <li>Preserves local distances.</li> <li>Converts distances to conditional probabilities:</li> </ul> \\[ p_{j|i} = \\frac{e^{-||x_i-x_j||^2/2\\sigma_i^2}}{\\sum_k e^{-||x_i-x_k||^2/2\\sigma_i^2}} \\] \\[ q_{j|i} = \\frac{e^{-||y_i-y_j||^2}}{\\sum_k e^{-||y_i-y_k||^2}} \\]"},{"location":"notes/lecture3/#17-from-sne-to-t-sne","title":"17. From SNE to t-SNE","text":"<ul> <li>High-dimensional similarities use Gaussian.</li> <li>Low-dimensional similarities use Student\u2019s t-distribution (heavier tails).</li> </ul> \\[ q_{ij} = \\frac{(1+||y_i-y_j||^2)^{-1}}{\\sum_{k \\neq l}(1+||y_k-y_l||^2)^{-1}} \\]"},{"location":"notes/lecture3/#18-kl-divergence-in-t-sne","title":"18. KL Divergence in t-SNE","text":"<p>Objective:</p> \\[ KL(P||Q) = \\sum_i \\sum_j p_{j|i}\\log \\frac{p_{j|i}}{q_{j|i}} \\] <ul> <li>Penalizes mismatches in similarity structure.</li> <li>Large \\(p_{j|i}\\) with small \\(q_{j|i}\\) \u2192 heavy penalty.</li> </ul>"},{"location":"notes/lecture3/#19-gradient-descent-optimization","title":"19. Gradient Descent Optimization","text":"<p>Gradient update rule:</p> \\[ \\frac{\\partial C}{\\partial y_i} = 2\\sum_j (y_j-y_i)(p_{j|i}-q_{j|i}+p_{i|j}-q_{i|j}) \\] <ul> <li>Emphasizes preserving close distances.</li> </ul>"},{"location":"notes/lecture3/#20-visualization-example-mnist","title":"20. Visualization Example (MNIST)","text":"<ul> <li>Applied to 6000 MNIST digits.</li> <li>t-SNE clusters digits cleanly compared to Sammon mapping.</li> </ul>"},{"location":"notes/lecture3/#21-summary-mindmap","title":"21. Summary Mindmap","text":"<pre><code>mindmap\n  root((Dimensionality Reduction))\n    Feature Selection\n      Wrapper\n        Forward Selection\n        Backward Elimination\n        Heuristic Search\n      Filter\n        Correlation\n        Chi-squared\n        Mutual Information\n      Embedded\n        Lasso (L1)\n        Decision Trees\n    Feature Learning\n      PCA\n      Kernel PCA\n      MDS\n      Autoencoders\n      t-SNE\n        SNE\n        KL Divergence\n        Gradient Descent</code></pre>"},{"location":"notes/lecture3/#key-takeaways","title":"\u2705 Key Takeaways","text":"<ul> <li>Dimensionality reduction combats the curse of dimensionality.</li> <li>Two approaches: Feature Selection (simpler, interpretable) vs. Feature Learning (powerful, less interpretable).</li> <li>Methods include Wrapper, Filter, Embedded selection, PCA, t-SNE, autoencoders.</li> <li>t-SNE is best for visualization, capturing local structures.</li> <li>Feature selection methods improve performance and interpretability for predictive modeling.</li> </ul>"},{"location":"notes/lecture3/#dimensionality-reduction-svd-pca-mf-nmf","title":"Dimensionality Reduction (SVD \u2022 PCA \u2022 MF \u2022 NMF)","text":""},{"location":"notes/lecture3/#motivation","title":"\ud83e\udded Motivation","text":"<p>We are deluged with data. Essential information often lives in a much smaller-dimensional subspace. Dimensionality reduction aims to:</p> <ul> <li>compress data (fewer numbers, less storage),</li> <li>denoise (throw away directions dominated by noise),</li> <li>uncover structure (clusters, topics, latent factors),</li> <li>speed up downstream learning.</li> </ul> <pre><code>flowchart LR\n  X[\"High-dim Data: X in R^(m\u00d7n)\"] --&gt;|Decompose / Project| Z[\"Low-dim Representation\"]\n  Z --&gt;|Reconstruct| Xhat[\"Approximate X\"]\n  Xhat --&gt;|Error| Err[\"Reconstruction Error e\"]\n  Z --&gt; Models[\"Downstream models: kNN, SVM, regressors\"]\n\n  subgraph Families\n    A1[\"SVD / PCA (linear, global)\"]\n    A2[\"MF / NMF (parts-based, sparse)\"]\n    A3[\"t-SNE / UMAP (nonlinear, local)\"]\n  end\n\n  X --&gt; A1\n  X --&gt; A2\n  X --&gt; A3\n</code></pre>"},{"location":"notes/lecture3/#1-singular-value-decomposition-svd","title":"1) Singular Value Decomposition (SVD)","text":""},{"location":"notes/lecture3/#definition","title":"Definition","text":"<p>For any real matrix \\(X \\in \\mathbb{R}^{m\\times n}\\):</p> \\[ X = U\\,\\Sigma\\,V^T, \\] <p>where</p> <ul> <li>\\(U\\in\\mathbb{R}^{m\\times m}\\) and \\(V\\in\\mathbb{R}^{n\\times n}\\) are orthogonal (\\(U^TU=I\\), \\(V^TV=I\\)),</li> <li>\\(\\Sigma\\in\\mathbb{R}^{m\\times n}\\) is diagonal with non\u2011negative singular values \\(\\sigma_1\\ge\\sigma_2\\ge\\cdots\\ge0\\).</li> </ul>"},{"location":"notes/lecture3/#intuition","title":"Intuition","text":"<p>A linear map = rotate (V) \u2192 scale (\u03a3) \u2192 rotate (U). Large \\(\\sigma_i\\) indicate energetic directions.</p> <pre><code>graph TD\n  V[Rotate by V^T] --&gt; S[Scale by \u03a3] --&gt; U[Rotate by U]</code></pre>"},{"location":"notes/lecture3/#best-rankk-approximation-eckartyoung","title":"Best rank\u2011k approximation (Eckart\u2013Young)","text":"<p>Let \\(X_k = U_{:,1:k}\\,\\Sigma_{1:k,1:k}\\,V_{:,1:k}^T\\). Then for any matrix \\(Y\\) with \\(\\operatorname{rank}(Y)\\le k\\):</p> \\[ \\|X - X_k\\|_F \\le \\|X - Y\\|_F\\quad\\text{and}\\quad \\|X - X_k\\|_2 = \\sigma_{k+1}. \\]"},{"location":"notes/lecture3/#worked-example-exact-svd-of-a-22-matrix","title":"\ud83d\udd22 Worked Example \u2013 Exact SVD of a 2\u00d72 matrix","text":"<p>Let</p> \\[ X=\\begin{bmatrix}3 &amp; 1\\\\1 &amp; 3\\end{bmatrix}. \\] <ol> <li>Compute \\(X^TX=\\begin{bmatrix}10&amp;6\\\\6&amp;10\\end{bmatrix}\\).</li> <li>Eigenpairs of \\(X^TX\\):</li> <li>\\(\\lambda_1=16\\) with eigenvector \\(v_1=\\tfrac{1}{\\sqrt2}[1,\\,1]^T\\),</li> <li>\\(\\lambda_2=4\\) with eigenvector \\(v_2=\\tfrac{1}{\\sqrt2}[1,\\,-1]^T\\). Hence singular values: \\(\\sigma_1=\\sqrt{16}=4\\), \\(\\sigma_2=\\sqrt{4}=2\\). Let \\(V=[v_1\\ v_2]\\).</li> <li>Compute left vectors: \\(u_i = \\tfrac{1}{\\sigma_i} X v_i\\).</li> <li>\\(u_1 = \\tfrac{1}{4}Xv_1 = \\tfrac{1}{4}\\tfrac{1}{\\sqrt2}[4,\\,4]^T = \\tfrac{1}{\\sqrt2}[1,1]^T\\),</li> <li>\\(u_2 = \\tfrac{1}{2}Xv_2 = \\tfrac{1}{2}\\tfrac{1}{\\sqrt2}[2,\\,-2]^T = \\tfrac{1}{\\sqrt2}[1,-1]^T\\). Let \\(U=[u_1\\ u_2]\\), \\(\\Sigma=\\operatorname{diag}(4,2)\\).</li> <li>Verify: \\(U\\Sigma V^T=X\\).</li> </ol>"},{"location":"notes/lecture3/#worked-example-rank1-truncation-error","title":"\ud83d\udd22 Worked Example \u2013 Rank\u20111 truncation &amp; error","text":"<p>Rank\u20111 approximation \\(X_1=4\\,u_1 v_1^T\\). Error in spectral norm = \\(\\sigma_2=2\\). In Frobenius norm: \\(\\|X-X_1\\|_F=\\sqrt{\\sigma_2^2}=2\\).</p>"},{"location":"notes/lecture3/#power-iteration-computing-top-singular-vector","title":"Power Iteration (computing top singular vector)","text":"<ul> <li>Initialize \\(u^{(0)}\\) randomly.</li> <li>Iterate: \\(v^{(t)}=X^Tu^{(t)};\\ u^{(t+1)} = Xv^{(t)}/\\|Xv^{(t)}\\|\\).</li> <li>Converges to top left singular vector.</li> </ul> <pre><code>sequenceDiagram\n  participant U as u^(t)\n  participant XT as X^T\n  participant V as v^(t)\n  participant X as X\n  U-&gt;&gt;XT: v = X^T u\n  XT--&gt;&gt;V: v^(t)\n  V-&gt;&gt;X: u' = X v\n  X--&gt;&gt;U: normalize(u')</code></pre>"},{"location":"notes/lecture3/#2-principal-component-analysis-pca","title":"2) Principal Component Analysis (PCA)","text":""},{"location":"notes/lecture3/#two-equivalent-views","title":"Two equivalent views","text":"<ol> <li>Eigenview on covariance: with centered data \\(X_c\\), \\(\\tfrac{1}{n}X_cX_c^T = U\\Lambda U^T\\). PCs are columns of \\(U\\).</li> <li>SVD view: \\(X_c=U\\Sigma V^T\\). Then principal directions = columns of \\(U\\); variances = \\(\\sigma_i^2/(n-1)\\).</li> </ol>"},{"location":"notes/lecture3/#algorithm","title":"Algorithm","text":"<ol> <li>Center features: \\(X_c = X - \\mu 1^T\\).</li> <li>Compute SVD: \\(X_c=U\\Sigma V^T\\).</li> <li>Choose \\(k\\) via explained variance ratio \\(\\sum_{i=1}^k \\sigma_i^2 / \\sum_{i} \\sigma_i^2\\).</li> <li>Low\u2011dim representation (scores): \\(Z=U_{:,1:k}^T X_c\\).</li> <li>Reconstruction: \\(\\hat X = U_{:,1:k}Z + \\mu 1^T\\).</li> </ol> <pre><code>flowchart LR\n  A[Raw data X] --&gt; B[Center by mean \u03bc]\n  B --&gt; C[SVD of X_c]\n  C --&gt; D[Pick k by EVR]\n  D --&gt; E[Scores Z = U_k^T X_c]\n  E --&gt; F[Use Z for ML]\n  E --&gt; G[Reconstruct X\u0302 = U_k Z + \u03bc]</code></pre>"},{"location":"notes/lecture3/#worked-example-pca-on-a-2d-toy-dataset","title":"\ud83d\udd22 Worked Example \u2013 PCA on a 2D toy dataset","text":"<p>Data (n=5 points): \\((2,0),(0,2),(3,1),(4,0),(0,3)\\).</p> <ol> <li>Mean: \\(\\mu=[1.8,\\ 1.2]^T\\). Centered matrix</li> </ol> \\[ X_c=\\begin{bmatrix} 0.2 &amp; -1.2\\\\ -1.8 &amp; 0.8\\\\ 1.2 &amp; -0.2\\\\ 2.2 &amp; -1.2\\\\ -1.8 &amp; 1.8 \\end{bmatrix}. \\] <ol> <li>Covariance: \\(C=\\tfrac{1}{n-1}X_c^TX_c=\\begin{bmatrix}3.7 &amp; -2.9\\\\ -2.9 &amp; 2.3\\end{bmatrix}.\\)</li> <li>Eigenpairs (rounded): \\(\\lambda_1\\approx5.89\\) (dir. \\(v_1\\propto[0.82,-0.57]\\)), \\(\\lambda_2\\approx0.11\\).</li> <li>Explained variance of PC1: \\(5.89/(5.89+0.11)\\approx98.2\\%\\).</li> <li>Project onto PC1: scores = \\(Z = X_c v_1\\) (one scalar per point).</li> <li>Reconstruction with \\(k=1\\): \\(\\hat X = (v_1 v_1^T)X_c + \\mu\\).\\    Result: good 1D compression with tiny error (since \\(\\lambda_2\\) small).</li> </ol>"},{"location":"notes/lecture3/#pca-for-images-patch-pca","title":"PCA for images (patch PCA)","text":"<ul> <li>Patch size 12\u00d712 \u2192 144\u2011D.</li> <li>Compute PCs \u2192 basis filters (edges, blobs).</li> <li>Reconstruction error vs k typically decays fast; keep \\(k\\) around 20\u201360 for strong compression on patches.</li> </ul>"},{"location":"notes/lecture3/#3-latent-semantic-indexing-lsi-via-svd-text","title":"3) Latent Semantic Indexing (LSI) via SVD (text)","text":"<p>Given a term\u2013document matrix \\(X\\) (tf or tf\u2013idf). Compute rank\u2011\\(k\\) SVD: \\(X\\approx U_k\\Sigma_kV_k^T\\).</p> <ul> <li>Columns of \\(U_k\\) give term embeddings; columns of \\(V_k\\) give document embeddings; \\(\\Sigma_k\\) rescales.</li> <li>Similarity in this space captures latent topics even if exact words differ.</li> </ul>"},{"location":"notes/lecture3/#worked-example-mini-tdm-3-terms-3-docs","title":"\ud83d\udd22 Worked Example \u2013 Mini TDM (3 terms \u00d7 3 docs)","text":"\\[ X=\\begin{bmatrix} 1&amp;1&amp;0\\\\ 1&amp;0&amp;1\\\\ 0&amp;1&amp;1 \\end{bmatrix}. \\] <ul> <li>Full SVD gives \\(\\sigma\\approx(2,1,0)\\).</li> <li>Rank\u20112 LSI keeps first two singular values/vectors; cosine similarities between docs improve (synonymy effect).</li> </ul>"},{"location":"notes/lecture3/#4-matrix-factorization-mf","title":"4) Matrix Factorization (MF)","text":"<p>We model an (often incomplete) matrix \\(R\\in\\mathbb{R}^{n_u\\times n_i}\\) of user\u2013item interactions as</p> \\[ R \\approx U^T V + b\\,1^T + 1\\,\\tilde b^T, \\] <p>where \\(U\\in\\mathbb{R}^{r\\times n_u}\\) and \\(V\\in\\mathbb{R}^{r\\times n_i}\\) are latent factors, and \\(b,\\tilde b\\) biases.</p>"},{"location":"notes/lecture3/#objective-with-l2-regularization","title":"Objective (with L2 regularization)","text":"\\[ \\min_{U,V,b,\\tilde b} \\sum_{(i,j)\\in\\Omega} (r_{ij}-u_i^T v_j - b_i - \\tilde b_j)^2  + \\lambda(\\|U\\|_F^2+\\|V\\|_F^2 + \\|b\\|_2^2 + \\|\\tilde b\\|_2^2), \\] <p>where \\(\\Omega\\) is the set of observed entries.</p>"},{"location":"notes/lecture3/#algorithms","title":"Algorithms","text":"<ul> <li>ALS (Alternating Least Squares): fix \\(U\\), solve least squares for \\(V\\); swap. Scales well in Spark.</li> <li>SGD: update on each observed triple using gradients; supports online/streaming.</li> </ul> <pre><code>flowchart LR\n  R[Observed ratings R] --&gt; ALS{ALS Loop}\n  ALS --&gt;|fix U| SolveV[Solve for each v_j]\n  ALS --&gt;|fix V| SolveU[Solve for each u_i]\n  SolveU --&gt; ALS\n  SolveV --&gt; ALS\n  ALS --&gt; Pred[Predict r\u0302_ij = u_i^T v_j + b_i + b\u0303_j]</code></pre>"},{"location":"notes/lecture3/#worked-example-tiny-mf-with-als-rank-r2","title":"\ud83d\udd22 Worked Example \u2013 Tiny MF with ALS (rank r=2)","text":"<p>Observed ratings (\"?\" missing):</p> \\[ R=\\begin{array}{c|ccc}  &amp; i_1 &amp; i_2 &amp; i_3 \\\\\\hline u_1 &amp; 5 &amp; 3 &amp; ?\\\\ u_2 &amp; 4 &amp; ? &amp; 1\\\\ \\end{array} \\] <p>Initialize \\(U=\\begin{bmatrix}1&amp;0\\\\0&amp;1\\end{bmatrix}\\), \\(V=\\begin{bmatrix}1&amp;1&amp;1\\\\1&amp;1&amp;1\\end{bmatrix}\\). One ALS sweep (showing item 2):</p> <ul> <li>Users who rated \\(i_2\\): only \\(u_1\\) with rating 3. Solve \\(\\min_{v_2}\\ (3 - u_1^Tv_2)^2 + \\lambda\\|v_2\\|^2\\). With \\(u_1=[1,0]^T\\) and \\(\\lambda=0.1\\), closed form gives \\(v_2 = (U^{(2)T}U^{(2)}+\\lambda I)^{-1}U^{(2)T}r^{(2)} = (1.1I)^{-1}[3,0]^T = [2.727,0]^T\\). Repeat for other items and then solve for user vectors given items. After a few ALS rounds, predict missing: \\(\\hat r_{1,3}=u_1^Tv_3 + b_1 + \\tilde b_3\\) (biases often raise accuracy by 5\u201310%).</li> </ul>"},{"location":"notes/lecture3/#worked-example-onestep-sgd-update","title":"\ud83d\udd22 Worked Example \u2013 One\u2011step SGD update","text":"<p>Loss on observed \\((i,j)=(1,2)\\): \\(\\ell = (r_{12}-u_1^Tv_2)^2 + \\lambda(\\|u_1\\|^2+\\|v_2\\|^2)\\). Gradients: \\(\\nabla_{u_1}=-2(r_{12}-u_1^Tv_2)v_2 + 2\\lambda u_1\\), similarly for \\(v_2\\). Update with step \\(\\eta\\): \\(u_1 \\leftarrow u_1 - \\eta\\nabla_{u_1}\\), \\(v_2 \\leftarrow v_2 - \\eta\\nabla_{v_2}\\).</p>"},{"location":"notes/lecture3/#5-nonnegative-matrix-factorization-nmf","title":"5) Non\u2011negative Matrix Factorization (NMF)","text":"<p>We seek non\u2011negative factors \\(W\\in\\mathbb{R}_+^{m\\times r}\\), \\(H\\in\\mathbb{R}_+^{r\\times n}\\) such that \\(X \\approx WH.\\) This encourages parts\u2011based representations (e.g., eyes, nose, mouth for faces; topics for text).</p>"},{"location":"notes/lecture3/#multiplicative-updates-lee-seung","title":"Multiplicative updates (Lee &amp; Seung)","text":"<p>For objective \\(\\min_{W,H}\\ \\|X-WH\\|_F^2\\):</p> \\[ H \\leftarrow H \\odot \\frac{W^T X}{W^T W H},\\quad W \\leftarrow W \\odot \\frac{X H^T}{W H H^T}\\quad (\\odot: \\text{elementwise}). \\]"},{"location":"notes/lecture3/#worked-example-nmf-on-a-43-matrix-1-iteration","title":"\ud83d\udd22 Worked Example \u2013 NMF on a 4\u00d73 matrix (1 iteration)","text":"\\[ X = \\begin{bmatrix}4&amp;1&amp;0\\\\3&amp;0&amp;1\\\\0&amp;2&amp;3\\\\0&amp;1&amp;4\\end{bmatrix},\\quad r=2, \\ W=\\begin{bmatrix}1&amp;0.5\\\\1&amp;0.5\\\\0.5&amp;1\\\\0.5&amp;1\\end{bmatrix},\\ H=\\begin{bmatrix}1&amp;1&amp;1\\\\1&amp;1&amp;1\\end{bmatrix}. \\] <p>Compute updates (showing \\(H\\) numerator/denominator first column):</p> <ul> <li>\\(W^TX = \\begin{bmatrix}7&amp;2&amp;1\\\\4.5&amp;2&amp;4.5\\end{bmatrix}\\Rightarrow (W^TX)_{:,1}=[7,\\ 4.5]^T\\).</li> <li>\\(W^TWH = (W^TW)H\\) with \\(W^TW=\\begin{bmatrix}2.5&amp;2\\\\2&amp;2.5\\end{bmatrix}\\) gives first col \\([4.5,\\ 4.5]^T\\).</li> <li>New first column of \\(H\\): elementwise multiply by \\([7/4.5,\\ 4.5/4.5]=[1.556,\\ 1]\\). Repeat for other columns; then update \\(W\\) analogously. Observation: factors stay non\u2011negative and begin specializing columns/rows.</li> </ul>"},{"location":"notes/lecture3/#6-pca-vs-autoencoders-ae-encoderdecoder-view","title":"6) PCA vs Autoencoders (AE) \u2013 Encoder\u2013Decoder view","text":"<ul> <li>PCA: linear AE with encoder \\(U_k^T\\), decoder \\(U_k\\), MSE loss, orthogonal columns.</li> <li>AE: can be deep/nonlinear; objective can include sparsity, denoising, contrastive losses.</li> </ul> <pre><code>flowchart LR\n  X[\"X \u2208 R^{m\u00d7n}\"] --&gt; E[\"Encoder f\u03b8\"]\n  E --&gt; Z[\"z \u2208 R^{k\u00d7n}\"]\n  Z --&gt; D[\"Decoder g\u03c6\"]\n  D --&gt; Xhat[\"X\u0302\"]\n\n  X --- PCA[\"PCA\"]\n  PCA --- Xhat\n  X --- AE[\"Autoencoder\"]\n  AE --- Xhat\n\n  %% Style edge labels as separate nodes\n  PCA:::linear\n  AE:::nonlinear\n\n  classDef linear fill:#d9ead3,stroke:#333,stroke-width:1px;\n  classDef nonlinear fill:#fce5cd,stroke:#333,stroke-width:1px;\n</code></pre>"},{"location":"notes/lecture3/#7-practical-guidance-diagnostics","title":"7) Practical guidance &amp; diagnostics","text":"<ul> <li>Centering &amp; Scaling: PCA requires centering; consider standardizing features with different scales.</li> <li>Choosing k: Scree plot (\\(\\sigma_i\\)), cumulative EVR \u2265 0.90\u20130.99 for compression; cross\u2011validate for downstream task.</li> <li>Out\u2011of\u2011sample transforms: Store \\(\\mu\\) and \\(U_k\\) to project new data: \\(z=U_k^T(x-\\mu)\\).</li> <li>Cold start in MF: use content features or priors; biases stabilize.</li> <li>Regularization: \\(\\lambda\\) prevents overfitting in MF; shrink small PCs if noisy (Tikhonov).</li> <li>Sparsity: Prefer MF/NMF when matrices are highly sparse; SVD on dense TDM often uses truncated solvers (Lanczos).</li> </ul>"},{"location":"notes/lecture3/#8-extended-solved-miniproblems","title":"8) Extended solved mini\u2011problems","text":""},{"location":"notes/lecture3/#81-pca-by-hand-32-matrix","title":"8.1 PCA by hand (3\u00d72 matrix)","text":"\\[ X=\\begin{bmatrix}2&amp;0\\\\0&amp;2\\\\3&amp;1\\end{bmatrix},\\ \\mu=\\tfrac{1}{3}[5,\\ 1]^T,\\ X_c=X-\\mu 1^T. \\] <p>Compute SVD of \\(X_c\\) (algebra similar to the earlier 2\u00d72 SVD). Keep largest \\(\\sigma\\) only; reconstruct \\(\\hat X\\); compute error \\(\\|X-\\hat X\\|_F\\).</p>"},{"location":"notes/lecture3/#82-filling-a-missing-rating-with-mf-closed-form-no-bias","title":"8.2 Filling a missing rating with MF (closed form, no bias)","text":"<p>Given two users/two items with observed \\(r_{11}=5, r_{21}=4, r_{12}=3\\); estimate \\(r_{22}\\) with rank\u20111 MF and \\(\\lambda=0\\).\\ Solve \\(\\min_{u_1,u_2,v_1,v_2}\\sum (r_{ij}-u_iv_j)^2\\). Closed form (normal equations) gives \\(u_1=\\tfrac{5}{v_1}, u_2=\\tfrac{4}{v_1}, v_2=\\tfrac{3}{u_1}\\Rightarrow r_{22}=u_2v_2=\\tfrac{4}{v_1}\\cdot\\tfrac{3}{u_1}=\\tfrac{12}{5}\\approx2.4\\) (one of the valid minima). Adding \\(\\lambda&gt;0\\) stabilizes.</p>"},{"location":"notes/lecture3/#83-choosing-k-via-error-bound","title":"8.3 Choosing k via error bound","text":"<p>If singular values (descending) are \\((10, 3, 1, 0.2, \u2026)\\) and we keep \\(k=2\\), then</p> <ul> <li>spectral\u2011norm error = \\(\\sigma_{3}=1\\),</li> <li>relative Frobenius error \\(\\approx\\sqrt{1^2+0.2^2}/\\sqrt{10^2+3^2+1^2+0.2^2}\\approx\\tfrac{1.02}{10.53}\\approx9.7\\%\\).</li> </ul>"},{"location":"notes/lecture3/#84-nmf-topic-sketch-on-tdm-toy","title":"8.4 NMF topic sketch on TDM (toy)","text":"<p>Terms = {color, fabric, size}, Docs = {d1,d2,d3} with \\(X=\\begin{bmatrix}4&amp;1&amp;0\\\\3&amp;0&amp;1\\\\0&amp;2&amp;3\\end{bmatrix}\\). With \\(r=2\\), one topic leans on {color,fabric}, the other on {fabric,size}. Multiplicative updates separate them over iterations.</p>"},{"location":"notes/lecture3/#9-quick-reference-cheatsheet","title":"9) Quick reference (cheatsheet)","text":"<ul> <li>SVD: \\(X=U\\Sigma V^T\\); rank\u2011k truncation is optimal (EYM theorem).</li> <li>PCA: SVD of centered data; scores = \\(U_k^T X_c\\); reconstruction = \\(U_k U_k^T X_c + \\mu\\).</li> <li>MF (ALS): solve \\((U^{(j)T}U^{(j)}+\\lambda I)v_j=U^{(j)T}r^{(j)}\\); symmetric for \\(u_i\\).</li> <li>NMF: multiplicative updates; non\u2011negativity \u21d2 parts\u2011based, interpretable factors.</li> </ul>"},{"location":"notes/lecture3/#appendix-a-notation","title":"Appendix A \u2014 Notation","text":"<ul> <li>\\(m\\): features; \\(n\\): samples; \\(r\\): reduced rank.</li> <li>\\(\\|\\cdot\\|_F\\): Frobenius norm; \\(\\|\\cdot\\|_2\\): spectral norm.</li> <li>\\(1\\): all\u2011ones vector; \\(I\\): identity.</li> </ul>"},{"location":"notes/lecture4/","title":"Unsupervised Learning","text":"<p>Unsupervised learning is a type of machine learning where the model is trained on unlabeled data. The goal is to discover hidden patterns, structures, or representations in the data without explicit supervision.</p>"},{"location":"notes/lecture4/#types-of-unsupervised-learning","title":"Types of Unsupervised Learning","text":"<ol> <li> <p>Dimensionality Reduction    Transformation of data from high-dimensional vector space to low-dimensional space without losing important information.</p> </li> <li> <p>Clustering    Grouping input data points into clusters based on similarity.</p> </li> <li> <p>Generative Modelling    Modeling the underlying data distribution and generating new data points from it.</p> </li> <li> <p>Representation Learning    Learning low-dimensional semantic representations of data.</p> </li> </ol>"},{"location":"notes/lecture4/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>Dimensionality reduction reduces the number of features while retaining essential information.</p> <ul> <li>Common methods: PCA, t\u2011SNE, Matrix factorization (NMF/SVD), Autoencoders</li> <li>PCA (Principal Component Analysis):   Projects data onto new axes that capture maximum variance.</li> <li>In PCA, the projection axes correspond to the eigenvectors of the covariance matrix.</li> </ul>"},{"location":"notes/lecture4/#mathematical-view","title":"Mathematical View","text":"<p>Given zero-mean data matrix \\(X\\in\\mathbb{R}^{n\\times d}\\):</p> <ol> <li>Center the data.</li> <li>Compute covariance matrix:    $$ C = \\frac{1}{n} X^T X $$</li> <li>Compute eigenvalues and eigenvectors of \\(C\\).</li> <li>Project data onto top-\\(k\\) eigenvectors.</li> </ol>"},{"location":"notes/lecture4/#pca-example-concept","title":"PCA Example (concept)","text":"<pre><code>%%{init: {'theme': 'default'}}%%\nflowchart LR\n    A[Raw 2D Data Distribution] --&gt; B[Projection on Primary Eigenvector]\n    A --&gt; C[Projection on Secondary Eigenvector]</code></pre>"},{"location":"notes/lecture4/#pca-minimal-python","title":"PCA: Minimal Python","text":"<pre><code>import numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.datasets import load_iris\n\nX, y = load_iris(return_X_y=True)\nX = X - X.mean(axis=0)  # center\n\npca = PCA(n_components=2, random_state=0)\nZ = pca.fit_transform(X)\n\nprint(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n# Z can be plotted or used downstream (e.g., clustering)\n</code></pre>"},{"location":"notes/lecture4/#clustering","title":"Clustering","text":"<p>Definition: Clustering is the task of grouping a set of objects so that objects within the same group are more similar to each other than to objects in other groups.</p>"},{"location":"notes/lecture4/#types-of-clustering-models","title":"Types of Clustering Models","text":"<ol> <li>Centroid-based </li> <li>Clusters are represented by a central vector (centroid).  </li> <li>Each data point belongs to the cluster with the closest centroid.  </li> <li> <p>Examples: K-means, K-medoids.</p> </li> <li> <p>Connectivity-based (Hierarchical) </p> </li> <li>Builds a hierarchy of clusters based on distances.  </li> <li>Two main strategies:  <ul> <li>Agglomerative: Start with each point as a cluster and merge.</li> <li>Divisive: Start with one big cluster and split recursively.  </li> </ul> </li> <li> <p>Produces a dendrogram.</p> </li> <li> <p>Graph-based </p> </li> <li>Models data as a graph where nodes are data points and edges represent similarity.  </li> <li> <p>Partitions graph into communities using methods like spectral clustering or minimum cut.</p> </li> <li> <p>Distribution-based </p> </li> <li>Assumes data is generated from a mixture of probability distributions.  </li> <li>Learns parameters of distributions using methods like Expectation-Maximization (EM).  </li> <li> <p>Example: Gaussian Mixture Models (GMMs).</p> </li> <li> <p>Density-based </p> </li> <li>Defines clusters as high-density regions separated by low-density regions.  </li> <li>Can discover arbitrarily shaped clusters and handle noise/outliers.  </li> <li> <p>Examples: DBSCAN, OPTICS.</p> </li> <li> <p>Others </p> </li> <li>Grid-based: Divides space into a grid and identifies dense cells.  </li> <li>Neural network\u2013based: e.g., Self-Organizing Maps (SOM).  </li> <li>Soft clustering: Assigns probabilities of belonging to clusters.  </li> <li>Mutual information clustering: Groups data by maximizing shared information.</li> </ol>"},{"location":"notes/lecture4/#clustering-minimal-python-snippets","title":"Clustering: Minimal Python Snippets","text":"<p>K-means / Elbow &amp; Silhouette <pre><code>import numpy as np\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nX = np.random.RandomState(0).randn(500, 2)\n\ninertias, silhouettes = [], []\nfor k in range(2, 10):\n    km = KMeans(n_clusters=k, n_init=10, random_state=0)\n    labels = km.fit_predict(X)\n    inertias.append(km.inertia_)\n    silhouettes.append(silhouette_score(X, labels))\n\nbest_k = 2 + int(np.argmax(silhouettes))\nprint({\"best_k_by_silhouette\": best_k})\n</code></pre></p> <p>Hierarchical (Agglomerative) &amp; Dendrogram <pre><code>from sklearn.datasets import make_blobs\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nimport matplotlib.pyplot as plt\n\nX, _ = make_blobs(n_samples=80, centers=3, cluster_std=0.60, random_state=0)\nZ = linkage(X, method=\"ward\")\n# In a notebook, you would: dendrogram(Z); plt.show()\n</code></pre></p> <p>DBSCAN (density-based) <pre><code>from sklearn.cluster import DBSCAN\n\nX, _ = make_blobs(n_samples=300, centers=3, cluster_std=(0.3, 0.5, 0.7), random_state=42)\nlabels = DBSCAN(eps=0.5, min_samples=5).fit_predict(X)\nprint(\"Clusters found (including -1 for noise):\", set(labels))\n</code></pre></p>"},{"location":"notes/lecture4/#k-means-clustering","title":"K-means Clustering","text":"<p>Definition: A method of grouping \\(N\\) data points into \\(K\\) clusters, where each data point belongs to the nearest cluster center based on a distance metric.</p>"},{"location":"notes/lecture4/#objective","title":"Objective","text":"<p>Minimize the within-cluster sum of squares (WCSS): \\(\\(\\min_{\\{C_k,\\mu_k\\}} \\sum_{k=1}^K \\sum_{x_i\\in C_k} \\lVert x_i-\\mu_k \\rVert^2\\)\\)</p>"},{"location":"notes/lecture4/#k-means-algorithm","title":"K-means Algorithm","text":"<ol> <li>Choose number of clusters \\(k\\).</li> <li>Initialize \\(k\\) cluster centers (e.g., random or k-means++).</li> <li>Assign each data point to its nearest center (by chosen distance metric).</li> <li>Update each center to be the centroid: \\(\\mu_k=\\frac{1}{|C_k|}\\sum_{x_i\\in C_k} x_i\\).</li> <li>Repeat steps 3\u20134 until convergence (centers stop moving or WCSS improvement \\(&lt;\\varepsilon\\)).</li> </ol> <pre><code>%%{init: {'theme': 'forest'}}%%\nflowchart TD\n    A[Initialize k cluster centers] --&gt; B[Assign points to nearest cluster]\n    B --&gt; C[Update cluster centers]\n    C --&gt; D{Converged?}\n    D -- No --&gt; B\n    D -- Yes --&gt; E[Final Clusters]</code></pre>"},{"location":"notes/lecture4/#k-means-minimal-python","title":"K-means: Minimal Python","text":"<pre><code>from sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\nX, _ = make_blobs(n_samples=400, centers=4, random_state=7)\nkm = KMeans(n_clusters=4, n_init=10, random_state=0)\nlabels = km.fit_predict(X)\ncenters = km.cluster_centers_\nprint(\"WCSS (inertia):\", km.inertia_)\n</code></pre>"},{"location":"notes/lecture4/#k-means-initialization-sketch","title":"K-means++ Initialization (sketch)","text":"<pre><code>import numpy as np\n\nrng = np.random.default_rng(0)\n# X shape: (n_samples, d)\ncenters = []\ncenters.append(X[rng.integers(0, len(X))])\nfor _ in range(1, 4):  # pick until K centers\n    d2 = np.min(((X[:, None, :] - np.array(centers)[None, :, :])**2).sum(axis=2), axis=1)\n    probs = d2 / d2.sum()\n    centers.append(X[rng.choice(len(X), p=probs)])\ncenters = np.array(centers)\n</code></pre>"},{"location":"notes/lecture4/#sample-iteration-illustrated","title":"Sample Iteration (Illustrated)","text":"<p>Below is a step-by-step depiction of one complete iteration of K-means (for \\(K=3\\)). Use it to understand the loop that repeats until convergence.</p>"},{"location":"notes/lecture4/#0-inputs-initialization","title":"0) Inputs &amp; Initialization","text":"<ul> <li>Data points \\(\\{x_1,\\dots,x_N\\}\\) in \\(\\mathbb{R}^d\\) (unlabeled).</li> <li>Number of clusters \\(K\\).</li> <li>Initial centers \\(\\mu^{(0)}_1,\\mu^{(0)}_2,\\mu^{(0)}_3\\) (e.g., random or k-means++).</li> </ul> <pre><code>%%{init: {'theme': 'default'}}%%\nflowchart LR\n    D[Data X: x1..xN] --&gt;|choose K| K[Set K=3]\n    K --&gt; I[Init centers \u03bc1\u207d\u2070\u207e, \u03bc2\u207d\u2070\u207e, \u03bc3\u207d\u2070\u207e]</code></pre>"},{"location":"notes/lecture4/#1-assignment-step-e-step-analogue","title":"1) Assignment Step (E-step analogue)","text":"<p>For each point \\(x_i\\), choose the closest center by distance \\(d(\\cdot,\\cdot)\\) (often Euclidean): $$ a_i^{(t)} = \\arg\\min_{k\\in{1,2,3}} d\\big(x_i,\\mu_k^{(t)}\\big). $$</p> <pre><code>%%{init: {'theme': 'neutral'}}%%\nflowchart TD\n    subgraph Assignment at iteration t\n    X1[x1] --&gt;|\"d(x1, mu1^t) d(x1, mu2^t) d(x1, mu3^t)\"| A1[Assign to closest]\n    X2[x2] --&gt; A1\n    XN[xN] --&gt; A1\n    A1 --&gt; C1[C1]\n    A1 --&gt; C2[C2]\n    A1 --&gt; C3[C3]\n    end\n</code></pre>"},{"location":"notes/lecture4/#2-update-step-m-step-analogue","title":"2) Update Step (M-step analogue)","text":"<p>Recompute each center as the mean of currently assigned points: $$ \\mu_k^{(t+1)} = \\frac{1}{|C_k^{(t)}|} \\sum_{x_i\\in C_k^{(t)}} x_i. $$</p> <pre><code>%%{init: {'theme': 'base'}}%%\nflowchart LR\n    C1[C\u2081 points] --&gt; U1[Compute \u03bc\u2081\u207d\u1d57\u207a\u00b9\u207e]\n    C2[C\u2082 points] --&gt; U2[Compute \u03bc\u2082\u207d\u1d57\u207a\u00b9\u207e]\n    C3[C\u2083 points] --&gt; U3[Compute \u03bc\u2083\u207d\u1d57\u207a\u00b9\u207e]\n    U1 --&gt; M[New centers \u03bc\u207d\u1d57\u207a\u00b9\u207e]\n    U2 --&gt; M\n    U3 --&gt; M</code></pre>"},{"location":"notes/lecture4/#3-convergence-check","title":"3) Convergence Check","text":"<ul> <li>Stop if centers move less than a tolerance \\(\\varepsilon\\) or max iterations reached.</li> <li>Otherwise, set \\(t\\leftarrow t+1\\) and repeat Assignment &amp; Update.</li> </ul> <pre><code>%%{init: {'theme': 'forest'}}%%\nstateDiagram-v2\n    [*] --&gt; Assign: Step 1\n    Assign --&gt; Update: Step 2\n    Update --&gt; Check: Step 3\n    Check --&gt; Assign: shift(\u03bc)&lt;\u03b5\n    Check --&gt; [*]: converged</code></pre>"},{"location":"notes/lecture4/#4-full-mini-run-example-t0-t2","title":"4) Full Mini-Run Example (t=0 \u2192 t=2)","text":"<p>A compact timeline of two iterations:</p> <pre><code>%%{init: {'theme': 'neutral'}}%%\nsequenceDiagram\n    participant X as Data X\n    participant M0 as \u03bc\u207d\u2070\u207e (init)\n    participant A1 as Assign (t=0)\n    participant U1 as Update (t=0\u21921)\n    participant M1 as \u03bc\u207d\u00b9\u207e\n    participant A2 as Assign (t=1)\n    participant U2 as Update (t=1\u21922)\n    participant M2 as \u03bc\u207d\u00b2\u207e\n\n    X-&gt;&gt;M0: Choose K &amp; initialize centers\n    M0-&gt;&gt;A1: Use \u03bc\u207d\u2070\u207e to assign each x\u1d62\n    A1-&gt;&gt;U1: Build clusters C\u2081,C\u2082,C\u2083\n    U1-&gt;&gt;M1: Compute new centers \u03bc\u207d\u00b9\u207e\n    M1-&gt;&gt;A2: Re-assign with \u03bc\u207d\u00b9\u207e\n    A2-&gt;&gt;U2: Update centers again \u2192 \u03bc\u207d\u00b2\u207e\n    U2--&gt;&gt;M2: Check convergence</code></pre> <p>Notes - Distance choice matters (Euclidean vs cosine). Scale features or standardize when needed. - Random init can trap in local minima \u2192 prefer k-means++ and multiple restarts. - Use inertia (WCSS) or silhouette score to pick \\(K\\).</p>"},{"location":"notes/lecture4/#applications-of-clustering","title":"Applications of Clustering","text":"<ul> <li>Exploratory Data Analysis</li> <li>Dimensionality Reduction + Clustering</li> <li>Feature Extraction + Clustering</li> <li>Lossy Image Compression</li> <li>Post-training Model Analysis</li> </ul>"},{"location":"notes/lecture4/#image-compression-using-k-means","title":"Image Compression using K-means","text":"<ul> <li>Represent image pixels as vectors (RGB values).</li> <li>Apply K-means to group similar colors.</li> <li>Replace pixels by their cluster centers \u2192 reduces color space \u2192 compression.</li> </ul> <pre><code>%%{init: {'theme': 'default'}}%%\nflowchart LR\n    A[Original Image] --&gt; B[K-means Clustering]\n    B --&gt; C[Compressed Image]</code></pre>"},{"location":"notes/lecture4/#minimal-python-for-color-quantization","title":"Minimal Python for Color Quantization","text":"<pre><code>import numpy as np\nfrom sklearn.cluster import KMeans\nfrom PIL import Image\n\nimg = Image.open(\"path/to/image.jpg\").convert(\"RGB\")\narr = np.array(img).reshape(-1, 3)\n\nK = 16  # number of colors\nkm = KMeans(n_clusters=K, n_init=5, random_state=0).fit(arr)\npalette = km.cluster_centers_.astype(np.uint8)\nlabels = km.labels_\ncompressed = palette[labels].reshape(np.array(img).shape)\n\nImage.fromarray(compressed).save(\"compressed_k16.jpg\")\n</code></pre>"},{"location":"notes/lecture4/#generative-modelling","title":"Generative Modelling","text":""},{"location":"notes/lecture4/#what-is-generative-modelling","title":"What is Generative Modelling?","text":"<ul> <li>A generative model learns the distribution of the input data.</li> <li>Once you have a model of input data, you can:</li> <li>Generate new examples</li> <li>Perform classification/regression (with small labels)</li> <li>Anomaly detection</li> <li>Fill missing data</li> <li>Examples: Density estimation models, Na\u00efve Bayes, Variational Autoencoders (VAE).</li> </ul>"},{"location":"notes/lecture4/#python-example-naive-bayes-generative-for-classification","title":"Python Example: Na\u00efve Bayes (generative for classification)","text":"<pre><code>from sklearn.naive_bayes import GaussianNB\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nX, y = load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nnb = GaussianNB().fit(X_train, y_train)\ny_pred = nb.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n</code></pre>"},{"location":"notes/lecture4/#generative-adversarial-networks-gans","title":"Generative Adversarial Networks (GANs)","text":"<ul> <li>Deep-learning based generative models.</li> <li>Two components:</li> <li>Generator (G): Produces synthetic data from noise.</li> <li>Discriminator (D): Distinguishes between real and fake samples.</li> <li>GANs are trained as a two-player minimax game:</li> </ul> <p>\\(\\min_G \\max_D V(D,G) = \\mathbb{E}_{x\\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z\\sim p_z(z)}[\\log(1-D(G(z)))]\\)</p>"},{"location":"notes/lecture4/#intuition-two-player-game","title":"Intuition: Two-Player Game","text":"<ul> <li>Criminal (Generator): Tries to produce counterfeit money indistinguishable from real money.</li> <li>Cop (Discriminator): Tries to detect fake money.</li> <li>Over iterations, both improve until the generator produces realistic samples.</li> </ul> <pre><code>%%{init: {'theme': 'forest'}}%%\nflowchart LR\n    Z[Random Noise z] --&gt; G[Generator]\n    G --&gt; X_fake[Fake Sample]\n    X_real[Real Sample] --&gt; D[Discriminator]\n    X_fake --&gt; D\n    D --&gt;|Real or Fake| Out[Training Feedback]\n    Out --&gt; G</code></pre>"},{"location":"notes/lecture4/#gan-training-algorithm-mini-batch-sgd","title":"GAN Training Algorithm (Mini-batch SGD)","text":"<ol> <li> <p>For k steps:</p> </li> <li> <p>Sample minibatch of $m$ noise vectors $z^{(1)},..,z^{(m)}$.</p> </li> <li>Sample minibatch of $m$ real examples $x^{(1)},..,x^{(m)}$.</li> <li> <p>Update discriminator $D$ by ascending: $\\nabla_\\theta \\frac{1}{m}\\sum_{i=1}^m [\\log D(x^{(i)}) + \\log(1-D(G(z^{(i)})))]$</p> </li> <li> <p>Update generator $G$ by descending: $\\nabla_\\theta \\frac{1}{m}\\sum_{i=1}^m [\\log(1-D(G(z^{(i)})))]$</p> </li> </ol>"},{"location":"notes/lecture4/#gan-sample-iteration-visualization","title":"GAN Sample Iteration (Visualization)","text":"<ul> <li>Iteration 0: Generator produces random noise.</li> <li>Iteration 1000: Generated samples start to mimic structure of data.</li> <li>Iteration 2000+: Generated distribution approximates real data.</li> </ul> <pre><code>%%{init: {'theme': 'neutral'}}%%\nsequenceDiagram\n    participant Z as Noise z\n    participant G as Generator\n    participant X_fake as Fake Data\n    participant D as Discriminator\n    participant Real as Real Data\n\n    Z-&gt;&gt;G: Generate G(z)\n    G-&gt;&gt;X_fake: Fake samples\n    Real-&gt;&gt;D: Pass real samples\n    X_fake-&gt;&gt;D: Pass fake samples\n    D-&gt;&gt;G: Gradient feedback</code></pre>"},{"location":"notes/lecture4/#minimal-python-gan-pytorch","title":"Minimal Python GAN (PyTorch)","text":"<pre><code>import torch, torch.nn as nn, torch.optim as optim\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(100, 128), nn.ReLU(),\n            nn.Linear(128, 784), nn.Tanh())\n    def forward(self, z): return self.net(z)\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(784, 128), nn.LeakyReLU(0.2),\n            nn.Linear(128, 1), nn.Sigmoid())\n    def forward(self, x): return self.net(x)\n\nG, D = Generator(), Discriminator()\noptG = optim.Adam(G.parameters(), lr=0.0002)\noptD = optim.Adam(D.parameters(), lr=0.0002)\nloss_fn = nn.BCELoss()\n\nz = torch.randn(16, 100)\nfake = G(z)\nprint(fake.shape)  # (16, 784)\n</code></pre>"},{"location":"notes/lecture4/#conditional-gans-cgans","title":"Conditional GANs (cGANs)","text":"<ul> <li>GANs conditioned on additional input $y$ (labels, class, or attributes).</li> <li>Generator learns mapping $G(z|y)$, Discriminator learns $D(x|y)$.</li> <li>Example: Generate digit images conditioned on label (0\u20139).</li> </ul> <pre><code>%%{init: {'theme': 'forest'}}%%\nflowchart TB\n    subgraph cGAN\n    Z[Noise z] --&gt; G[\"Generator G(z|y)\"]\n    Y1[Condition y] --&gt; G\n    G --&gt; X_fake[Fake Sample]\n    X_real[Real Sample] --&gt; D[\"Discriminator D(x|y)\"]\n    Y2[Condition y] --&gt; D\n    X_fake --&gt; D\n    D --&gt; Out[Real/Fake conditioned]\n    Out --&gt; G\n    end\n</code></pre>"},{"location":"notes/lecture4/#pytorch-snippet-for-cgan","title":"PyTorch Snippet for cGAN","text":"<pre><code>class CondGenerator(nn.Module):\n    def __init__(self, n_classes, z_dim=100):\n        super().__init__()\n        self.embed = nn.Embedding(n_classes, 10)\n        self.net = nn.Sequential(\n            nn.Linear(z_dim+10, 128), nn.ReLU(),\n            nn.Linear(128, 784), nn.Tanh())\n    def forward(self, z, y):\n        y_embed = self.embed(y)\n        return self.net(torch.cat([z, y_embed], dim=1))\n</code></pre>"},{"location":"notes/lecture4/#applications-of-gans","title":"Applications of GANs","text":"<p>GANs have been successfully applied to a wide variety of domains:</p> <ol> <li> <p>Super-Resolution</p> </li> <li> <p>Enhancing low-resolution images to high resolution.</p> </li> <li> <p>Example: SRGAN.</p> </li> <li> <p>Connectivity-based</p> </li> <li> <p>Semantic segmentation and label-to-image translation.</p> </li> <li> <p>Example: GauGAN.</p> </li> <li> <p>Graph-based</p> </li> <li> <p>Filling missing regions in images (image inpainting).</p> </li> <li> <p>Example: Context encoders.</p> </li> <li> <p>Distribution-based</p> </li> <li> <p>Style transfer, domain adaptation, aerial-to-map, map-to-aerial transformations.</p> </li> <li> <p>Density-based</p> </li> <li> <p>Face generation, avatar synthesis, human image generation.</p> </li> <li> <p>Others</p> </li> <li> <p>Image editing</p> </li> <li>Imitation learning</li> <li>Music generation</li> </ol>"},{"location":"notes/lecture4/#representation-learning","title":"Representation Learning","text":"<p>Representation learning is a set of techniques in machine learning that enables a system to automatically discover useful representations of raw data. These representations transform raw inputs into more structured formats that make downstream tasks such as feature detection, classification, or regression easier.</p> <p>Instead of manually engineering features, representation learning techniques automatically identify the right abstractions.</p>"},{"location":"notes/lecture4/#types-of-representation-learning","title":"Types of Representation Learning","text":"<ol> <li>Text Representations</li> <li>Converts raw text into vectors that encode semantic meaning.</li> <li>Captures word similarity, syntactic roles, and semantic context.</li> <li> <p>Examples:</p> <ul> <li>Word2Vec (continuous representations of words)</li> <li>BERT (contextual embeddings)</li> </ul> </li> <li> <p>Graph Representations</p> </li> <li>Encodes nodes and edges into dense vectors.</li> <li>Preserves structural information such as neighborhood, connectivity, and paths.</li> <li> <p>Examples:</p> <ul> <li>DeepWalk (random walks + Skip-gram)</li> <li>node2vec (biased random walks for flexible embeddings)</li> </ul> </li> <li> <p>Image Representations</p> </li> <li>Learns features automatically from pixels.</li> <li>Self-supervised contrastive learning extracts features without labeled data.</li> <li>Enables clustering and transfer learning across visual tasks.</li> </ol> <pre><code>flowchart LR\n    subgraph Text\n      A[\"Raw Text corpus\"] --&gt; B[\"Tokenizer\"]\n      B --&gt; C[\"Context Windows\"]\n      C --&gt; D[\"Embedding Model&lt;br&gt;Word2Vec / BERT\"]\n      D --&gt; E[\"Downstream Tasks\"]\n    end\n    subgraph Graph\n      G1[\"Graph Data&lt;br&gt;nodes &amp; edges\"] --&gt; G2[\"Random Walks\"]\n      G2 --&gt; G3[\"Skip-gram Training\"]\n      G3 --&gt; G4[\"Node Embeddings\"]\n    end\n    subgraph Image\n      I1[\"Unlabeled Images\"] --&gt; I2[\"Augmentations\"]\n      I2 --&gt; I3[\"Contrastive Objective\"]\n      I3 --&gt; I4[\"Image Embeddings\"]\n    end\n</code></pre>"},{"location":"notes/lecture4/#distributed-representations","title":"Distributed Representations","text":"<p>A core idea in representation learning is moving from sparse representations (e.g., one-hot vectors) to dense distributed representations (embeddings).</p> <ul> <li>One-hot representation (Sparse)</li> <li>Each word is represented as a binary vector with a single 1.</li> <li>Does not capture similarity between words.</li> <li> <p>High dimensional and memory inefficient.   <pre><code>banana = [0 0 0 0 0 0 0 1 0 0 0]\nmango  = [0 0 0 0 0 1 0 0 0 0 0]\ndog    = [0 0 1 0 0 0 0 0 0 0 0]\n</code></pre></p> </li> <li> <p>Distributed representation (Dense)</p> </li> <li>Each word is a low-dimensional vector of real numbers.</li> <li>Similar words have similar vectors.</li> <li>Captures semantic and syntactic properties.   <pre><code>banana = [0.23  0.1 -0.1 -0.4 -0.01 -0.121 0.342 0.561]\nmango  = [-0.73 0.0 -0.5  0.4  0.4   0.591 0.732 0.891]\ndog    = [0.61  0.21 0.55 0.45 0.134 0.752 0.525 0.64 ]\n</code></pre></li> </ul> <pre><code>flowchart LR\n    O1[One-hot Vectors\\nHigh-dim, sparse] -- limitations --&gt; L1[No similarity\\ninfo]\n    O1 --&gt; L2[Large memory]\n    D1[Dense Embeddings\\nLow-dim, real-valued] -- benefits --&gt; B1[Capture similarity]\n    D1 --&gt; B2[Generalize across contexts]\n    D1 --&gt; B3[Efficient storage]</code></pre>"},{"location":"notes/lecture4/#word2vec-word-representations","title":"Word2Vec: Word Representations","text":"<p>\u201cYou shall know a word by the company it keeps\u201d \u2013 J.R. Firth (1957)</p> <p>Word2Vec is a neural model that learns word embeddings by leveraging context.</p> <ul> <li>A word\u2019s meaning is inferred from its neighboring words in a large corpus.</li> <li>Example:</li> <li>Target word: banking </li> <li>Context words: <code>w_{t-1}, w_{t+1}, ...</code></li> </ul> <p>Word2Vec comes in two variants: CBOW and Skip-gram.</p>"},{"location":"notes/lecture4/#basic-idea-of-word2vec","title":"Basic Idea of Word2Vec","text":"<p>Input assumptions: - A huge corpus of text \\(C\\) - A pre-defined vocabulary \\(V\\)</p> <p>Representation: - Each word has two vectors:   - \\( v_{w_t} \\): word as a target   - \\( v_{w_c} \\): word as a context</p>"},{"location":"notes/lecture4/#models","title":"Models","text":"<ol> <li>Continuous Bag of Words (CBOW)</li> <li>Predicts the target word given surrounding context words.</li> <li> <p>Uses softmax over similarity between the target vector and the sum of context vectors.</p> </li> <li> <p>Skip-gram (SG)</p> </li> <li>Predicts the context words given a target word.</li> <li>Uses softmax over similarity between target and context vectors.</li> </ol> <pre><code>flowchart LR\n    subgraph CBOW\n        w1((\"w_{t-2}\")) --&gt; SUM((\"\u03a3\"))\n        w2((\"w_{t-1}\")) --&gt; SUM\n        w3((\"w_{t+1}\")) --&gt; SUM\n        w4((\"w_{t+2}\")) --&gt; SUM\n        SUM --&gt; target((\"w_t\"))\n    end\n\n    subgraph SkipGram\n        target2((\"w_t\")) --&gt; c1((\"w_{t-2}\"))\n        target2 --&gt; c2((\"w_{t-1}\"))\n        target2 --&gt; c3((\"w_{t+1}\"))\n        target2 --&gt; c4((\"w_{t+2}\"))\n    end\n</code></pre>"},{"location":"notes/lecture4/#skip-gram-with-window","title":"Skip-gram with Window","text":"<p>Skip-gram works by predicting context words within a fixed window size around the target word.</p> <p>For example, with a window size of 2:</p> \\[ P(w_{t-2} | w_t), P(w_{t-1} | w_t), P(w_{t+1} | w_t), P(w_{t+2} | w_t) \\] <pre><code>flowchart LR\n    T((\"banking&lt;br&gt;w_t\")) --&gt; L1((\"problems&lt;br&gt;w_{t-2}\"))\n    T --&gt; L2((\"turning&lt;br&gt;w_{t-1}\"))\n    T --&gt; R1((\"crises&lt;br&gt;w_{t+1}\"))\n    T --&gt; R2((\"as&lt;br&gt;w_{t+2}\"))\n\n    classDef ctx fill:#fff,stroke:#999,stroke-width:1px;\n    class L1,L2,R1,R2 ctx;\n</code></pre> <p>This ensures both left and right contexts contribute to training.</p>"},{"location":"notes/lecture4/#skip-gram-objective-function","title":"Skip-gram Objective Function","text":"<p>The optimization goal is:</p> \\[ \\theta^* = \\arg\\max_{\\theta} \\prod_{i=1}^{T} \\prod_{-m \\leq j \\leq m, j \\neq 0} P(w_{i+j} | w_i; \\theta) \\] <p>Equivalent minimization:</p> \\[ \\theta^* = \\arg\\min_{\\theta} -\\frac{1}{T}\\sum_{i=1}^T \\sum_{-m \\leq j \\leq m, j \\neq 0} \\log P(w_{i+j} | w_i; \\theta) \\] <p>Where probability is defined as:</p> \\[ P(w_{i+j} | w_i) = \\frac{\\exp(v_{w_i}^T v_{w_{i+j}})}{\\sum_{w \\in V} \\exp(v_{w_i}^T v_{w_c})} \\] <ul> <li>Similarity: computed as dot product of target and context vectors.</li> <li>Normalization: softmax ensures probabilities sum to 1 over the entire vocabulary.</li> </ul> <pre><code>flowchart TB\n    subgraph \"Training Step\"\n        A[\"Target word&lt;br&gt;v_{w_t}\"] --&gt; D[\"Dot Product\"]\n        C[\"Context word&lt;br&gt;v_{w_c}\"] --&gt; D\n        D --&gt; S[\"Score\"]\n        S --&gt; SM[\"Softmax /&lt;br&gt;Neg. Sampling\"]\n        SM --&gt; L[\"Loss\"]\n        L --&gt;|backprop| A\n        L --&gt;|backprop| C\n    end\n</code></pre>"},{"location":"notes/lecture4/#word2vec-properties","title":"Word2Vec Properties","text":"<ul> <li>Embeddings place similar words close in vector space.</li> <li>Captures rich semantic relationships, for example:</li> <li>Gender relationship: <code>king - man + woman \u2248 queen</code></li> <li>Verb tense: <code>walked - walking \u2248 swam - swimming</code></li> <li>Geographical relation: <code>Paris - France \u2248 Berlin - Germany</code></li> </ul> <pre><code>flowchart LR\n    K[king] --- M[man]\n    Q[queen] --- W[woman]\n    subgraph Analogies\n      direction LR\n      K --&gt;| - man + woman | Q\n    end\n    subgraph Tense\n      direction LR\n      Walked[walked] --&gt;| - walking | Delta1[\u0394]\n      Swam[swam] --&gt;| - swimming | Delta2[\u0394]\n      Delta1 --- Delta2\n    end\n    subgraph Capitals\n      direction LR\n      France --- Paris\n      Germany --- Berlin\n      France --&gt;|offset \u2248| Germany\n      Paris --&gt;|offset \u2248| Berlin\n    end</code></pre> <p>These relationships emerge naturally without explicit supervision.</p>"},{"location":"notes/lecture4/#negative-sampling","title":"Negative Sampling","text":"<p>A challenge in Word2Vec is that the softmax denominator involves summing over the entire vocabulary, which is computationally expensive for large vocabularies.</p> <p>Solution: Negative Sampling (a form of NCE) - Instead of updating all weights, only update a few sampled negative examples at each step. - The model learns to discriminate between:   - Positive (real context) pairs   - Negative (random noise) pairs</p> <p>New simplified objective:</p> \\[ \\log \\sigma(v_{w_t}^T v_{w_{c}}) + \\sum_{l=1}^k \\mathbb{E}_{w_l \\sim P_n(w)} [\\log \\sigma(-v_{w_t}^T v_{w_l})] \\] <p>Where the noise distribution is:</p> \\[ P_n(w) \\propto U(w)^{3/4} \\] <pre><code>flowchart LR\n    subgraph Sampling\n      T((\"Target&lt;br&gt;w_t\")) --&gt; P1((\"Pos Context&lt;br&gt;w_c\"))\n      T --&gt; N1((\"Neg 1\"))\n      T --&gt; N2((\"Neg 2\"))\n      T --&gt; Nk((\"Neg k\"))\n    end\n\n    P1 --&gt; L1[\"log \u03c3(v_t^T v_c)\"]\n    N1 --&gt; L2[\"log \u03c3(-v_t^T v_{n1})\"]\n    N2 --&gt; L3[\"log \u03c3(-v_t^T v_{n2})\"]\n    Nk --&gt; Lk[\"log \u03c3(-v_t^T v_{nk})\"]\n\n    L1 --&gt; SUM((\"Sum Loss\"))\n    L2 --&gt; SUM\n    L3 --&gt; SUM\n    Lk --&gt; SUM\n</code></pre> <p>This reduces training complexity while preserving embedding quality.</p>"},{"location":"notes/lecture4/#how-deepwalk-node2vec-connect-to-skip-gram","title":"How DeepWalk / node2vec Connect to Skip-gram","text":"<p>Both methods generate node sequences via random walks and then train a Skip-gram model on those sequences\u2014treating walks like sentences.</p> <pre><code>flowchart LR\n    G[Graph] --&gt; RW[\"Random Walks&lt;br&gt;(sequences of nodes)\"]\n    RW --&gt; SG[\"Skip-gram Training\"]\n    SG --&gt; Z[\"Node Embeddings\"]\n\n    style G stroke-width:2px\n</code></pre>"},{"location":"notes/lecture4/#summary","title":"Summary","text":"<ul> <li>Representation learning eliminates the need for manual feature engineering.</li> <li>Dense embeddings capture semantic, syntactic, and relational properties of data.</li> <li>Word2Vec is a landmark method for learning text representations using CBOW and Skip-gram.</li> <li>Techniques like negative sampling make large-scale training feasible.</li> <li>Graph and image modalities use analogous pipelines to learn meaningful embeddings without heavy labeling.</li> </ul> <p>Representation learning underpins many modern AI systems, from NLP to computer vision and graph learning.</p>"},{"location":"notes/lecture4/#advanced-topics-in-representation-learning","title":"Advanced Topics in Representation Learning","text":""},{"location":"notes/lecture4/#contextual-word-embeddings","title":"Contextual Word Embeddings","text":"<p>Traditional embeddings like Word2Vec assign a single vector per word, regardless of context. However, the same word can mean different things in different contexts:</p> <ul> <li>open a bank account</li> <li>on the river bank**</li> </ul> <p>This motivates contextual embeddings, which adapt to surrounding words.</p>"},{"location":"notes/lecture4/#elmo-embeddings-from-language-models","title":"ELMo (Embeddings from Language Models)","text":"<ul> <li>ELMo uses a 2-layer bidirectional LSTM (biLM) trained as a language model.</li> <li>Unlike Word2Vec, which gives static embeddings, ELMo embeddings are dynamic and depend on the entire sentence context.</li> <li>Produces better representations for polysemous words.</li> </ul> <p>Training Objective:</p> \\[ \\sum_{k=1}^{N} \\Big( \\log p(t_k | t_1, \u2026, t_{k-1}; \\theta_{LSTM}^{\u2192}) + \\log p(t_k | t_{k+1}, \u2026, t_N; \\theta_{LSTM}^{\u2190}) \\Big) \\] <pre><code>flowchart TB\n    E1[Input E1] --&gt; L1[LSTM Forward]\n    E2[Input E2] --&gt; L1\n    E3[Input E3] --&gt; L1\n    E1 --&gt; L2[LSTM Backward]\n    E2 --&gt; L2\n    E3 --&gt; L2\n    L1 --&gt; O1[Contextual Embeddings]\n    L2 --&gt; O1</code></pre>"},{"location":"notes/lecture4/#bert-bidirectional-encoder-representations-from-transformers","title":"BERT (Bidirectional Encoder Representations from Transformers)","text":"<ul> <li>Uses the Transformer architecture with multi-head self-attention.</li> <li>Generates bidirectional contextual embeddings, allowing words to \"see\" both left and right contexts simultaneously.</li> <li>Pretrained on two tasks:</li> <li>Masked Language Modeling (MLM): randomly mask tokens and predict them.</li> <li>Next Sentence Prediction (NSP): predict if sentence B follows sentence A.</li> </ul> <p>Advantages: - Deep bidirectional attention captures richer context than LSTMs. - Strong transfer learning: pretrained weights can be fine-tuned on many downstream NLP tasks.</p> <pre><code>flowchart TB\n    subgraph BERT Pretraining\n      direction TB\n      A[Input Tokens] --&gt; M1[Masked Tokens]\n      M1 --&gt; T[Transformer Layers]\n      T --&gt; MLM[Masked LM Prediction]\n      A2[Sentence A] --&gt; T\n      B2[Sentence B] --&gt; T\n      T --&gt; NSP[Next Sentence Prediction]\n    end</code></pre>"},{"location":"notes/lecture4/#bert-pretraining-tasks","title":"BERT Pretraining Tasks","text":"<ol> <li>Masked Language Model (MLM):</li> <li>Randomly mask 15% of tokens.</li> <li>Of these: 80% replaced with [MASK], 10% replaced with random tokens, 10% unchanged.</li> <li> <p>Example:</p> <ul> <li>Input: the man went to the [MASK] to buy a [MASK] of milk</li> <li>Predictions: <code>store</code>, <code>gallon</code></li> </ul> </li> <li> <p>Next Sentence Prediction (NSP):</p> </li> <li>Sentence A: The man went to the store.</li> <li>Sentence B: He bought a gallon of milk. \u2192 Label = IsNextSentence</li> <li>Sentence B: Penguins are flightless. \u2192 Label = NotNextSentence</li> </ol>"},{"location":"notes/lecture4/#graph-embeddings-why","title":"Graph Embeddings \u2013 Why?","text":"<p>Graph data is ubiquitous: - Knowledge Graphs (search engines, question answering) - Social Graphs (friend/follow recommendations) - Recommendation Systems (products, items, users)</p> <pre><code>flowchart LR\n    A((Alice)) --&gt;|visits| B((Paris))\n    A --&gt;|is interested in| C((Mona Lisa))\n    C --&gt;|is created by| D((Leonardo da Vinci))</code></pre>"},{"location":"notes/lecture4/#graph-basics","title":"Graph Basics","text":"<p>A graph is defined as:</p> <p>[ G = (V, E) ] - \\( V \\): set of vertices (nodes) - \\( E \\subset V \\times V \\): edges (connections)</p> <p>Modalities: - Node embeddings - Edge embeddings - Subgraph embeddings - Whole-graph embeddings</p> <p>Goal: learn embeddings that preserve graph structure while being useful for downstream tasks.</p>"},{"location":"notes/lecture4/#deepwalk","title":"DeepWalk","text":"<ul> <li>Generates random walks on the graph.</li> <li>Treats walks as sentences and applies Skip-gram for training node embeddings.</li> <li>Preserves both local and global graph structures.</li> </ul> <pre><code>flowchart LR\n    subgraph DeepWalk\n      N1((v1)) --&gt; N2((v5)) --&gt; N3((v7)) --&gt; N4((v2))\n    end\n    DeepWalk --&gt; SG[Skip-gram Training]\n    SG --&gt; Emb[Node Embeddings]</code></pre> <p>Algorithm (simplified): 1. Perform multiple random walks per node. 2. Train Skip-gram on sequences of visited nodes. 3. Output low-dimensional node embeddings.</p>"},{"location":"notes/lecture4/#node2vec","title":"Node2Vec","text":"<p>Node2Vec extends DeepWalk with a flexible biased random walk strategy: - Parameters:   - \\(p\\): return parameter (exploration vs revisiting)   - \\(q\\): in-out parameter (BFS vs DFS bias)</p> <ul> <li>If \\(p &gt; 1 &gt; q\\): encourages global exploration.</li> <li>If \\(p &lt; 1 &lt; q\\): encourages local exploration.</li> </ul> <pre><code>flowchart LR\n    U((u)) --&gt;|BFS| S1((s1))\n    U --&gt;|BFS| S2((s2))\n    U --&gt;|DFS| S3((s3))\n    U --&gt;|DFS| S4((s4))</code></pre>"},{"location":"notes/lecture4/#node2vec-community-vs-structural-roles","title":"Node2Vec Community vs Structural Roles","text":"<ul> <li>By tuning \\(p, q\\), Node2Vec can:</li> <li>Capture community structure (nodes in the same cluster).</li> <li>Capture structural equivalence (nodes with similar roles, even in different communities).</li> </ul> <pre><code>flowchart TB\n    subgraph Community Structure\n      A1((Node A)) --- A2((Node B)) --- A3((Node C))\n      A2 --- A4((Node D))\n    end\n    subgraph Structural Equivalence\n      B1((Hub 1)) --- X((Peripheral))\n      B2((Hub 2)) --- X\n    end</code></pre>"},{"location":"notes/lecture4/#image-representations","title":"Image Representations","text":""},{"location":"notes/lecture4/#contrastive-learning","title":"Contrastive Learning","text":"<ul> <li>Goal: learn an embedding space where similar pairs are close and dissimilar pairs are far apart.</li> <li>Given anchor \\(x\\), positive \\(x^+\\), and negative \\(x^-\\):</li> </ul> <p>Contrastive Loss: \\(\\lVert x - x^+ \\rVert^2 + \\max(0, m - \\lVert x - x^- \\rVert^2)\\)</p> <p>Triplet Loss: \\(\\max(0, \\lVert x - x^+ \\rVert^2 - \\lVert x - x^- \\rVert^2 + m)\\)</p> <pre><code>flowchart LR\n    A[Anchor x] --&gt; P[Positive x\u207a]\n    A --&gt; N[Negative x\u207b]\n    P --&gt;|close in embedding| Z[Embedding Space]\n    N --&gt;|far apart| Z</code></pre>"},{"location":"notes/lecture4/#simclr-simple-framework-for-contrastive-learning-of-representations","title":"SimCLR (Simple Framework for Contrastive Learning of Representations)","text":"<ul> <li>Framework for self-supervised learning (SSL) with contrastive loss.</li> <li>Given \\(n\\) images, generate \\(2n\\) augmented samples.</li> <li>For each positive pair, there are \\(2(n-1)\\) negative pairs.</li> </ul> <p>NT-Xent (Normalized Temperature-scaled Cross Entropy) Loss: \\(\\(\\mathcal{L}_{i,j} = - \\log \\frac{\\exp(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k=1}^{2n} 1_{[k \\neq i]} \\exp(\\text{sim}(z_i, z_k)/\\tau)}\\)\\)</p> <ul> <li>Key idea: maximize agreement between positive pairs.</li> <li>Uses strong data augmentations (cropping, flipping, color distortion, Gaussian blur, etc.).</li> </ul> <pre><code>flowchart TB\n    X[\"Input Image\"] --&gt; Aug1[\"Augmentation t\"]\n    X --&gt; Aug2[\"Augmentation t'\"]\n    Aug1 --&gt; F1[\"Encoder f(\u00b7)\"] --&gt; H1[\"Projection Head h(\u00b7)\"] --&gt; Z1[\"Embedding z_i\"]\n    Aug2 --&gt; F2[\"Encoder f(\u00b7)\"] --&gt; H2[\"Projection Head h(\u00b7)\"] --&gt; Z2[\"Embedding z_j\"]\n    Z1 --&gt; Loss[\"Contrastive Loss\"]\n    Z2 --&gt; Loss\n</code></pre> <p>Impact: - Achieved state-of-the-art in SSL, surpassing supervised ResNet-50 on ImageNet Top-1 accuracy.</p>"},{"location":"notes/lecture4/#summary-of-advanced-topics","title":"Summary of Advanced Topics","text":"<ul> <li>ELMo: Contextual embeddings using BiLSTM.</li> <li>BERT: Transformer-based contextual embeddings with MLM and NSP.</li> <li>Graph embeddings: Learn node/edge/subgraph representations preserving structure.</li> <li>DeepWalk: Uses random walks + Skip-gram.</li> <li>Node2Vec: Extends DeepWalk with biased random walks for better control of context.</li> <li>Contrastive Learning &amp; SimCLR: Powerful SSL methods for images, learning embeddings without labels.</li> </ul> <p>These advances form the foundation of modern NLP, graph, and image representation learning.</p>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/","title":"Reinforcement Learning \u2014 Advanced, No\u2011Detail\u2011Left\u2011Behind Notes","text":"<p>These notes expand your earlier summary with deep, implementation\u2011ready detail that covers tabular Q\u2011learning, policy gradients / REINFORCE, and Deep Q\u2011Learning (DQN) with the practical tricks used in Atari\u2011style setups.</p>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#1-qlearning-update-from-bellman-optimality-to-stochastic-updates","title":"1) Q\u2011Learning Update \u2014 From Bellman Optimality to Stochastic Updates","text":""},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#bellman-optimality-actionvalue-form","title":"Bellman Optimality (action\u2011value form)","text":"<p>For any MDP with optimal \\(Q^*\\), the fixed point satisfies $$ Q^(s,a) = \\mathbb E\\left[ r_{t+1} + \\gamma \\max_{a'} Q^(s_{t+1},a') \\mid s_t=s, a_t=a \\right]. $$ The operator \\(\\mathcal T^*\\) defined by \\((\\mathcal T^*Q)(s,a) = \\mathbb E\\left[ r + \\gamma \\max_{a'} Q(s',a')\\right]\\) is a \\(\\gamma\\)\u2011contraction in the sup\u2011norm; therefore it has a unique fixed point \\(Q^*\\).</p>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#samplebased-target-and-td-error","title":"Sample\u2011based target and TD error","text":"<p>Given a single transition \\((s_t,a_t,r_{t+1},s_{t+1})\\), form a sample target $$ \\underbrace{y_t}{\\text{target}} \\;=\\; r,a') $$ and the } + \\gamma \\max_{a'} Q(s_{t+1TD error $$ \\delta_t \\;=\\; y_t - Q(s_t,a_t). $$</p>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#decayed-exponentialmovingaverage-update","title":"Decayed (exponential\u2011moving\u2011average) update","text":"<p>Blend the old estimate with the new target using a learning rate \\(\\alpha_t\\): $$ Q(s_t,a_t) \\leftarrow Q(s_t,a_t) + \\alpha_t\\,\\delta_t \\;=\\; (1-\\alpha_t)\\,Q(s_t,a_t) + \\alpha_t\\,\\big(r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1},a')\\big). $$ This is exactly the final line on your slide rewritten as an incremental update.</p>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#convergence-tabular-case","title":"Convergence (tabular case)","text":"<p>Under standard conditions (finite MDP, GLIE exploration so every \\((s,a)\\) is visited infinitely often; step sizes satisfy \\(\\sum_t \\alpha_t=\\infty\\), \\(\\sum_t \\alpha_t^2&lt;\\infty\\)), tabular Q\u2011learning converges to \\(Q^*\\) with probability 1. In practice, a constant \\(\\alpha\\) is common; then you trade strict convergence guarantees for faster tracking.</p>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#onpolicy-vs-offpolicy-and-sarsa","title":"On\u2011policy vs Off\u2011policy and SARSA","text":"<ul> <li>Q\u2011learning (off\u2011policy): learns \\(Q^*\\) using the max over \\(a'\\) irrespective of the behavior policy (often \\(\\epsilon\\)\u2011greedy).</li> <li>SARSA (on\u2011policy): target uses the actual next action \\(a_{t+1}\\) sampled by the behavior policy:   \\(y_t = r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1})\\) \u2014 safer under stochastic dynamics or when function approximation is unstable.</li> </ul>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#recovering-the-greedy-policy","title":"Recovering the greedy policy","text":"<p>Given a learned \\(Q\\), the greedy policy is $$ \\pi^(s) \\;=\\; \\arg\\max_{a\\in\\mathcal A} Q(s,a). $$ During learning, use \\(\\epsilon\\)\u2011greedy* (or softmax) to ensure exploration: pick a random action w.p. \\(\\epsilon\\), otherwise the argmax.</p>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#practical-hyperparameters-tabular","title":"Practical hyperparameters (tabular)","text":"<ul> <li>\\(\\epsilon\\) schedule: start high (0.9\u20131.0), linearly decay to 0.05\u20130.1; or cosine/exp schedules.</li> <li>Learning rate: \\(\\alpha\\in[0.05,0.5]\\) depending on reward scale.</li> <li>Discount: \\(\\gamma\\in[0.95,0.999]\\) for continuing tasks; smaller if horizons are short.</li> </ul>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#common-failure-modes","title":"Common failure modes","text":"<ul> <li>Insufficient exploration: Q values overfit early experience.</li> <li>Non\u2011stationarity: changing dynamics reward requires non\u2011decaying or adaptive \\(\\alpha\\).</li> <li>Deadly triad (with function approximation + bootstrapping + off\u2011policy): divergence. Use target networks, double estimators, or on\u2011policy control.</li> </ul>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#2-policybased-rl-reinforce-the-policy-gradient-theorem","title":"2) Policy\u2011based RL (REINFORCE &amp; the Policy Gradient Theorem)","text":""},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#episodic-objective","title":"Episodic objective","text":"<p>Let a trajectory be \\(\\tau = (s_0,a_0,r_1,\\dots,s_T)\\) generated by a differentiable policy \\(\\pi_\\theta(a\\mid s)\\). Define the expected return of an episode $$ J(\\pi_\\theta) = \\mathbb E_{\\tau\\sim p_\\theta(\\tau)}[R(\\tau)],\\quad R(\\tau)=\\sum_{t=0}^{T-1} r_{t+1}. $$ The trajectory density factorizes as $$ p_\\theta(\\tau) = p(s_0)\\prod_{t=0}^{T-1} \\pi_\\theta(a_t\\mid s_t)\\,p(s_{t+1}\\mid s_t,a_t). $$</p>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#policy-gradient-theorem-logderivative-trick","title":"Policy Gradient Theorem (log\u2011derivative trick)","text":"<p>Bring the gradient under the integral and use \\(\\nabla_\\theta \\log \\pi_\\theta\\): $$ \\nabla_\\theta J(\\pi_\\theta) = \\mathbb E_{\\tau}\\Bigg[\\sum_{t=0}^{T-1} \\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\, G_t\\Bigg], $$ where \\(G_t=\\sum_{k=t}^{T-1} r_{k+1}\\) is the return\u2011to\u2011go. This result holds because \\(\\nabla_\\theta \\log p(s_{t+1}\\mid s_t,a_t)=0\\) (dynamics do not depend on \\(\\theta\\) in model\u2011free settings).</p>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#reinforce-algorithm-monte-carlo-policy-gradient","title":"REINFORCE algorithm (Monte Carlo policy gradient)","text":"<p>Vanilla form (without baseline): 1. Roll out episodes with current \\(\\theta\\). 2. For each time step, compute \\(G_t\\). 3. Ascend the gradient: \\(\\theta \\leftarrow \\theta + \\alpha\\,\\sum_t \\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\,G_t\\).</p> <p>With a baseline (variance reduction): replace \\(G_t\\) by advantage \\(A_t = G_t - b(s_t)\\); unbiased if \\(b\\) does not depend on \\(a\\). Common \\(b\\): state\u2011value \\(V_\\phi(s_t)\\) learned by regression. Update becomes $$ \\theta \\leftarrow \\theta + \\alpha\\,\\sum_t \\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\,\\underbrace{(G_t - V_\\phi(s_t))}_{A_t}. $$</p>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#practical-improvements","title":"Practical improvements","text":"<ul> <li>Reward\u2011to\u2011go: use \\(G_t\\) not full \\(R(\\tau)\\) to reduce variance.</li> <li>Normalize advantages within a batch (zero mean / unit variance).</li> <li>Entropy regularization: add \\(+\\beta\\,\\mathbb E[\\mathcal H(\\pi_\\theta(\\cdot\\mid s))]\\) to encourage exploration; equivalent to a soft\u2011maximization of reward.</li> <li>Generalized Advantage Estimation (GAE): exponentially\u2011weighted TD(\\(\\lambda\\)) advantages: \\(A_t^{\\text{GAE}(\\lambda)}\\) balances bias/variance.</li> <li>Trust regions / clipping: TRPO/PPO stabilize large updates; PPO\u2019s clipped surrogate is widely used in practice.</li> </ul>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#strengths-vs-weaknesses-policy-gradients","title":"Strengths vs weaknesses (policy gradients)","text":"<ul> <li>Pros: works in continuous action spaces, directly optimizes stochastic policies, easy to combine with differentiable nets.</li> <li>Cons: high variance, sample\u2011inefficient, typically on\u2011policy; mitigated by baselines, variance reduction, and richer critics (actor\u2011critic).</li> </ul>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#3-qlearning-vs-reinforce-biasvariance-onoff-policy","title":"3) Q\u2011Learning vs REINFORCE \u2014 Bias/Variance, On/Off Policy","text":"Aspect Q\u2011Learning REINFORCE Learning type Value\u2011based, bootstrapped Policy\u2011based, Monte\u2011Carlo Policy Greedy/\\(\\epsilon\\)-greedy derived from \\(Q\\) Directly parameterized \\(\\pi_\\theta\\) On/Off policy Off\u2011policy (can learn from any behavior) On\u2011policy (data must match current policy) Bias/Variance High bias (bootstrapping) but low variance Unbiased gradient; high variance Action spaces Discrete (tabular/approx.) Naturally handles continuous actions Sample efficiency Good with replay Poor without replay; PPO/TRPO/A2C improve Stability Sensitive with function approx. (deadly triad) Sensitive to step size; stabilized by baselines/critics"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#4-deep-qlearning-dqn-why-deep-learning","title":"4) Deep Q\u2011Learning (DQN) \u2014 Why Deep Learning?","text":""},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#curse-of-dimensionality-from-pixels","title":"Curse of dimensionality from pixels","text":"<p>If a state is a stack of the last 4 grayscale frames at 84\u00d784 resolution (256 intensities), the raw state space size is roughly \\(256^{84\\times84\\times4} \\approx 10^{67970}\\) \u2014 tabular methods are impossible. We approximate \\(Q(s,a)\\) with a convolutional neural network.</p>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#qnetwork-parameterization","title":"Q\u2011Network parameterization","text":"<ul> <li>Single\u2011head form: one forward pass outputs a vector \\(Q_\\mathbf w(s,\\cdot)\\in\\mathbb R^{|\\mathcal A|}\\); the entry for index \\(a\\) is \\(Q(s,a;\\mathbf w)\\).</li> <li>Train by minimizing a temporal\u2011difference regression loss over mini\u2011batches of replayed transitions.</li> </ul>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#experience-replay-break-temporal-correlations","title":"Experience Replay (break temporal correlations)","text":"<p>Maintain a buffer \\(\\mathcal D\\) of tuples \\((s,a,r,s',\\text{done})\\). Periodically sample i.i.d. minibatches to compute the TD loss $$ \\mathcal L(\\mathbf w) = \\mathbb E_{(s,a,r,s')\\sim\\mathcal D} \\Big[\\;\\underbrace{\\big(r + \\gamma\\,\\mathbf 1_{\\neg\\text{done}}\\,\\max_{a'} Q_{\\mathbf w^-}(s',a')\\; -\\; Q_{\\mathbf w}(s,a)\\big)^2}_{\\text{TD error squared (or Huber)}}\\;\\Big]. $$ Here \\(\\mathbf w^-\\) are target network parameters, copied from \\(\\mathbf w\\) every \\(C\\) steps (or updated by Polyak averaging). Targets computed with \\(\\mathbf w^-\\) stabilize training.</p>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#dqn-algorithm-pseudocode","title":"DQN Algorithm (pseudocode)","text":"<pre><code>Initialize Q-network w with random weights\nInitialize target network w^- \u2190 w\nInitialize replay buffer D\nfor episode = 1..M:\n  s \u2190 env.reset();  \u03b5 \u2190 schedule(t)\n  for t = 1..T:\n    with prob \u03b5 choose a random action; else a \u2190 argmax_a Q_w(s,a)\n    s', r, done \u2190 env.step(a)\n    D.add(s,a,r,s',done)\n    if |D| \u2265 batch_size:\n      B \u2190 sample_minibatch(D)\n      y \u2190 r + \u03b3\u00b71_{\u00acdone}\u00b7max_{a'} Q_{w^-}(s',a')\n      L \u2190 Huber(y - Q_w(s,a))\n      w \u2190 w - \u03b7\u00b7\u2207_w L\n    every C steps: w^- \u2190 w (or soft update)\n    s \u2190 s'\n    if done: break\n</code></pre>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#ataristyle-preprocessing-typical-choices","title":"Atari\u2011style Preprocessing (typical choices)","text":"<ul> <li>Frame preprocessing: convert to grayscale; downsample to 84\u00d784; max\u2011pool over 2 frames to remove flicker.</li> <li>Stack the last 4 frames to encode velocity.</li> <li>Action repeat / frame skip (e.g., 4) to reduce computation.</li> <li>Reward clipping to \\([-1, 1]\\) for scale\u2011invariance; gradient clipping in optimizer.</li> <li>Exploration schedule: \\(\\epsilon\\) linearly decays from 1.0 to 0.1 over 1e6 steps (or 0.01 for evaluation).</li> <li>Optimizer: RMSProp or Adam with learning rate around \\(10^{-4}\\)\u2013\\(10^{-5}\\); minibatch sizes 32\u201364.</li> <li>Target update period \\(C\\): 10^3\u201310^4 env steps; replay buffer size 1e5\u20131e6.</li> </ul>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#key-stability-tricks","title":"Key stability tricks","text":"<ul> <li>Target networks (fixed \\(\\mathbf w^-\\) for targets).</li> <li>Huber loss instead of MSE (less sensitive to outliers):   \\(\\text{Huber}_\\kappa(x)=\\begin{cases} \\tfrac12x^2 &amp; |x|\\le\\kappa \\\\ \\kappa(|x|-\\tfrac12\\kappa) &amp; |x|&gt;\\kappa.\\end{cases}\\)</li> <li>Gradient clipping (e.g., global norm to 10).</li> <li>Replay buffer warmup before learning starts.</li> </ul>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#reducing-overestimation-double-dqn","title":"Reducing overestimation: Double DQN","text":"<p>Use the online network to select the action and the target network to evaluate it: $$  y = r + \\gamma\\,Q_{\\mathbf w^-}!\\Big(s',\\arg\\max_{a'} Q_{\\mathbf w}(s',a')\\Big). $$ This curbs the positive bias from \\(\\max\\) over noisy estimates.</p>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#prioritized-replay","title":"Prioritized Replay","text":"<p>Sample transitions with probability \\(p_i \\propto (|\\delta_i| + \\epsilon)^\\alpha\\); correct the bias with importance sampling weights \\(w_i = (N\\,p_i)^{-\\beta}\\) (normalized) inside the loss. Typical \\(\\alpha\\in[0.5,0.7]\\), \\(\\beta\\) annealed to 1.0.</p>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#dueling-networks","title":"Dueling Networks","text":"<p>Decompose Q into state value and advantage: \\(Q(s,a)=V(s)+A(s,a)-\\tfrac1{|\\mathcal A|}\\sum_{a'}A(s,a')\\). Helps when many actions share similar value.</p>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#multistep-returns-rainbow","title":"Multi\u2011step returns &amp; Rainbow","text":"<p>Use \\(n\\)\u2011step targets: \\(y = \\sum_{i=0}^{n-1} \\gamma^i r_{t+i+1} + \\gamma^n \\max_{a'} Q_{\\mathbf w^-}(s_{t+n},a')\\). Combine Double DQN, Prioritized Replay, Dueling, Noisy Nets, Distributional RL (Categorical/Quantile) \u2192 Rainbow DQN gains large performance boosts.</p>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#5-dqn-in-atari-endtoend-from-pixels","title":"5) DQN in Atari \u2014 End\u2011to\u2011End from Pixels","text":"<ul> <li>Input: stack of 4 processed frames \\(\\in\\mathbb R^{84\\times84\\times4}\\).</li> <li>Conv net: e.g., conv(32,8\u00d78,stride4) \u2192 conv(64,4\u00d74,stride2) \u2192 conv(64,3\u00d73,stride1) \u2192 FC(512) \u2192 FC(\\(|\\mathcal A|\\)).</li> <li>Output: one scalar per discrete joystick/button combination (\u224818 in ALE).</li> <li>Reward: per\u2011step score change; often clipped.</li> <li>Evaluation: average over multiple seeds and sticky actions for robustness.</li> </ul> <p>Implementation notes: - Maintain separate training and evaluation \\(\\epsilon\\). - Reset lives handling consistently (some works treat loss of life as terminal for training only). - Monitor: average return, TD error histogram, Q\u2011values magnitude, buffer age, action visitation.</p>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#6-deep-rl-breakthroughs-context-intuition","title":"6) Deep RL Breakthroughs (context &amp; intuition)","text":"<ul> <li>Atari (2013/2015): A single DQN architecture reached human\u2011level performance on many Atari 2600 games using only pixels and score as reward. Key ideas: replay, target network, convnets.</li> <li>AlphaZero (2017): Self\u2011play with Monte Carlo Tree Search guided by a shared policy\u2013value network; no human data, exceeded prior programs in chess, shogi, Go.</li> <li>AlphaStar (2019): Multi\u2011agent RL + imitation learning + league training reached Grandmaster level in StarCraft II (partial observability, macro/micro decisions, long horizons).</li> </ul> <p>These illustrate how value learning, policy learning, and planning can be combined at scale.</p>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#7-putting-it-all-together-practical-recipes","title":"7) Putting It All Together \u2014 Practical Recipes","text":""},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#choosing-an-algorithm","title":"Choosing an algorithm","text":"<ul> <li>Small discrete spaces: tabular Q\u2011learning/SARSA.</li> <li>High\u2011dimensional discrete (pixels): DQN (Double + Dueling + PER + n\u2011step = Rainbow).</li> <li>Continuous actions: Policy gradients with actor\u2011critic: PPO/SAC/TD3 (not covered in slides but essential in practice).</li> </ul>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#hyperparameter-starter-kit-dqn","title":"Hyperparameter starter kit (DQN)","text":"<ul> <li>Buffer 1e6; batch 32/64; learning rate \\(1\\times10^{-4}\\); target update 1e4 steps; \\(\\gamma=0.99\\); warmup 5e4 steps; total env steps 2e7+ for difficult games.</li> </ul>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#debugging-checklist","title":"Debugging checklist","text":"<ul> <li>Verify reward scaling/clipping and done flags.</li> <li>Ensure no bootstrapping at terminal (mask with \\((1-\\text{done})\\)).</li> <li>Plot \\(\\epsilon\\) vs steps, replay age distribution, Q\u2011value ranges, loss curves.</li> <li>Sanity check: on a tiny MDP, overfit with a tiny network to confirm learning signal.</li> </ul>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#8-mathematics-why-the-updates-work","title":"8) Mathematics: Why the Updates Work","text":""},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#contraction-and-fixed-point","title":"Contraction and fixed point","text":"<p>The Bellman optimality operator \\(\\mathcal T^*\\) is a \\(\\gamma\\)\u2011contraction: for any \\(Q_1,Q_2\\), \\(\\|\\mathcal T^*Q_1-\\mathcal T^*Q_2\\|_\\infty \\le \\gamma\\,\\|Q_1-Q_2\\|_\\infty\\). Banach\u2019s fixed\u2011point theorem \u21d2 unique \\(Q^*\\).</p>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#stochastic-approximation-view","title":"Stochastic Approximation view","text":"<p>Q\u2011learning is Robbins\u2013Monro SA tracking the root of \\(\\mathbb E[\\delta_t\\mid s_t,a_t]=0\\). Conditions on \\(\\alpha_t\\) and visitation ensure almost\u2011sure convergence in tabular settings.</p>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#policy-gradient-derivation-sketch","title":"Policy gradient derivation sketch","text":"<p>Use \\(\\nabla_\\theta J = \\int R(\\tau)\\,\\nabla_\\theta p_\\theta(\\tau)\\,d\\tau = \\int R(\\tau)\\,p_\\theta(\\tau)\\,\\nabla_\\theta\\log p_\\theta(\\tau)\\,d\\tau\\). Since \\(\\log p_\\theta(\\tau)\\) sums only \\(\\log\\pi_\\theta\\) terms, the gradient becomes an expectation of \\(\\sum_t \\nabla\\log\\pi_\\theta(a_t\\mid s_t)\\,G_t\\).</p>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#9-algorithm-boxes-copypaste-friendly","title":"9) Algorithm Boxes (copy\u2011paste friendly)","text":""},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#tabular-qlearning-episodic","title":"Tabular Q\u2011learning (episodic)","text":"<pre><code>Q = zeros(|S|, |A|)\nfor episode in range(M):\n    s = env.reset()\n    for t in range(T):\n        a = \u03b5_greedy(Q[s])\n        s2, r, done, _ = env.step(a)\n        td_target = r + \u03b3 * (0 if done else max(Q[s2]))\n        td_error  = td_target - Q[s,a]\n        Q[s,a]   += \u03b1 * td_error\n        s = s2\n        if done: break\n</code></pre>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#reinforce-with-baseline","title":"REINFORCE (with baseline)","text":"<pre><code>for update in range(U):\n    traj = collect_episodes(\u03c0_\u03b8)\n    Gt = returns_to_go(traj.rewards, \u03b3)\n    b  = V_\u03c6(traj.states).detach()\n    adv = (Gt - b).normalize()\n    loss_actor = -(log\u03c0_\u03b8(traj.actions|traj.states) * adv).mean()\n    loss_critic = ((V_\u03c6(traj.states) - Gt)**2).mean()\n    optimize(loss_actor + c_v*loss_critic - c_ent*entropy(\u03c0_\u03b8))\n</code></pre>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#dqn-double-huber-target-net","title":"DQN (Double + Huber + target net)","text":"<pre><code>batch = sample(D)\ns,a,r,s2,d = batch\nwith torch.no_grad():\n    a2 = Q_w(s2).argmax(dim=1)\n    y  = r + \u03b3*(1-d)*Q_wminus(s2).gather(1, a2.unsqueeze(1)).squeeze(1)\nqsa = Q_w(s).gather(1, a.unsqueeze(1)).squeeze(1)\nloss = huber(y - qsa)\nopt.zero_grad(); loss.backward(); clip_grad_norm_(Q_w.parameters(), 10); opt.step()\n</code></pre>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#10-glossary-of-symbols","title":"10) Glossary of Symbols","text":"<ul> <li>\\(s_t,a_t,r_{t+1},s_{t+1}\\) \u2014 state, action, reward, next state at time \\(t\\).</li> <li>\\(\\gamma\\) \u2014 discount factor.</li> <li>\\(Q(s,a)\\) \u2014 action\u2011value; \\(V(s)\\) \u2014 state\u2011value.</li> <li>\\(\\pi_\\theta(a\\mid s)\\) \u2014 (stochastic) policy parameterized by \\(\\theta\\).</li> <li>\\(G_t\\) \u2014 return\u2011to\u2011go; \\(A_t\\) \u2014 advantage (e.g., \\(G_t - V(s_t)\\)).</li> <li>\\(\\mathbf w,\\mathbf w^-\\) \u2014 online and target Q\u2011network parameters.</li> <li>PER hyperparams: \\(\\alpha\\) (priority exponent), \\(\\beta\\) (IS correction exponent).</li> </ul>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#11-extended-topics-quick-references","title":"11) Extended Topics (quick references)","text":"<ul> <li>Expected\u2011SARSA: target uses expectation over behavior policy at \\(s'\\); less variance than SARSA.</li> <li>Distributional RL: learn full return distribution \\(Z(s,a)\\) (categorical/quantile). Targets use distributional Bellman operator.</li> <li>Noisy Nets: parameterized noise in linear layers for exploration.</li> <li>n\u2011step Q\u2011learning: blend Monte\u2011Carlo and bootstrapping; better credit assignment.</li> <li>Actor\u2011Critic family (beyond slides): A2C/A3C (synchronous/asynchronous), PPO (clipped surrogate), SAC/TD3/DDPG (continuous control, entropy regularization or twin critics).</li> </ul>"},{"location":"notes/rl_mega_notes_q_learning_policy_gradients_and_deep_q_networks/#closing","title":"Closing","text":"<p>You now have slide\u2011level theory and practitioner\u2011level knobs for Q\u2011learning, policy gradients, and DQN\u2014from the Bellman equations and policy gradient theorem all the way to Double/Dueling/PR replay and Atari\u2011grade pipelines. If you want, we can add worked examples (gridworld, bandit, CartPole, or Pong) with step\u2011by\u2011step calculations and reference hyperparameters.</p>"},{"location":"notes/supervised/","title":"Supervised Learning","text":""}]}