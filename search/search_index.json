{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome \ud83d\udc4b","text":"<p>This is my ML notes site.</p>"},{"location":"cheatsheets/","title":"Cheatsheets","text":""},{"location":"notes/lecture3/","title":"Dimensionality Reduction","text":"<p>Dimensionality reduction is the process of reducing the number of features (dimensions) in a dataset while preserving the most informative and discriminative aspects.</p> <p>It is one of the most critical techniques in machine learning and data science, because:</p> <ul> <li>Models often perform poorly when overwhelmed with too many features (curse of dimensionality).</li> <li>Redundant and irrelevant features introduce noise.</li> <li>High-dimensional spaces are sparse, which makes clustering and classification difficult.</li> <li>Reduced dimensionality leads to better generalization, interpretability, visualization, and faster computation.</li> </ul>"},{"location":"notes/lecture3/#1-the-usual-supervised-learning-approach","title":"1. The Usual Supervised Learning Approach","text":"<pre><code>flowchart LR\n    subgraph Training\n        A[Data: Features] --&gt; M[Learning Algorithm]\n        L[Labels] --&gt; M\n        M --&gt; B[Model]\n    end\n\n    subgraph Testing\n        T[Test Data] --&gt; B\n        B --&gt; P[Predicted Label]\n    end</code></pre> <ul> <li>A dataset consists of features (X) and labels (Y).</li> <li>A supervised learning algorithm learns a mapping X \u2192 Y.</li> <li>With high-dimensional data:</li> <li>The algorithm becomes overwhelmed.</li> <li>Training time is long.</li> <li>The risk of overfitting increases due to sparse data.</li> </ul>"},{"location":"notes/lecture3/#2-feature-types","title":"2. Feature Types","text":"<p>Not all features contribute equally to prediction.</p> <ul> <li> <p>Relevant Features</p> </li> <li> <p>Provide useful predictive information.</p> </li> <li> <p>Example: Diaper, Stroller, Bassinet, Cradle for predicting baby products.</p> </li> <li> <p>Irrelevant Features</p> </li> <li> <p>Provide no predictive signal.</p> </li> <li> <p>Example: Random identifiers.</p> </li> <li> <p>Redundant Features</p> </li> <li> <p>Highly correlated with other features, duplicating information.</p> </li> <li>Example: Stroller and wheels often co-occur.</li> </ul> <pre><code>mindmap\n  root((Features))\n    Relevant\n      \"Strong signal for prediction\"\n    Irrelevant\n      \"No contribution\"\n    Redundant\n      \"Overlaps with others\"</code></pre>"},{"location":"notes/lecture3/#3-approaches-to-dimensionality-reduction","title":"3. Approaches to Dimensionality Reduction","text":"<p>Two broad strategies:</p>"},{"location":"notes/lecture3/#31-feature-selection-downsizing-existing-features","title":"3.1 Feature Selection (downsizing existing features)","text":"<ul> <li>Removes noisy, irrelevant, or redundant features.</li> <li>Keeps original features intact.</li> <li>Preserves interpretability.</li> <li>Useful when:</li> <li>Budget constraints exist (e.g., costly medical tests).</li> <li>Small number of features is required for human interpretability.</li> </ul>"},{"location":"notes/lecture3/#32-low-dimensional-feature-learning-new-derived-features","title":"3.2 Low-Dimensional Feature Learning (new derived features)","text":"<ul> <li>Learns a new representation of the data.</li> <li>May lose interpretability but increases performance.</li> <li>Examples:</li> <li>Linear methods: PCA, MDS</li> <li>Non-linear methods: Kernel PCA, t-SNE</li> <li>Representation learning: Neural embeddings</li> </ul> <pre><code>graph TD\n    X[High-Dimensional Data] --&gt;|Feature Selection| S[Reduced Subset of Features]\n    X --&gt;|Feature Learning| F[New Derived Features]</code></pre>"},{"location":"notes/lecture3/#4-feature-selection-techniques","title":"4. Feature Selection Techniques","text":"Method Idea Pros Cons Examples Wrapper Use ML models to search best subset Captures feature interactions Computationally expensive Sequential Forward Selection, Backward Elimination Filter Rank features by statistical score Fast, scalable Ignores interactions Pearson Correlation, Chi-squared, Mutual Information Embedded Feature selection during training Efficient, task-specific Model-dependent Lasso Regression, Decision Trees"},{"location":"notes/lecture3/#5-wrapper-methods","title":"5. Wrapper Methods","text":""},{"location":"notes/lecture3/#51-exhaustive-search","title":"5.1 Exhaustive Search","text":"<ul> <li>\\(N\\) features \u2192 \\(2^N\\) possible subsets.</li> <li>Quickly infeasible:</li> <li>20 features \u2192 \\~1 million subsets</li> <li>25 features \u2192 \\~33.5 million subsets</li> <li>30 features \u2192 \\~1.1 billion subsets</li> </ul>"},{"location":"notes/lecture3/#52-sequential-forward-selection-sfs","title":"5.2 Sequential Forward Selection (SFS)","text":"<ol> <li>Start with empty set \\(S = \\emptyset\\).</li> <li>While stopping criteria not met:</li> <li>For each feature \\(X_f \\notin S\\):<ul> <li>\\(S' = S \\cup \\{X_f\\}\\)</li> <li>Train model on \\(S'\\).</li> <li>Evaluate accuracy.</li> </ul> </li> <li>Select feature with highest improvement.</li> <li>Return final \\(S\\).</li> </ol> <pre><code>flowchart TD\n    Start[Start with empty set S = empty] --&gt; Loop{Stopping Criteria Met?}\n    Loop -- No --&gt; Add[Add best feature]\n    Add --&gt; Train[Train + Evaluate]\n    Train --&gt; Loop\n    Loop -- Yes --&gt; End[Return Subset S]</code></pre> <ul> <li>Advantage: Captures strong features.</li> <li>Disadvantage: Cannot remove redundant features once added.</li> </ul>"},{"location":"notes/lecture3/#53-sequential-backward-elimination-sbe","title":"5.3 Sequential Backward Elimination (SBE)","text":"<ul> <li>Start with all features.</li> <li>Iteratively remove least useful feature.</li> <li>Stop when removal hurts performance.</li> </ul>"},{"location":"notes/lecture3/#54-heuristic-search","title":"5.4 Heuristic Search","text":"<ul> <li>Use optimization strategies like Genetic Algorithms, Simulated Annealing, Greedy search.</li> </ul>"},{"location":"notes/lecture3/#6-filter-methods","title":"6. Filter Methods","text":"<p>Principle: Replace costly model evaluation with fast statistics \\(J(X_f)\\).</p> <p>Examples:</p> <ul> <li>Mutual Information (MI): Captures dependence.</li> <li>Pearson Correlation: Measures linear relationships.</li> <li>Chi-squared test: For categorical features.</li> </ul>"},{"location":"notes/lecture3/#example-ranking-table","title":"Example Ranking Table","text":"Feature Index Score (\\(J(X_f)\\)) 32 0.98 5501 0.94 101 0.91 345 0.85 1104 0.81"},{"location":"notes/lecture3/#7-pearsons-correlation-coefficient","title":"7. Pearson\u2019s Correlation Coefficient","text":"<p>Captures linear relationships between feature \\(A\\) and target \\(Y\\):</p> \\[ \\rho(A,Y) = \\frac{\\text{cov}(A,Y)}{\\sigma_A \\cdot \\sigma_Y} \\] <p>Expanded:</p> \\[ \\rho(A,Y) = \\frac{\\sum_i (A_i - \\bar{A})(Y_i - \\bar{Y})}{\\sqrt{\\sum_i (A_i - \\bar{A})^2} \\cdot \\sqrt{\\sum_i (Y_i - \\bar{Y})^2}} \\] <ul> <li>\\(A_i, Y_i\\): sample values.</li> <li>\\(\\bar{A}, \\bar{Y}\\): means.</li> <li>Covariance matrix captures pairwise correlations.</li> </ul>"},{"location":"notes/lecture3/#8-embedded-methods","title":"8. Embedded Methods","text":"<ul> <li>Selection happens inside the learning algorithm.</li> <li>Examples:</li> <li>Lasso regression (L1): zeroes out unimportant features.</li> <li>Tree-based methods: select splits only on important features.</li> </ul> <pre><code>flowchart LR\n    Data --&gt; Model[Model with Embedded Selection]\n    Model --&gt; Output[Reduced Feature Set]</code></pre>"},{"location":"notes/lecture3/#extended-notes-on-dimensionality-reduction","title":"Extended Notes on Dimensionality Reduction","text":""},{"location":"notes/lecture3/#9-mutual-information-mi","title":"9. Mutual Information (MI)","text":"<p>Measures reduction in uncertainty:</p> \\[ I(A,Y) = \\sum_{x \\in A}\\sum_{y \\in Y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)} \\] <ul> <li>If \\(A\\) and \\(Y\\) are independent \u2192 \\(I(A,Y)=0\\).</li> </ul>"},{"location":"notes/lecture3/#10-feature-interactions","title":"10. Feature Interactions","text":"<p>Individual features may appear weak, but combinations can be strong.</p> <ul> <li>Example: Two independent features become predictive when combined.</li> <li>Highlights the limitation of filter methods which evaluate features individually.</li> </ul>"},{"location":"notes/lecture3/#11-pros-cons-of-filter-methods","title":"11. Pros &amp; Cons of Filter Methods","text":"<p>Advantages:</p> <ul> <li>Simple, scalable.</li> <li>Easily parallelizable.</li> </ul> <p>Disadvantages:</p> <ul> <li>Cannot capture feature interactions.</li> <li>May select redundant features.</li> </ul>"},{"location":"notes/lecture3/#12-embedded-methods-lasso","title":"12. Embedded Methods: LASSO","text":"<p>Optimization problem:</p> \\[ \\min_\\beta \\sum_{i=1}^M (y_i - \\sum_j x_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^N |\\beta_j| \\] <ul> <li>L1 penalty induces sparsity.</li> <li>Contrast with Ridge (L2):</li> </ul> \\[ \\lambda \\sum_{j=1}^N \\beta_j^2 \\] <ul> <li>L2 shrinks weights but rarely eliminates them.</li> </ul>"},{"location":"notes/lecture3/#13-benefits-of-feature-selection","title":"13. Benefits of Feature Selection","text":"<ul> <li>Improves accuracy.</li> <li>Reduces computation.</li> <li>Improves interpretability.</li> <li>Prevents overfitting.</li> </ul>"},{"location":"notes/lecture3/#14-t-sne-motivation","title":"14. t-SNE Motivation","text":"<ul> <li>High-dimensional visualization is difficult.</li> <li>Project data into 2D or 3D for interpretability.</li> <li>t-SNE preserves local structure.</li> </ul>"},{"location":"notes/lecture3/#15-general-problem-statement","title":"15. General Problem Statement","text":"<p>Given high-dimensional \\(X = \\{x_1,...,x_M\\}\\), \\(x_i \\in \\mathbb{R}^N\\):</p> <p>Find \\(Y = \\{y_1,...,y_M\\}\\), \\(y_i \\in \\mathbb{R}^n, n &lt; N\\), minimizing information loss.</p>"},{"location":"notes/lecture3/#16-stochastic-neighbor-embedding-sne","title":"16. Stochastic Neighbor Embedding (SNE)","text":"<ul> <li>Preserves local distances.</li> <li>Converts distances to conditional probabilities:</li> </ul> \\[ p_{j|i} = \\frac{e^{-||x_i-x_j||^2/2\\sigma_i^2}}{\\sum_k e^{-||x_i-x_k||^2/2\\sigma_i^2}} \\] \\[ q_{j|i} = \\frac{e^{-||y_i-y_j||^2}}{\\sum_k e^{-||y_i-y_k||^2}} \\]"},{"location":"notes/lecture3/#17-from-sne-to-t-sne","title":"17. From SNE to t-SNE","text":"<ul> <li>High-dimensional similarities use Gaussian.</li> <li>Low-dimensional similarities use Student\u2019s t-distribution (heavier tails).</li> </ul> \\[ q_{ij} = \\frac{(1+||y_i-y_j||^2)^{-1}}{\\sum_{k \\neq l}(1+||y_k-y_l||^2)^{-1}} \\]"},{"location":"notes/lecture3/#18-kl-divergence-in-t-sne","title":"18. KL Divergence in t-SNE","text":"<p>Objective:</p> \\[ KL(P||Q) = \\sum_i \\sum_j p_{j|i}\\log \\frac{p_{j|i}}{q_{j|i}} \\] <ul> <li>Penalizes mismatches in similarity structure.</li> <li>Large \\(p_{j|i}\\) with small \\(q_{j|i}\\) \u2192 heavy penalty.</li> </ul>"},{"location":"notes/lecture3/#19-gradient-descent-optimization","title":"19. Gradient Descent Optimization","text":"<p>Gradient update rule:</p> \\[ \\frac{\\partial C}{\\partial y_i} = 2\\sum_j (y_j-y_i)(p_{j|i}-q_{j|i}+p_{i|j}-q_{i|j}) \\] <ul> <li>Emphasizes preserving close distances.</li> </ul>"},{"location":"notes/lecture3/#20-visualization-example-mnist","title":"20. Visualization Example (MNIST)","text":"<ul> <li>Applied to 6000 MNIST digits.</li> <li>t-SNE clusters digits cleanly compared to Sammon mapping.</li> </ul>"},{"location":"notes/lecture3/#21-summary-mindmap","title":"21. Summary Mindmap","text":"<pre><code>mindmap\n  root((Dimensionality Reduction))\n    Feature Selection\n      Wrapper\n        Forward Selection\n        Backward Elimination\n        Heuristic Search\n      Filter\n        Correlation\n        Chi-squared\n        Mutual Information\n      Embedded\n        Lasso (L1)\n        Decision Trees\n    Feature Learning\n      PCA\n      Kernel PCA\n      MDS\n      Autoencoders\n      t-SNE\n        SNE\n        KL Divergence\n        Gradient Descent</code></pre>"},{"location":"notes/lecture3/#key-takeaways","title":"\u2705 Key Takeaways","text":"<ul> <li>Dimensionality reduction combats the curse of dimensionality.</li> <li>Two approaches: Feature Selection (simpler, interpretable) vs. Feature Learning (powerful, less interpretable).</li> <li>Methods include Wrapper, Filter, Embedded selection, PCA, t-SNE, autoencoders.</li> <li>t-SNE is best for visualization, capturing local structures.</li> <li>Feature selection methods improve performance and interpretability for predictive modeling.</li> </ul>"},{"location":"notes/supervised/","title":"Supervised Learning","text":""}]}