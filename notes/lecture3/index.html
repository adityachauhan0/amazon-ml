
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="My ML notes, cheatsheets, and experiments">
      
      
      
        <link rel="canonical" href="https://adityachauhan0.github.io/amazon-ml/notes/lecture3/">
      
      
        <link rel="prev" href="../..">
      
      
        <link rel="next" href="../lecture4/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.17">
    
    
      
        <title>Dimensionality Reduction - Machine Learning Notes</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.7e37652d.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="pink">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#dimensionality-reduction" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Machine Learning Notes" class="md-header__button md-logo" aria-label="Machine Learning Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Machine Learning Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Dimensionality Reduction
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="pink"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="pink"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="pink"  aria-label="Switch to high contrast"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to high contrast" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 20c-2.21 0-4.21-.9-5.66-2.34L17.66 6.34A8.01 8.01 0 0 1 20 12a8 8 0 0 1-8 8M6 8h2V6h1.5v2h2v1.5h-2v2H8v-2H6M12 2A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10A10 10 0 0 0 12 2m0 14h5v-1.5h-5z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/adityachauhan0/amazon-ml" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    adityachauhan0/amazon-ml
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="./" class="md-tabs__link">
          
  
  
    
  
  Notes

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../cheatsheets/" class="md-tabs__link">
        
  
  
    
  
  Cheatsheets

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Machine Learning Notes" class="md-nav__button md-logo" aria-label="Machine Learning Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Machine Learning Notes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/adityachauhan0/amazon-ml" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    adityachauhan0/amazon-ml
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Notes
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Notes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Dimensionality Reduction
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Dimensionality Reduction
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-the-usual-supervised-learning-approach" class="md-nav__link">
    <span class="md-ellipsis">
      1. The Usual Supervised Learning Approach
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-feature-types" class="md-nav__link">
    <span class="md-ellipsis">
      2. Feature Types
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-approaches-to-dimensionality-reduction" class="md-nav__link">
    <span class="md-ellipsis">
      3. Approaches to Dimensionality Reduction
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Approaches to Dimensionality Reduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-feature-selection-downsizing-existing-features" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 Feature Selection (downsizing existing features)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-low-dimensional-feature-learning-new-derived-features" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 Low-Dimensional Feature Learning (new derived features)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-feature-selection-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      4. Feature Selection Techniques
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-wrapper-methods" class="md-nav__link">
    <span class="md-ellipsis">
      5. Wrapper Methods
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Wrapper Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-exhaustive-search" class="md-nav__link">
    <span class="md-ellipsis">
      5.1 Exhaustive Search
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-sequential-forward-selection-sfs" class="md-nav__link">
    <span class="md-ellipsis">
      5.2 Sequential Forward Selection (SFS)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#53-sequential-backward-elimination-sbe" class="md-nav__link">
    <span class="md-ellipsis">
      5.3 Sequential Backward Elimination (SBE)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#54-heuristic-search" class="md-nav__link">
    <span class="md-ellipsis">
      5.4 Heuristic Search
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-filter-methods" class="md-nav__link">
    <span class="md-ellipsis">
      6. Filter Methods
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. Filter Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#example-ranking-table" class="md-nav__link">
    <span class="md-ellipsis">
      Example Ranking Table
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-pearsons-correlation-coefficient" class="md-nav__link">
    <span class="md-ellipsis">
      7. Pearson’s Correlation Coefficient
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-embedded-methods" class="md-nav__link">
    <span class="md-ellipsis">
      8. Embedded Methods
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lecture4/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Unsupervised Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lec5/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sequential Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lec6/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lec8/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Causal Inference
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cheatsheets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Cheatsheets
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/adityachauhan0/amazon-ml/edit/main/docs/notes/lecture3.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/adityachauhan0/amazon-ml/raw/main/docs/notes/lecture3.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


<h1 id="dimensionality-reduction">Dimensionality Reduction<a class="headerlink" href="#dimensionality-reduction" title="Permanent link">&para;</a></h1>
<p>Dimensionality reduction is the process of <strong>reducing the number of features</strong> (dimensions) in a dataset while preserving the most <strong>informative</strong> and <strong>discriminative</strong> aspects.</p>
<p>It is one of the most critical techniques in machine learning and data science, because:</p>
<ul>
<li>Models often perform poorly when overwhelmed with too many features (curse of dimensionality).</li>
<li>Redundant and irrelevant features introduce noise.</li>
<li>High-dimensional spaces are sparse, which makes clustering and classification difficult.</li>
<li>Reduced dimensionality leads to better generalization, interpretability, visualization, and faster computation.</li>
</ul>
<hr />
<h2 id="1-the-usual-supervised-learning-approach">1. The Usual Supervised Learning Approach<a class="headerlink" href="#1-the-usual-supervised-learning-approach" title="Permanent link">&para;</a></h2>
<pre class="mermaid"><code>flowchart LR
    subgraph Training
        A[Data: Features] --&gt; M[Learning Algorithm]
        L[Labels] --&gt; M
        M --&gt; B[Model]
    end

    subgraph Testing
        T[Test Data] --&gt; B
        B --&gt; P[Predicted Label]
    end</code></pre>
<ul>
<li>A dataset consists of <strong>features (X)</strong> and <strong>labels (Y)</strong>.</li>
<li>A supervised learning algorithm learns a mapping <strong>X → Y</strong>.</li>
<li>With high-dimensional data:</li>
<li>The algorithm becomes <strong>overwhelmed</strong>.</li>
<li>Training time is long.</li>
<li>The risk of <strong>overfitting</strong> increases due to sparse data.</li>
</ul>
<hr />
<h2 id="2-feature-types">2. Feature Types<a class="headerlink" href="#2-feature-types" title="Permanent link">&para;</a></h2>
<p>Not all features contribute equally to prediction.</p>
<ul>
<li>
<p><strong>Relevant Features</strong></p>
</li>
<li>
<p>Provide useful predictive information.</p>
</li>
<li>
<p>Example: <em>Diaper, Stroller, Bassinet, Cradle</em> for predicting baby products.</p>
</li>
<li>
<p><strong>Irrelevant Features</strong></p>
</li>
<li>
<p>Provide no predictive signal.</p>
</li>
<li>
<p>Example: Random identifiers.</p>
</li>
<li>
<p><strong>Redundant Features</strong></p>
</li>
<li>
<p>Highly correlated with other features, duplicating information.</p>
</li>
<li>Example: <em>Stroller</em> and <em>wheels</em> often co-occur.</li>
</ul>
<pre class="mermaid"><code>mindmap
  root((Features))
    Relevant
      "Strong signal for prediction"
    Irrelevant
      "No contribution"
    Redundant
      "Overlaps with others"</code></pre>
<hr />
<h2 id="3-approaches-to-dimensionality-reduction">3. Approaches to Dimensionality Reduction<a class="headerlink" href="#3-approaches-to-dimensionality-reduction" title="Permanent link">&para;</a></h2>
<p>Two broad strategies:</p>
<h3 id="31-feature-selection-downsizing-existing-features">3.1 Feature Selection (downsizing existing features)<a class="headerlink" href="#31-feature-selection-downsizing-existing-features" title="Permanent link">&para;</a></h3>
<ul>
<li>Removes noisy, irrelevant, or redundant features.</li>
<li>Keeps original features intact.</li>
<li>Preserves <strong>interpretability</strong>.</li>
<li>Useful when:</li>
<li>Budget constraints exist (e.g., costly medical tests).</li>
<li>Small number of features is required for human interpretability.</li>
</ul>
<h3 id="32-low-dimensional-feature-learning-new-derived-features">3.2 Low-Dimensional Feature Learning (new derived features)<a class="headerlink" href="#32-low-dimensional-feature-learning-new-derived-features" title="Permanent link">&para;</a></h3>
<ul>
<li>Learns a <strong>new representation</strong> of the data.</li>
<li>May lose interpretability but increases performance.</li>
<li>Examples:</li>
<li><strong>Linear methods</strong>: PCA, MDS</li>
<li><strong>Non-linear methods</strong>: Kernel PCA, t-SNE</li>
<li><strong>Representation learning</strong>: Neural embeddings</li>
</ul>
<pre class="mermaid"><code>graph TD
    X[High-Dimensional Data] --&gt;|Feature Selection| S[Reduced Subset of Features]
    X --&gt;|Feature Learning| F[New Derived Features]</code></pre>
<hr />
<h2 id="4-feature-selection-techniques">4. Feature Selection Techniques<a class="headerlink" href="#4-feature-selection-techniques" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Idea</th>
<th>Pros</th>
<th>Cons</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td>Wrapper</td>
<td>Use ML models to search best subset</td>
<td>Captures feature interactions</td>
<td>Computationally expensive</td>
<td>Sequential Forward Selection, Backward Elimination</td>
</tr>
<tr>
<td>Filter</td>
<td>Rank features by statistical score</td>
<td>Fast, scalable</td>
<td>Ignores interactions</td>
<td>Pearson Correlation, Chi-squared, Mutual Information</td>
</tr>
<tr>
<td>Embedded</td>
<td>Feature selection during training</td>
<td>Efficient, task-specific</td>
<td>Model-dependent</td>
<td>Lasso Regression, Decision Trees</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="5-wrapper-methods">5. Wrapper Methods<a class="headerlink" href="#5-wrapper-methods" title="Permanent link">&para;</a></h2>
<h3 id="51-exhaustive-search">5.1 Exhaustive Search<a class="headerlink" href="#51-exhaustive-search" title="Permanent link">&para;</a></h3>
<ul>
<li><span class="arithmatex">\(N\)</span> features → <span class="arithmatex">\(2^N\)</span> possible subsets.</li>
<li>Quickly infeasible:</li>
<li>20 features → \~1 million subsets</li>
<li>25 features → \~33.5 million subsets</li>
<li>30 features → \~1.1 billion subsets</li>
</ul>
<h3 id="52-sequential-forward-selection-sfs">5.2 Sequential Forward Selection (SFS)<a class="headerlink" href="#52-sequential-forward-selection-sfs" title="Permanent link">&para;</a></h3>
<ol>
<li>Start with empty set <span class="arithmatex">\(S = \emptyset\)</span>.</li>
<li>While stopping criteria not met:</li>
<li>For each feature <span class="arithmatex">\(X_f \notin S\)</span>:<ul>
<li><span class="arithmatex">\(S' = S \cup \{X_f\}\)</span></li>
<li>Train model on <span class="arithmatex">\(S'\)</span>.</li>
<li>Evaluate accuracy.</li>
</ul>
</li>
<li>Select feature with highest improvement.</li>
<li>Return final <span class="arithmatex">\(S\)</span>.</li>
</ol>
<pre class="mermaid"><code>flowchart TD
    Start[Start with empty set S = empty] --&gt; Loop{Stopping Criteria Met?}
    Loop -- No --&gt; Add[Add best feature]
    Add --&gt; Train[Train + Evaluate]
    Train --&gt; Loop
    Loop -- Yes --&gt; End[Return Subset S]</code></pre>
<ul>
<li><strong>Advantage:</strong> Captures strong features.</li>
<li><strong>Disadvantage:</strong> Cannot remove redundant features once added.</li>
</ul>
<h3 id="53-sequential-backward-elimination-sbe">5.3 Sequential Backward Elimination (SBE)<a class="headerlink" href="#53-sequential-backward-elimination-sbe" title="Permanent link">&para;</a></h3>
<ul>
<li>Start with all features.</li>
<li>Iteratively remove least useful feature.</li>
<li>Stop when removal hurts performance.</li>
</ul>
<h3 id="54-heuristic-search">5.4 Heuristic Search<a class="headerlink" href="#54-heuristic-search" title="Permanent link">&para;</a></h3>
<ul>
<li>Use optimization strategies like Genetic Algorithms, Simulated Annealing, Greedy search.</li>
</ul>
<hr />
<h2 id="6-filter-methods">6. Filter Methods<a class="headerlink" href="#6-filter-methods" title="Permanent link">&para;</a></h2>
<p><strong>Principle:</strong> Replace costly model evaluation with fast statistics <span class="arithmatex">\(J(X_f)\)</span>.</p>
<p>Examples:</p>
<ul>
<li><strong>Mutual Information (MI)</strong>: Captures dependence.</li>
<li><strong>Pearson Correlation</strong>: Measures linear relationships.</li>
<li><strong>Chi-squared test</strong>: For categorical features.</li>
</ul>
<h3 id="example-ranking-table">Example Ranking Table<a class="headerlink" href="#example-ranking-table" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Feature Index</th>
<th>Score (<span class="arithmatex">\(J(X_f)\)</span>)</th>
</tr>
</thead>
<tbody>
<tr>
<td>32</td>
<td>0.98</td>
</tr>
<tr>
<td>5501</td>
<td>0.94</td>
</tr>
<tr>
<td>101</td>
<td>0.91</td>
</tr>
<tr>
<td>345</td>
<td>0.85</td>
</tr>
<tr>
<td>1104</td>
<td>0.81</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="7-pearsons-correlation-coefficient">7. Pearson’s Correlation Coefficient<a class="headerlink" href="#7-pearsons-correlation-coefficient" title="Permanent link">&para;</a></h2>
<p>Captures linear relationships between feature <span class="arithmatex">\(A\)</span> and target <span class="arithmatex">\(Y\)</span>:</p>
<div class="arithmatex">\[
\rho(A,Y) = \frac{\text{cov}(A,Y)}{\sigma_A \cdot \sigma_Y}
\]</div>
<p>Expanded:</p>
<div class="arithmatex">\[
\rho(A,Y) = \frac{\sum_i (A_i - \bar{A})(Y_i - \bar{Y})}{\sqrt{\sum_i (A_i - \bar{A})^2} \cdot \sqrt{\sum_i (Y_i - \bar{Y})^2}}
\]</div>
<ul>
<li><span class="arithmatex">\(A_i, Y_i\)</span>: sample values.</li>
<li><span class="arithmatex">\(\bar{A}, \bar{Y}\)</span>: means.</li>
<li>Covariance matrix captures pairwise correlations.</li>
</ul>
<hr />
<h2 id="8-embedded-methods">8. Embedded Methods<a class="headerlink" href="#8-embedded-methods" title="Permanent link">&para;</a></h2>
<ul>
<li>Selection happens <strong>inside the learning algorithm</strong>.</li>
<li>Examples:</li>
<li><strong>Lasso regression (L1):</strong> zeroes out unimportant features.</li>
<li><strong>Tree-based methods:</strong> select splits only on important features.</li>
</ul>
<pre class="mermaid"><code>flowchart LR
    Data --&gt; Model[Model with Embedded Selection]
    Model --&gt; Output[Reduced Feature Set]</code></pre>
<hr />
<h1 id="extended-notes-on-dimensionality-reduction">Extended Notes on Dimensionality Reduction<a class="headerlink" href="#extended-notes-on-dimensionality-reduction" title="Permanent link">&para;</a></h1>
<h2 id="9-mutual-information-mi">9. Mutual Information (MI)<a class="headerlink" href="#9-mutual-information-mi" title="Permanent link">&para;</a></h2>
<p>Measures reduction in uncertainty:</p>
<div class="arithmatex">\[
I(A,Y) = \sum_{x \in A}\sum_{y \in Y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}
\]</div>
<ul>
<li>If <span class="arithmatex">\(A\)</span> and <span class="arithmatex">\(Y\)</span> are independent → <span class="arithmatex">\(I(A,Y)=0\)</span>.</li>
</ul>
<hr />
<h2 id="10-feature-interactions">10. Feature Interactions<a class="headerlink" href="#10-feature-interactions" title="Permanent link">&para;</a></h2>
<p>Individual features may appear weak, but combinations can be strong.</p>
<ul>
<li>Example: Two independent features become predictive when combined.</li>
<li>Highlights the limitation of <strong>filter methods</strong> which evaluate features individually.</li>
</ul>
<hr />
<h2 id="11-pros-cons-of-filter-methods">11. Pros &amp; Cons of Filter Methods<a class="headerlink" href="#11-pros-cons-of-filter-methods" title="Permanent link">&para;</a></h2>
<p><strong>Advantages:</strong></p>
<ul>
<li>Simple, scalable.</li>
<li>Easily parallelizable.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li>Cannot capture feature interactions.</li>
<li>May select redundant features.</li>
</ul>
<hr />
<h2 id="12-embedded-methods-lasso">12. Embedded Methods: LASSO<a class="headerlink" href="#12-embedded-methods-lasso" title="Permanent link">&para;</a></h2>
<p>Optimization problem:</p>
<div class="arithmatex">\[
\min_\beta \sum_{i=1}^M (y_i - \sum_j x_{ij}\beta_j)^2 + \lambda \sum_{j=1}^N |\beta_j|
\]</div>
<ul>
<li>L1 penalty induces sparsity.</li>
<li>Contrast with Ridge (L2):</li>
</ul>
<div class="arithmatex">\[
\lambda \sum_{j=1}^N \beta_j^2
\]</div>
<ul>
<li>L2 shrinks weights but rarely eliminates them.</li>
</ul>
<hr />
<h2 id="13-benefits-of-feature-selection">13. Benefits of Feature Selection<a class="headerlink" href="#13-benefits-of-feature-selection" title="Permanent link">&para;</a></h2>
<ul>
<li>Improves accuracy.</li>
<li>Reduces computation.</li>
<li>Improves interpretability.</li>
<li>Prevents overfitting.</li>
</ul>
<hr />
<h2 id="14-t-sne-motivation">14. t-SNE Motivation<a class="headerlink" href="#14-t-sne-motivation" title="Permanent link">&para;</a></h2>
<ul>
<li>High-dimensional visualization is difficult.</li>
<li>Project data into 2D or 3D for interpretability.</li>
<li>t-SNE preserves <strong>local structure</strong>.</li>
</ul>
<hr />
<h2 id="15-general-problem-statement">15. General Problem Statement<a class="headerlink" href="#15-general-problem-statement" title="Permanent link">&para;</a></h2>
<p>Given high-dimensional <span class="arithmatex">\(X = \{x_1,...,x_M\}\)</span>, <span class="arithmatex">\(x_i \in \mathbb{R}^N\)</span>:</p>
<p>Find <span class="arithmatex">\(Y = \{y_1,...,y_M\}\)</span>, <span class="arithmatex">\(y_i \in \mathbb{R}^n, n &lt; N\)</span>, minimizing information loss.</p>
<hr />
<h2 id="16-stochastic-neighbor-embedding-sne">16. Stochastic Neighbor Embedding (SNE)<a class="headerlink" href="#16-stochastic-neighbor-embedding-sne" title="Permanent link">&para;</a></h2>
<ul>
<li>Preserves local distances.</li>
<li>Converts distances to conditional probabilities:</li>
</ul>
<div class="arithmatex">\[
p_{j|i} = \frac{e^{-||x_i-x_j||^2/2\sigma_i^2}}{\sum_k e^{-||x_i-x_k||^2/2\sigma_i^2}}
\]</div>
<div class="arithmatex">\[
q_{j|i} = \frac{e^{-||y_i-y_j||^2}}{\sum_k e^{-||y_i-y_k||^2}}
\]</div>
<hr />
<h2 id="17-from-sne-to-t-sne">17. From SNE to t-SNE<a class="headerlink" href="#17-from-sne-to-t-sne" title="Permanent link">&para;</a></h2>
<ul>
<li>High-dimensional similarities use Gaussian.</li>
<li>Low-dimensional similarities use Student’s t-distribution (heavier tails).</li>
</ul>
<div class="arithmatex">\[
q_{ij} = \frac{(1+||y_i-y_j||^2)^{-1}}{\sum_{k \neq l}(1+||y_k-y_l||^2)^{-1}}
\]</div>
<hr />
<h2 id="18-kl-divergence-in-t-sne">18. KL Divergence in t-SNE<a class="headerlink" href="#18-kl-divergence-in-t-sne" title="Permanent link">&para;</a></h2>
<p>Objective:</p>
<div class="arithmatex">\[
KL(P||Q) = \sum_i \sum_j p_{j|i}\log \frac{p_{j|i}}{q_{j|i}}
\]</div>
<ul>
<li>Penalizes mismatches in similarity structure.</li>
<li>Large <span class="arithmatex">\(p_{j|i}\)</span> with small <span class="arithmatex">\(q_{j|i}\)</span> → heavy penalty.</li>
</ul>
<hr />
<h2 id="19-gradient-descent-optimization">19. Gradient Descent Optimization<a class="headerlink" href="#19-gradient-descent-optimization" title="Permanent link">&para;</a></h2>
<p>Gradient update rule:</p>
<div class="arithmatex">\[
\frac{\partial C}{\partial y_i} = 2\sum_j (y_j-y_i)(p_{j|i}-q_{j|i}+p_{i|j}-q_{i|j})
\]</div>
<ul>
<li>Emphasizes preserving close distances.</li>
</ul>
<hr />
<h2 id="20-visualization-example-mnist">20. Visualization Example (MNIST)<a class="headerlink" href="#20-visualization-example-mnist" title="Permanent link">&para;</a></h2>
<ul>
<li>Applied to 6000 MNIST digits.</li>
<li>t-SNE clusters digits cleanly compared to Sammon mapping.</li>
</ul>
<hr />
<h2 id="21-summary-mindmap">21. Summary Mindmap<a class="headerlink" href="#21-summary-mindmap" title="Permanent link">&para;</a></h2>
<pre class="mermaid"><code>mindmap
  root((Dimensionality Reduction))
    Feature Selection
      Wrapper
        Forward Selection
        Backward Elimination
        Heuristic Search
      Filter
        Correlation
        Chi-squared
        Mutual Information
      Embedded
        Lasso (L1)
        Decision Trees
    Feature Learning
      PCA
      Kernel PCA
      MDS
      Autoencoders
      t-SNE
        SNE
        KL Divergence
        Gradient Descent</code></pre>
<hr />
<h1 id="key-takeaways">✅ Key Takeaways<a class="headerlink" href="#key-takeaways" title="Permanent link">&para;</a></h1>
<ul>
<li>Dimensionality reduction combats the curse of dimensionality.</li>
<li>Two approaches: <strong>Feature Selection</strong> (simpler, interpretable) vs. <strong>Feature Learning</strong> (powerful, less interpretable).</li>
<li>Methods include Wrapper, Filter, Embedded selection, PCA, t-SNE, autoencoders.</li>
<li>t-SNE is best for visualization, capturing local structures.</li>
<li>Feature selection methods improve performance and interpretability for predictive modeling.</li>
</ul>
<h1 id="dimensionality-reduction-svd-pca-mf-nmf">Dimensionality Reduction (SVD • PCA • MF • NMF)<a class="headerlink" href="#dimensionality-reduction-svd-pca-mf-nmf" title="Permanent link">&para;</a></h1>
<hr />
<h2 id="motivation">🧭 Motivation<a class="headerlink" href="#motivation" title="Permanent link">&para;</a></h2>
<p>We are deluged with data. Essential information often lives in a much <strong>smaller-dimensional subspace</strong>. Dimensionality reduction aims to:</p>
<ul>
<li>compress data (fewer numbers, less storage),</li>
<li>denoise (throw away directions dominated by noise),</li>
<li>uncover structure (clusters, topics, latent factors),</li>
<li>speed up downstream learning.</li>
</ul>
<pre class="mermaid"><code>flowchart LR
  X["High-dim Data: X in R^(m×n)"] --&gt;|Decompose / Project| Z["Low-dim Representation"]
  Z --&gt;|Reconstruct| Xhat["Approximate X"]
  Xhat --&gt;|Error| Err["Reconstruction Error e"]
  Z --&gt; Models["Downstream models: kNN, SVM, regressors"]

  subgraph Families
    A1["SVD / PCA (linear, global)"]
    A2["MF / NMF (parts-based, sparse)"]
    A3["t-SNE / UMAP (nonlinear, local)"]
  end

  X --&gt; A1
  X --&gt; A2
  X --&gt; A3
</code></pre>
<hr />
<h2 id="1-singular-value-decomposition-svd">1) Singular Value Decomposition (SVD)<a class="headerlink" href="#1-singular-value-decomposition-svd" title="Permanent link">&para;</a></h2>
<h3 id="definition">Definition<a class="headerlink" href="#definition" title="Permanent link">&para;</a></h3>
<p>For any real matrix <span class="arithmatex">\(X \in \mathbb{R}^{m\times n}\)</span>:</p>
<div class="arithmatex">\[
X = U\,\Sigma\,V^T,
\]</div>
<p>where</p>
<ul>
<li><span class="arithmatex">\(U\in\mathbb{R}^{m\times m}\)</span> and <span class="arithmatex">\(V\in\mathbb{R}^{n\times n}\)</span> are <strong>orthogonal</strong> (<span class="arithmatex">\(U^TU=I\)</span>, <span class="arithmatex">\(V^TV=I\)</span>),</li>
<li><span class="arithmatex">\(\Sigma\in\mathbb{R}^{m\times n}\)</span> is diagonal with non‑negative <strong>singular values</strong> <span class="arithmatex">\(\sigma_1\ge\sigma_2\ge\cdots\ge0\)</span>.</li>
</ul>
<h3 id="intuition">Intuition<a class="headerlink" href="#intuition" title="Permanent link">&para;</a></h3>
<p>A linear map = <strong>rotate (V)</strong> → <strong>scale (Σ)</strong> → <strong>rotate (U)</strong>. Large <span class="arithmatex">\(\sigma_i\)</span> indicate energetic directions.</p>
<pre class="mermaid"><code>graph TD
  V[Rotate by V^T] --&gt; S[Scale by Σ] --&gt; U[Rotate by U]</code></pre>
<h3 id="best-rankk-approximation-eckartyoung">Best rank‑k approximation (Eckart–Young)<a class="headerlink" href="#best-rankk-approximation-eckartyoung" title="Permanent link">&para;</a></h3>
<p>Let <span class="arithmatex">\(X_k = U_{:,1:k}\,\Sigma_{1:k,1:k}\,V_{:,1:k}^T\)</span>. Then for any matrix <span class="arithmatex">\(Y\)</span> with <span class="arithmatex">\(\operatorname{rank}(Y)\le k\)</span>:</p>
<div class="arithmatex">\[
\|X - X_k\|_F \le \|X - Y\|_F\quad\text{and}\quad \|X - X_k\|_2 = \sigma_{k+1}.
\]</div>
<h3 id="worked-example-exact-svd-of-a-22-matrix">🔢 Worked Example – Exact SVD of a 2×2 matrix<a class="headerlink" href="#worked-example-exact-svd-of-a-22-matrix" title="Permanent link">&para;</a></h3>
<p>Let</p>
<div class="arithmatex">\[
X=\begin{bmatrix}3 &amp; 1\\1 &amp; 3\end{bmatrix}.
\]</div>
<ol>
<li>Compute <span class="arithmatex">\(X^TX=\begin{bmatrix}10&amp;6\\6&amp;10\end{bmatrix}\)</span>.</li>
<li>Eigenpairs of <span class="arithmatex">\(X^TX\)</span>:</li>
<li><span class="arithmatex">\(\lambda_1=16\)</span> with eigenvector <span class="arithmatex">\(v_1=\tfrac{1}{\sqrt2}[1,\,1]^T\)</span>,</li>
<li><span class="arithmatex">\(\lambda_2=4\)</span> with eigenvector <span class="arithmatex">\(v_2=\tfrac{1}{\sqrt2}[1,\,-1]^T\)</span>. Hence singular values: <span class="arithmatex">\(\sigma_1=\sqrt{16}=4\)</span>, <span class="arithmatex">\(\sigma_2=\sqrt{4}=2\)</span>. Let <span class="arithmatex">\(V=[v_1\ v_2]\)</span>.</li>
<li>Compute left vectors: <span class="arithmatex">\(u_i = \tfrac{1}{\sigma_i} X v_i\)</span>.</li>
<li><span class="arithmatex">\(u_1 = \tfrac{1}{4}Xv_1 = \tfrac{1}{4}\tfrac{1}{\sqrt2}[4,\,4]^T = \tfrac{1}{\sqrt2}[1,1]^T\)</span>,</li>
<li><span class="arithmatex">\(u_2 = \tfrac{1}{2}Xv_2 = \tfrac{1}{2}\tfrac{1}{\sqrt2}[2,\,-2]^T = \tfrac{1}{\sqrt2}[1,-1]^T\)</span>. Let <span class="arithmatex">\(U=[u_1\ u_2]\)</span>, <span class="arithmatex">\(\Sigma=\operatorname{diag}(4,2)\)</span>.</li>
<li>Verify: <span class="arithmatex">\(U\Sigma V^T=X\)</span>.</li>
</ol>
<h3 id="worked-example-rank1-truncation-error">🔢 Worked Example – Rank‑1 truncation &amp; error<a class="headerlink" href="#worked-example-rank1-truncation-error" title="Permanent link">&para;</a></h3>
<p>Rank‑1 approximation <span class="arithmatex">\(X_1=4\,u_1 v_1^T\)</span>. Error in spectral norm = <span class="arithmatex">\(\sigma_2=2\)</span>. In Frobenius norm: <span class="arithmatex">\(\|X-X_1\|_F=\sqrt{\sigma_2^2}=2\)</span>.</p>
<h3 id="power-iteration-computing-top-singular-vector">Power Iteration (computing top singular vector)<a class="headerlink" href="#power-iteration-computing-top-singular-vector" title="Permanent link">&para;</a></h3>
<ul>
<li>Initialize <span class="arithmatex">\(u^{(0)}\)</span> randomly.</li>
<li>Iterate: <span class="arithmatex">\(v^{(t)}=X^Tu^{(t)};\ u^{(t+1)} = Xv^{(t)}/\|Xv^{(t)}\|\)</span>.</li>
<li>Converges to top left singular vector.</li>
</ul>
<pre class="mermaid"><code>sequenceDiagram
  participant U as u^(t)
  participant XT as X^T
  participant V as v^(t)
  participant X as X
  U-&gt;&gt;XT: v = X^T u
  XT--&gt;&gt;V: v^(t)
  V-&gt;&gt;X: u' = X v
  X--&gt;&gt;U: normalize(u')</code></pre>
<hr />
<h2 id="2-principal-component-analysis-pca">2) Principal Component Analysis (PCA)<a class="headerlink" href="#2-principal-component-analysis-pca" title="Permanent link">&para;</a></h2>
<h3 id="two-equivalent-views">Two equivalent views<a class="headerlink" href="#two-equivalent-views" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Eigenview</strong> on covariance: with centered data <span class="arithmatex">\(X_c\)</span>, <span class="arithmatex">\(\tfrac{1}{n}X_cX_c^T = U\Lambda U^T\)</span>. PCs are columns of <span class="arithmatex">\(U\)</span>.</li>
<li><strong>SVD view</strong>: <span class="arithmatex">\(X_c=U\Sigma V^T\)</span>. Then principal directions = columns of <span class="arithmatex">\(U\)</span>; variances = <span class="arithmatex">\(\sigma_i^2/(n-1)\)</span>.</li>
</ol>
<h3 id="algorithm">Algorithm<a class="headerlink" href="#algorithm" title="Permanent link">&para;</a></h3>
<ol>
<li>Center features: <span class="arithmatex">\(X_c = X - \mu 1^T\)</span>.</li>
<li>Compute SVD: <span class="arithmatex">\(X_c=U\Sigma V^T\)</span>.</li>
<li>Choose <span class="arithmatex">\(k\)</span> via explained variance ratio <span class="arithmatex">\(\sum_{i=1}^k \sigma_i^2 / \sum_{i} \sigma_i^2\)</span>.</li>
<li>Low‑dim representation (scores): <span class="arithmatex">\(Z=U_{:,1:k}^T X_c\)</span>.</li>
<li>Reconstruction: <span class="arithmatex">\(\hat X = U_{:,1:k}Z + \mu 1^T\)</span>.</li>
</ol>
<pre class="mermaid"><code>flowchart LR
  A[Raw data X] --&gt; B[Center by mean μ]
  B --&gt; C[SVD of X_c]
  C --&gt; D[Pick k by EVR]
  D --&gt; E[Scores Z = U_k^T X_c]
  E --&gt; F[Use Z for ML]
  E --&gt; G[Reconstruct X̂ = U_k Z + μ]</code></pre>
<h3 id="worked-example-pca-on-a-2d-toy-dataset">🔢 Worked Example – PCA on a 2D toy dataset<a class="headerlink" href="#worked-example-pca-on-a-2d-toy-dataset" title="Permanent link">&para;</a></h3>
<p>Data (n=5 points): <span class="arithmatex">\((2,0),(0,2),(3,1),(4,0),(0,3)\)</span>.</p>
<ol>
<li>Mean: <span class="arithmatex">\(\mu=[1.8,\ 1.2]^T\)</span>. Centered matrix</li>
</ol>
<div class="arithmatex">\[
X_c=\begin{bmatrix}
0.2 &amp; -1.2\\ -1.8 &amp; 0.8\\ 1.2 &amp; -0.2\\ 2.2 &amp; -1.2\\ -1.8 &amp; 1.8
\end{bmatrix}.
\]</div>
<ol>
<li>Covariance: <span class="arithmatex">\(C=\tfrac{1}{n-1}X_c^TX_c=\begin{bmatrix}3.7 &amp; -2.9\\ -2.9 &amp; 2.3\end{bmatrix}.\)</span></li>
<li>Eigenpairs (rounded): <span class="arithmatex">\(\lambda_1\approx5.89\)</span> (dir. <span class="arithmatex">\(v_1\propto[0.82,-0.57]\)</span>), <span class="arithmatex">\(\lambda_2\approx0.11\)</span>.</li>
<li><strong>Explained variance</strong> of PC1: <span class="arithmatex">\(5.89/(5.89+0.11)\approx98.2\%\)</span>.</li>
<li>Project onto PC1: scores = <span class="arithmatex">\(Z = X_c v_1\)</span> (one scalar per point).</li>
<li>Reconstruction with <span class="arithmatex">\(k=1\)</span>: <span class="arithmatex">\(\hat X = (v_1 v_1^T)X_c + \mu\)</span>.\
   Result: good 1D compression with tiny error (since <span class="arithmatex">\(\lambda_2\)</span> small).</li>
</ol>
<h3 id="pca-for-images-patch-pca">PCA for images (patch PCA)<a class="headerlink" href="#pca-for-images-patch-pca" title="Permanent link">&para;</a></h3>
<ul>
<li>Patch size 12×12 → 144‑D.</li>
<li>Compute PCs → basis filters (edges, blobs).</li>
<li>Reconstruction error vs k typically decays fast; keep <span class="arithmatex">\(k\)</span> around 20–60 for strong compression on patches.</li>
</ul>
<hr />
<h2 id="3-latent-semantic-indexing-lsi-via-svd-text">3) Latent Semantic Indexing (LSI) via SVD (text)<a class="headerlink" href="#3-latent-semantic-indexing-lsi-via-svd-text" title="Permanent link">&para;</a></h2>
<p>Given a <strong>term–document</strong> matrix <span class="arithmatex">\(X\)</span> (tf or tf–idf). Compute rank‑<span class="arithmatex">\(k\)</span> SVD: <span class="arithmatex">\(X\approx U_k\Sigma_kV_k^T\)</span>.</p>
<ul>
<li>Columns of <span class="arithmatex">\(U_k\)</span> give <strong>term embeddings</strong>; columns of <span class="arithmatex">\(V_k\)</span> give <strong>document embeddings</strong>; <span class="arithmatex">\(\Sigma_k\)</span> rescales.</li>
<li>Similarity in this space captures <strong>latent topics</strong> even if exact words differ.</li>
</ul>
<h3 id="worked-example-mini-tdm-3-terms-3-docs">🔢 Worked Example – Mini TDM (3 terms × 3 docs)<a class="headerlink" href="#worked-example-mini-tdm-3-terms-3-docs" title="Permanent link">&para;</a></h3>
<div class="arithmatex">\[
X=\begin{bmatrix}
1&amp;1&amp;0\\
1&amp;0&amp;1\\
0&amp;1&amp;1
\end{bmatrix}.
\]</div>
<ul>
<li>Full SVD gives <span class="arithmatex">\(\sigma\approx(2,1,0)\)</span>.</li>
<li>Rank‑2 LSI keeps first two singular values/vectors; cosine similarities between docs improve (synonymy effect).</li>
</ul>
<hr />
<h2 id="4-matrix-factorization-mf">4) Matrix Factorization (MF)<a class="headerlink" href="#4-matrix-factorization-mf" title="Permanent link">&para;</a></h2>
<p>We model an (often <strong>incomplete</strong>) matrix <span class="arithmatex">\(R\in\mathbb{R}^{n_u\times n_i}\)</span> of user–item interactions as</p>
<div class="arithmatex">\[
R \approx U^T V + b\,1^T + 1\,\tilde b^T,
\]</div>
<p>where <span class="arithmatex">\(U\in\mathbb{R}^{r\times n_u}\)</span> and <span class="arithmatex">\(V\in\mathbb{R}^{r\times n_i}\)</span> are latent factors, and <span class="arithmatex">\(b,\tilde b\)</span> biases.</p>
<h3 id="objective-with-l2-regularization">Objective (with L2 regularization)<a class="headerlink" href="#objective-with-l2-regularization" title="Permanent link">&para;</a></h3>
<div class="arithmatex">\[
\min_{U,V,b,\tilde b} \sum_{(i,j)\in\Omega} (r_{ij}-u_i^T v_j - b_i - \tilde b_j)^2 
+ \lambda(\|U\|_F^2+\|V\|_F^2 + \|b\|_2^2 + \|\tilde b\|_2^2),
\]</div>
<p>where <span class="arithmatex">\(\Omega\)</span> is the set of observed entries.</p>
<h3 id="algorithms">Algorithms<a class="headerlink" href="#algorithms" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>ALS</strong> (Alternating Least Squares): fix <span class="arithmatex">\(U\)</span>, solve least squares for <span class="arithmatex">\(V\)</span>; swap. Scales well in Spark.</li>
<li><strong>SGD</strong>: update on each observed triple using gradients; supports online/streaming.</li>
</ul>
<pre class="mermaid"><code>flowchart LR
  R[Observed ratings R] --&gt; ALS{ALS Loop}
  ALS --&gt;|fix U| SolveV[Solve for each v_j]
  ALS --&gt;|fix V| SolveU[Solve for each u_i]
  SolveU --&gt; ALS
  SolveV --&gt; ALS
  ALS --&gt; Pred[Predict r̂_ij = u_i^T v_j + b_i + b̃_j]</code></pre>
<h3 id="worked-example-tiny-mf-with-als-rank-r2">🔢 Worked Example – Tiny MF with ALS (rank r=2)<a class="headerlink" href="#worked-example-tiny-mf-with-als-rank-r2" title="Permanent link">&para;</a></h3>
<p>Observed ratings ("?" missing):</p>
<div class="arithmatex">\[
R=\begin{array}{c|ccc}
 &amp; i_1 &amp; i_2 &amp; i_3 \\\hline
u_1 &amp; 5 &amp; 3 &amp; ?\\
u_2 &amp; 4 &amp; ? &amp; 1\\
\end{array}
\]</div>
<p>Initialize <span class="arithmatex">\(U=\begin{bmatrix}1&amp;0\\0&amp;1\end{bmatrix}\)</span>, <span class="arithmatex">\(V=\begin{bmatrix}1&amp;1&amp;1\\1&amp;1&amp;1\end{bmatrix}\)</span>. <strong>One ALS sweep</strong> (showing item 2):</p>
<ul>
<li>Users who rated <span class="arithmatex">\(i_2\)</span>: only <span class="arithmatex">\(u_1\)</span> with rating 3. Solve <span class="arithmatex">\(\min_{v_2}\ (3 - u_1^Tv_2)^2 + \lambda\|v_2\|^2\)</span>. With <span class="arithmatex">\(u_1=[1,0]^T\)</span> and <span class="arithmatex">\(\lambda=0.1\)</span>, closed form gives <span class="arithmatex">\(v_2 = (U^{(2)T}U^{(2)}+\lambda I)^{-1}U^{(2)T}r^{(2)} = (1.1I)^{-1}[3,0]^T = [2.727,0]^T\)</span>. Repeat for other items and then solve for user vectors given items. After a few ALS rounds, predict missing: <span class="arithmatex">\(\hat r_{1,3}=u_1^Tv_3 + b_1 + \tilde b_3\)</span> (biases often raise accuracy by 5–10%).</li>
</ul>
<h3 id="worked-example-onestep-sgd-update">🔢 Worked Example – One‑step SGD update<a class="headerlink" href="#worked-example-onestep-sgd-update" title="Permanent link">&para;</a></h3>
<p>Loss on observed <span class="arithmatex">\((i,j)=(1,2)\)</span>: <span class="arithmatex">\(\ell = (r_{12}-u_1^Tv_2)^2 + \lambda(\|u_1\|^2+\|v_2\|^2)\)</span>. Gradients: <span class="arithmatex">\(\nabla_{u_1}=-2(r_{12}-u_1^Tv_2)v_2 + 2\lambda u_1\)</span>, similarly for <span class="arithmatex">\(v_2\)</span>. Update with step <span class="arithmatex">\(\eta\)</span>: <span class="arithmatex">\(u_1 \leftarrow u_1 - \eta\nabla_{u_1}\)</span>, <span class="arithmatex">\(v_2 \leftarrow v_2 - \eta\nabla_{v_2}\)</span>.</p>
<hr />
<h2 id="5-nonnegative-matrix-factorization-nmf">5) Non‑negative Matrix Factorization (NMF)<a class="headerlink" href="#5-nonnegative-matrix-factorization-nmf" title="Permanent link">&para;</a></h2>
<p>We seek non‑negative factors <span class="arithmatex">\(W\in\mathbb{R}_+^{m\times r}\)</span>, <span class="arithmatex">\(H\in\mathbb{R}_+^{r\times n}\)</span> such that <span class="arithmatex">\(X \approx WH.\)</span> This encourages <strong>parts‑based</strong> representations (e.g., eyes, nose, mouth for faces; topics for text).</p>
<h3 id="multiplicative-updates-lee-seung">Multiplicative updates (Lee &amp; Seung)<a class="headerlink" href="#multiplicative-updates-lee-seung" title="Permanent link">&para;</a></h3>
<p>For objective <span class="arithmatex">\(\min_{W,H}\ \|X-WH\|_F^2\)</span>:</p>
<div class="arithmatex">\[
H \leftarrow H \odot \frac{W^T X}{W^T W H},\quad
W \leftarrow W \odot \frac{X H^T}{W H H^T}\quad (\odot: \text{elementwise}).
\]</div>
<h3 id="worked-example-nmf-on-a-43-matrix-1-iteration">🔢 Worked Example – NMF on a 4×3 matrix (1 iteration)<a class="headerlink" href="#worked-example-nmf-on-a-43-matrix-1-iteration" title="Permanent link">&para;</a></h3>
<div class="arithmatex">\[
X = \begin{bmatrix}4&amp;1&amp;0\\3&amp;0&amp;1\\0&amp;2&amp;3\\0&amp;1&amp;4\end{bmatrix},\quad r=2,
\ W=\begin{bmatrix}1&amp;0.5\\1&amp;0.5\\0.5&amp;1\\0.5&amp;1\end{bmatrix},\ H=\begin{bmatrix}1&amp;1&amp;1\\1&amp;1&amp;1\end{bmatrix}.
\]</div>
<p>Compute updates (showing <span class="arithmatex">\(H\)</span> numerator/denominator first column):</p>
<ul>
<li><span class="arithmatex">\(W^TX = \begin{bmatrix}7&amp;2&amp;1\\4.5&amp;2&amp;4.5\end{bmatrix}\Rightarrow (W^TX)_{:,1}=[7,\ 4.5]^T\)</span>.</li>
<li><span class="arithmatex">\(W^TWH = (W^TW)H\)</span> with <span class="arithmatex">\(W^TW=\begin{bmatrix}2.5&amp;2\\2&amp;2.5\end{bmatrix}\)</span> gives first col <span class="arithmatex">\([4.5,\ 4.5]^T\)</span>.</li>
<li>New first column of <span class="arithmatex">\(H\)</span>: elementwise multiply by <span class="arithmatex">\([7/4.5,\ 4.5/4.5]=[1.556,\ 1]\)</span>. Repeat for other columns; then update <span class="arithmatex">\(W\)</span> analogously. Observation: factors stay non‑negative and begin specializing columns/rows.</li>
</ul>
<hr />
<h2 id="6-pca-vs-autoencoders-ae-encoderdecoder-view">6) PCA vs Autoencoders (AE) – Encoder–Decoder view<a class="headerlink" href="#6-pca-vs-autoencoders-ae-encoderdecoder-view" title="Permanent link">&para;</a></h2>
<ul>
<li>PCA: linear AE with encoder <span class="arithmatex">\(U_k^T\)</span>, decoder <span class="arithmatex">\(U_k\)</span>, MSE loss, orthogonal columns.</li>
<li>AE: can be deep/nonlinear; objective can include sparsity, denoising, contrastive losses.</li>
</ul>
<pre class="mermaid"><code>flowchart LR
  X["X ∈ R^{m×n}"] --&gt; E["Encoder fθ"]
  E --&gt; Z["z ∈ R^{k×n}"]
  Z --&gt; D["Decoder gφ"]
  D --&gt; Xhat["X̂"]

  X --- PCA["PCA"]
  PCA --- Xhat
  X --- AE["Autoencoder"]
  AE --- Xhat

  %% Style edge labels as separate nodes
  PCA:::linear
  AE:::nonlinear

  classDef linear fill:#d9ead3,stroke:#333,stroke-width:1px;
  classDef nonlinear fill:#fce5cd,stroke:#333,stroke-width:1px;
</code></pre>
<hr />
<h2 id="7-practical-guidance-diagnostics">7) Practical guidance &amp; diagnostics<a class="headerlink" href="#7-practical-guidance-diagnostics" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Centering &amp; Scaling</strong>: PCA requires centering; consider standardizing features with different scales.</li>
<li><strong>Choosing k</strong>: Scree plot (<span class="arithmatex">\(\sigma_i\)</span>), cumulative EVR ≥ 0.90–0.99 for compression; cross‑validate for downstream task.</li>
<li><strong>Out‑of‑sample transforms</strong>: Store <span class="arithmatex">\(\mu\)</span> and <span class="arithmatex">\(U_k\)</span> to project new data: <span class="arithmatex">\(z=U_k^T(x-\mu)\)</span>.</li>
<li><strong>Cold start in MF</strong>: use content features or priors; biases stabilize.</li>
<li><strong>Regularization</strong>: <span class="arithmatex">\(\lambda\)</span> prevents overfitting in MF; shrink small PCs if noisy (Tikhonov).</li>
<li><strong>Sparsity</strong>: Prefer MF/NMF when matrices are highly sparse; SVD on dense TDM often uses truncated solvers (Lanczos).</li>
</ul>
<hr />
<h2 id="8-extended-solved-miniproblems">8) Extended solved mini‑problems<a class="headerlink" href="#8-extended-solved-miniproblems" title="Permanent link">&para;</a></h2>
<h3 id="81-pca-by-hand-32-matrix">8.1 PCA by hand (3×2 matrix)<a class="headerlink" href="#81-pca-by-hand-32-matrix" title="Permanent link">&para;</a></h3>
<div class="arithmatex">\[
X=\begin{bmatrix}2&amp;0\\0&amp;2\\3&amp;1\end{bmatrix},\ \mu=\tfrac{1}{3}[5,\ 1]^T,\ X_c=X-\mu 1^T.
\]</div>
<p>Compute SVD of <span class="arithmatex">\(X_c\)</span> (algebra similar to the earlier 2×2 SVD). Keep largest <span class="arithmatex">\(\sigma\)</span> only; reconstruct <span class="arithmatex">\(\hat X\)</span>; compute error <span class="arithmatex">\(\|X-\hat X\|_F\)</span>.</p>
<h3 id="82-filling-a-missing-rating-with-mf-closed-form-no-bias">8.2 Filling a missing rating with MF (closed form, no bias)<a class="headerlink" href="#82-filling-a-missing-rating-with-mf-closed-form-no-bias" title="Permanent link">&para;</a></h3>
<p>Given two users/two items with observed <span class="arithmatex">\(r_{11}=5, r_{21}=4, r_{12}=3\)</span>; estimate <span class="arithmatex">\(r_{22}\)</span> with rank‑1 MF and <span class="arithmatex">\(\lambda=0\)</span>.\
Solve <span class="arithmatex">\(\min_{u_1,u_2,v_1,v_2}\sum (r_{ij}-u_iv_j)^2\)</span>. Closed form (normal equations) gives <span class="arithmatex">\(u_1=\tfrac{5}{v_1}, u_2=\tfrac{4}{v_1}, v_2=\tfrac{3}{u_1}\Rightarrow r_{22}=u_2v_2=\tfrac{4}{v_1}\cdot\tfrac{3}{u_1}=\tfrac{12}{5}\approx2.4\)</span> (one of the valid minima). Adding <span class="arithmatex">\(\lambda&gt;0\)</span> stabilizes.</p>
<h3 id="83-choosing-k-via-error-bound">8.3 Choosing k via error bound<a class="headerlink" href="#83-choosing-k-via-error-bound" title="Permanent link">&para;</a></h3>
<p>If singular values (descending) are <span class="arithmatex">\((10, 3, 1, 0.2, …)\)</span> and we keep <span class="arithmatex">\(k=2\)</span>, then</p>
<ul>
<li>spectral‑norm error = <span class="arithmatex">\(\sigma_{3}=1\)</span>,</li>
<li>relative Frobenius error <span class="arithmatex">\(\approx\sqrt{1^2+0.2^2}/\sqrt{10^2+3^2+1^2+0.2^2}\approx\tfrac{1.02}{10.53}\approx9.7\%\)</span>.</li>
</ul>
<h3 id="84-nmf-topic-sketch-on-tdm-toy">8.4 NMF topic sketch on TDM (toy)<a class="headerlink" href="#84-nmf-topic-sketch-on-tdm-toy" title="Permanent link">&para;</a></h3>
<p>Terms = {<strong>color, fabric, size</strong>}, Docs = {d1,d2,d3} with <span class="arithmatex">\(X=\begin{bmatrix}4&amp;1&amp;0\\3&amp;0&amp;1\\0&amp;2&amp;3\end{bmatrix}\)</span>. With <span class="arithmatex">\(r=2\)</span>, one topic leans on {color,fabric}, the other on {fabric,size}. Multiplicative updates separate them over iterations.</p>
<hr />
<h2 id="9-quick-reference-cheatsheet">9) Quick reference (cheatsheet)<a class="headerlink" href="#9-quick-reference-cheatsheet" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>SVD</strong>: <span class="arithmatex">\(X=U\Sigma V^T\)</span>; rank‑k truncation is optimal (EYM theorem).</li>
<li><strong>PCA</strong>: SVD of centered data; scores = <span class="arithmatex">\(U_k^T X_c\)</span>; reconstruction = <span class="arithmatex">\(U_k U_k^T X_c + \mu\)</span>.</li>
<li><strong>MF (ALS)</strong>: solve <span class="arithmatex">\((U^{(j)T}U^{(j)}+\lambda I)v_j=U^{(j)T}r^{(j)}\)</span>; symmetric for <span class="arithmatex">\(u_i\)</span>.</li>
<li><strong>NMF</strong>: multiplicative updates; non‑negativity ⇒ parts‑based, interpretable factors.</li>
</ul>
<hr />
<h3 id="appendix-a-notation">Appendix A — Notation<a class="headerlink" href="#appendix-a-notation" title="Permanent link">&para;</a></h3>
<ul>
<li><span class="arithmatex">\(m\)</span>: features; <span class="arithmatex">\(n\)</span>: samples; <span class="arithmatex">\(r\)</span>: reduced rank.</li>
<li><span class="arithmatex">\(\|\cdot\|_F\)</span>: Frobenius norm; <span class="arithmatex">\(\|\cdot\|_2\)</span>: spectral norm.</li>
<li><span class="arithmatex">\(1\)</span>: all‑ones vector; <span class="arithmatex">\(I\)</span>: identity.</li>
</ul>
<hr />







  
    
  
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="August 31, 2025 10:20:26 UTC">August 31, 2025</span>
  </span>

    
    
    
    
  </aside>





                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../.." class="md-footer__link md-footer__link--prev" aria-label="Previous: Home">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Home
              </div>
            </div>
          </a>
        
        
          
          <a href="../lecture4/" class="md-footer__link md-footer__link--next" aria-label="Next: Unsupervised Learning">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Unsupervised Learning
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="https://github.com/adityachauhan0" target="_blank" rel="noopener" title="GitHub" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.instant", "navigation.tracking", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.footer", "toc.integrate", "toc.follow", "search.suggest", "search.highlight", "search.share", "content.code.copy", "content.code.annotate", "content.tooltips", "header.autohide", "content.action.edit", "content.action.view"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.92b07e13.min.js"></script>
      
        <script src="https://unpkg.com/mermaid@10/dist/mermaid.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
      
        <script src="../../js/katex-init.js"></script>
      
    
  </body>
</html>