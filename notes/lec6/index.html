
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="My ML notes, cheatsheets, and experiments">
      
      
      
        <link rel="canonical" href="https://adityachauhan0.github.io/amazon-ml/notes/lec6/">
      
      
        <link rel="prev" href="../lec5/">
      
      
        <link rel="next" href="../../cheatsheets/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.17">
    
    
      
        <title>Reinforcement Learning - Machine Learning Notes</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.7e37652d.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="pink">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#reinforcement-learning-rl-notes" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Machine Learning Notes" class="md-header__button md-logo" aria-label="Machine Learning Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Machine Learning Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Reinforcement Learning
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="pink"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="pink"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="pink"  aria-label="Switch to high contrast"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to high contrast" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 20c-2.21 0-4.21-.9-5.66-2.34L17.66 6.34A8.01 8.01 0 0 1 20 12a8 8 0 0 1-8 8M6 8h2V6h1.5v2h2v1.5h-2v2H8v-2H6M12 2A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10A10 10 0 0 0 12 2m0 14h5v-1.5h-5z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/adityachauhan0/amazon-ml" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    adityachauhan0/amazon-ml
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../lecture3/" class="md-tabs__link">
          
  
  
    
  
  Notes

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../cheatsheets/" class="md-tabs__link">
        
  
  
    
  
  Cheatsheets

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Machine Learning Notes" class="md-nav__button md-logo" aria-label="Machine Learning Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Machine Learning Notes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/adityachauhan0/amazon-ml" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    adityachauhan0/amazon-ml
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Notes
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Notes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lecture3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dimensionality Reduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lecture4/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Unsupervised Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lec5/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sequential Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#what-is-reinforcement-learning-rl" class="md-nav__link">
    <span class="md-ellipsis">
      📌 What is Reinforcement Learning (RL)?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#difference-between-supervised-learning-and-rl" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Difference Between Supervised Learning and RL
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#agentenvironment-loop" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Agent–Environment Loop
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#markov-decision-process-mdp" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Markov Decision Process (MDP)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#discount-factor-gamma" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Discount Factor (\(\gamma\))
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#exploration-vs-exploitation-dilemma" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Exploration vs Exploitation Dilemma
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multi-armed-bandit-mab-problem" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Multi-Armed Bandit (MAB) Problem
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regret-in-bandits" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Regret in Bandits
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#epsilon-greedy-strategy" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Epsilon-Greedy Strategy
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#upper-confidence-bound-ucb" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Upper Confidence Bound (UCB)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#thompson-sampling" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Thompson Sampling
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contextual-bandits" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Contextual Bandits
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contextual-bandit-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Contextual Bandit Algorithms
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary-of-contextual-bandits" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Summary of Contextual Bandits
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#big-picture" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Big Picture
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cheatsheets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Cheatsheets
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/adityachauhan0/amazon-ml/edit/main/docs/notes/lec6.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/adityachauhan0/amazon-ml/raw/main/docs/notes/lec6.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


<h1 id="reinforcement-learning-rl-notes">Reinforcement Learning (RL) Notes<a class="headerlink" href="#reinforcement-learning-rl-notes" title="Permanent link">&para;</a></h1>
<h2 id="what-is-reinforcement-learning-rl">📌 What is Reinforcement Learning (RL)?<a class="headerlink" href="#what-is-reinforcement-learning-rl" title="Permanent link">&para;</a></h2>
<p>Reinforcement Learning (RL) is a <strong>computational framework for sequential decision making</strong>. The central idea is that an <strong>agent</strong> learns to interact with an <strong>environment</strong> in order to maximize some notion of <strong>cumulative reward</strong>.</p>
<ul>
<li><strong>Key Characteristics:</strong></li>
<li><strong>Sequential:</strong> Decisions happen over time, not independently.</li>
<li><strong>Trial-and-error learning:</strong> The agent must try actions and learn from feedback.</li>
<li><strong>Delayed consequences:</strong> Rewards may not be immediate, requiring planning.</li>
<li><strong>Exploration vs exploitation:</strong> Balance between trying new actions vs. leveraging known good ones.</li>
</ul>
<pre class="mermaid"><code>flowchart TD
    subgraph RL [Reinforcement Learning Framework]
        A[Agent] -- Action/Control --&gt; E[Environment]
        E -- State/Observation --&gt; A
        E -- Reward/Cost --&gt; A
    end</code></pre>
<hr />
<h2 id="difference-between-supervised-learning-and-rl">📌 Difference Between Supervised Learning and RL<a class="headerlink" href="#difference-between-supervised-learning-and-rl" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Supervised Learning</th>
<th>Reinforcement Learning</th>
</tr>
</thead>
<tbody>
<tr>
<td>Training Data</td>
<td>Pre-collected, labeled</td>
<td>No initial dataset; learns through interaction</td>
</tr>
<tr>
<td>Labels</td>
<td>Always known</td>
<td>Rewards are <strong>unknown</strong>, possibly stochastic</td>
</tr>
<tr>
<td>Assumptions</td>
<td>IID data</td>
<td>Data is sequential, non-IID</td>
</tr>
<tr>
<td>Goal</td>
<td>Minimize prediction error</td>
<td>Maximize cumulative reward over time</td>
</tr>
<tr>
<td>Example</td>
<td>Spam filtering, regression</td>
<td>Robot navigation, recommendation systems</td>
</tr>
</tbody>
</table>
<pre class="mermaid"><code>flowchart LR
    SL[Supervised Learning] --&gt;|Pre-collected Data| Model
    RL[Reinforcement Learning] --&gt;|Interaction| Env
    Env --&gt;|Reward/Next State| RL</code></pre>
<hr />
<h2 id="agentenvironment-loop">📌 Agent–Environment Loop<a class="headerlink" href="#agentenvironment-loop" title="Permanent link">&para;</a></h2>
<p>At each discrete time step <span class="arithmatex">\(t\)</span>:
1. Agent observes <strong>state</strong> <span class="arithmatex">\(S_t\)</span>.
2. Agent selects an <strong>action</strong> <span class="arithmatex">\(A_t\)</span>.
3. Environment responds with a <strong>reward</strong> <span class="arithmatex">\(R_t\)</span> and a new <strong>state</strong> <span class="arithmatex">\(S_{t+1}\)</span>.</p>
<pre class="mermaid"><code>sequenceDiagram
    participant Agent
    participant Environment

    Agent-&gt;&gt;Environment: Selects Action $A_t$
    Environment--&gt;&gt;Agent: Returns Reward $R_t$
    Environment--&gt;&gt;Agent: Provides Next State $S_{t+1}$</code></pre>
<p>Mathematically:
$$
S_t \xrightarrow{A_t} S_{t+1}, \quad R_t = R(S_t, A_t)
$$</p>
<hr />
<h2 id="markov-decision-process-mdp">📌 Markov Decision Process (MDP)<a class="headerlink" href="#markov-decision-process-mdp" title="Permanent link">&para;</a></h2>
<p>An MDP provides the <strong>mathematical foundation</strong> for RL. Defined as a tuple:
$$
(S, A, R, P, \gamma)
$$</p>
<ul>
<li><strong>States (S):</strong> The set of all possible configurations of the environment.</li>
<li><strong>Actions (A):</strong> The set of possible moves the agent can make.</li>
<li><strong>Reward function (R):</strong> Maps <span class="arithmatex">\((s, a)\)</span> to a scalar signal.</li>
<li><strong>Transition probabilities (P):</strong>
  $$ P_a(s,s') = Pr(S_{t+1}=s'|S_t=s, A_t=a) $$</li>
<li><strong>Discount factor (<span class="arithmatex">\(\gamma\)</span>):</strong> Weighs immediate vs. future rewards.</li>
</ul>
<blockquote>
<p><strong>Objective:</strong>
Maximize the expected discounted return:
$$
G_t = E\left[ \sum_{k=0}^\infty \gamma^k R_{t+k+1} \right]
$$</p>
</blockquote>
<pre class="mermaid"><code>flowchart LR
    S0((S0)) --&gt;|A0| S1((S1))
    S1 --&gt;|A1| S2((S2))
    S2 --&gt;|A2| S3((S3))
    S3 --&gt;|...| S4((...))</code></pre>
<hr />
<h2 id="discount-factor-gamma">📌 Discount Factor (<span class="arithmatex">\(\gamma\)</span>)<a class="headerlink" href="#discount-factor-gamma" title="Permanent link">&para;</a></h2>
<p>The <strong>discount factor</strong> <span class="arithmatex">\(\gamma \in [0,1]\)</span> represents how much future rewards are valued relative to immediate ones.</p>
<ul>
<li>If <span class="arithmatex">\(\gamma = 0\)</span> → Only immediate rewards matter.</li>
<li>If <span class="arithmatex">\(\gamma = 1\)</span> → Future rewards are valued equally as present ones.</li>
<li>Typically: <span class="arithmatex">\(0 &lt; \gamma &lt; 1\)</span> for balance.</li>
</ul>
<p>Example:
- Predicting stock prices 5 years from now is uncertain → lower weight.
- Receiving $10 today vs. $10 after 5 years → prefer immediate.</p>
<div class="arithmatex">\[
V(s) = E\left[ R_0 + \gamma R_1 + \gamma^2 R_2 + \dots \mid S_0=s \right]
\]</div>
<pre class="mermaid"><code>flowchart TD
    Now[Immediate Reward] -. High Weight .-&gt; Utility
    Future[Future Reward] -. Discounted by γ .-&gt; Utility</code></pre>
<hr />
<h2 id="exploration-vs-exploitation-dilemma">📌 Exploration vs Exploitation Dilemma<a class="headerlink" href="#exploration-vs-exploitation-dilemma" title="Permanent link">&para;</a></h2>
<p>The agent must <strong>explore</strong> new actions to learn but also <strong>exploit</strong> known good ones to maximize reward.</p>
<ul>
<li><strong>Exploration:</strong> Try less-known actions → gather information.</li>
<li><strong>Exploitation:</strong> Choose best-known action → maximize immediate reward.</li>
</ul>
<p>Examples:
- Restaurant choice: try new vs. go to favorite.
- Music: listen to fav vs. discover new.</p>
<pre class="mermaid"><code>flowchart TD
    Start --&gt;|Exploration| TryNew[Try a New Action]
    Start --&gt;|Exploitation| Best[Choose Best Known Action]</code></pre>
<hr />
<h2 id="multi-armed-bandit-mab-problem">📌 Multi-Armed Bandit (MAB) Problem<a class="headerlink" href="#multi-armed-bandit-mab-problem" title="Permanent link">&para;</a></h2>
<p>Simplest RL setting:
- One state only.
- Multiple actions (arms).
- Each arm yields a random reward.</p>
<ul>
<li><strong>Tuple:</strong> <span class="arithmatex">\((A, R)\)</span></li>
<li><strong>Actions (A):</strong> Finite set of arms <span class="arithmatex">\(\{a_1, a_2, ..., a_m\}\)</span></li>
<li><strong>Rewards (R):</strong> Unknown distributions.</li>
</ul>
<p>At each step <span class="arithmatex">\(t\)</span>:
- Agent picks <span class="arithmatex">\(A_t\)</span>.
- Environment provides <span class="arithmatex">\(R_t\)</span>.</p>
<p><strong>Goal:</strong>
$$
\max E \left[ \sum_{t=1}^T R_t \right]
$$</p>
<pre class="mermaid"><code>flowchart LR
    Agent --&gt;|Pull Arm| Slot[Slot Machine Arms]
    Slot --&gt;|Reward| Agent</code></pre>
<hr />
<h2 id="regret-in-bandits">📌 Regret in Bandits<a class="headerlink" href="#regret-in-bandits" title="Permanent link">&para;</a></h2>
<p>Regret quantifies <strong>how much reward was lost compared to the optimal policy</strong>.</p>
<ul>
<li>Mean reward of action <span class="arithmatex">\(A\)</span>:
$$ V_A = E_{R \sim R_A}[R|A] $$</li>
<li>Optimal action:
$$ A^* = \arg\max_A V_A $$</li>
<li>Regret after <span class="arithmatex">\(T\)</span> rounds:
$$ Regret(T) = T \cdot V_{A^*} - \sum_{t=1}^T V_{A_t} $$</li>
</ul>
<pre class="mermaid"><code>flowchart LR
    Opt[Optimal Arm A*] --&gt; HighReward
    Agent[Chosen Arms] --&gt; ActualReward
    HighReward -. difference .-&gt; Regret</code></pre>
<hr />
<h2 id="epsilon-greedy-strategy">📌 Epsilon-Greedy Strategy<a class="headerlink" href="#epsilon-greedy-strategy" title="Permanent link">&para;</a></h2>
<ul>
<li>With probability <span class="arithmatex">\(\epsilon\)</span>: pick a random arm (exploration).</li>
<li>With probability <span class="arithmatex">\(1-\epsilon\)</span>: pick the best-known arm (exploitation).</li>
</ul>
<pre class="mermaid"><code>flowchart TD
    Start --&gt;|ε| Rand[Choose Random Arm]
    Start --&gt;|1-ε| Best[Choose Best Arm]
    Rand --&gt; Arms
    Best --&gt; Arms
    Arms[Reward Observed]</code></pre>
<p>Variants:
- <strong>ε-first:</strong> Explore first few rounds, then exploit.
- <strong>ε-decreasing:</strong> Reduce <span class="arithmatex">\(ε\)</span> over time.</p>
<hr />
<h2 id="upper-confidence-bound-ucb">📌 Upper Confidence Bound (UCB)<a class="headerlink" href="#upper-confidence-bound-ucb" title="Permanent link">&para;</a></h2>
<p>Balances exploration and exploitation via confidence intervals.</p>
<ul>
<li>Estimate mean reward <span class="arithmatex">\(\bar V_a\)</span>.</li>
<li>Add exploration bonus:
$$
UCB_a(t) = \bar V_a + \sqrt{\frac{2 \ln t}{N_a(t)}}
$$</li>
</ul>
<pre class="mermaid"><code>flowchart LR
    Est[Estimate Mean Reward] --&gt; Bonus[Add Confidence Bonus]
    Bonus --&gt; UCB[UCB Score]
    UCB --&gt; Select[Choose Best Arm]</code></pre>
<hr />
<h2 id="thompson-sampling">📌 Thompson Sampling<a class="headerlink" href="#thompson-sampling" title="Permanent link">&para;</a></h2>
<p>Bayesian method: sample parameters from posterior and act greedily.</p>
<p>Steps:
1. Assume prior distribution <span class="arithmatex">\(p(\theta)\)</span>.
2. Update posterior <span class="arithmatex">\(p(\theta|D_t)\)</span> using Bayes’ theorem.
3. Sample parameter from posterior.
4. Choose action maximizing expected reward.</p>
<p>For Bernoulli rewards:
- Prior: Beta(<span class="arithmatex">\(\alpha, \beta\)</span>).
- Posterior after <span class="arithmatex">\(N\)</span> trials:
$$
p(\theta|D_t) = Beta(\alpha + \text{successes}, \beta + \text{failures})
$$</p>
<pre class="mermaid"><code>flowchart TD
    Prior["Prior Beta(α,β)"] --&gt; Posterior["Update with Data"]
    Posterior --&gt; Sample["Sample θ from Posterior"]
    Sample --&gt; Action["Choose Arm with Max θ"]
</code></pre>
<hr />
<h2 id="contextual-bandits">📌 Contextual Bandits<a class="headerlink" href="#contextual-bandits" title="Permanent link">&para;</a></h2>
<p>Unlike MAB, <strong>contextual bandits</strong> incorporate <strong>features of states and actions</strong>.</p>
<ul>
<li>State (context): user demographics, time, history.</li>
<li>Action: item/arm to show.</li>
<li>Reward: observed click/purchase.</li>
</ul>
<p>Tuple: <span class="arithmatex">\((S,A,R)\)</span></p>
<p>At each step:
1. Environment provides context <span class="arithmatex">\(s_t\)</span>.
2. Agent selects action <span class="arithmatex">\(a_t\)</span>.
3. Environment returns reward <span class="arithmatex">\(r_t\)</span>.</p>
<p><strong>Objective:</strong>
$$
\max \sum_{t=1}^T E[r_t | s_t, a_t]
$$</p>
<pre class="mermaid"><code>flowchart LR
    Context[Context Features] --&gt; Agent
    Agent --&gt;|Action a_t| Item[Chosen Item]
    Item --&gt; Reward[r_t]</code></pre>
<hr />
<h2 id="contextual-bandit-algorithms">📌 Contextual Bandit Algorithms<a class="headerlink" href="#contextual-bandit-algorithms" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>ε-greedy with context:</strong> Use predictive model <span class="arithmatex">\(f(s,a)\)</span> for reward estimation.</li>
<li><strong>Linear UCB:</strong>
  $$ q_\theta(s,a) = \phi(s,a)^T \theta $$</li>
<li><strong>Contextual Thompson Sampling:</strong> Posterior inference over contextual parameters.</li>
</ul>
<hr />
<h2 id="summary-of-contextual-bandits">📌 Summary of Contextual Bandits<a class="headerlink" href="#summary-of-contextual-bandits" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Strengths</th>
<th>Weaknesses</th>
</tr>
</thead>
<tbody>
<tr>
<td>ε-Greedy</td>
<td>Easy, simple</td>
<td>Sensitive to ε tuning</td>
</tr>
<tr>
<td>UCB</td>
<td>Theoretically strong</td>
<td>Computationally heavy</td>
</tr>
<tr>
<td>Thompson Sampling</td>
<td>Handles uncertainty</td>
<td>Posterior inference hard</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="big-picture">📌 Big Picture<a class="headerlink" href="#big-picture" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Multi-armed bandits:</strong> No context.</li>
<li><strong>Contextual bandits:</strong> Context-aware recommendations.</li>
<li><strong>Full RL (MDP):</strong> Sequential, long-term decision-making.</li>
</ul>
<pre class="mermaid"><code>flowchart LR
    Bandit[Multi-Armed Bandit] --&gt; Contextual[Contextual Bandit]
    Contextual --&gt; RL[Full Reinforcement Learning]</code></pre>
<p>Reinforcement Learning spans from <strong>simple bandits</strong> to <strong>complex long-horizon planning problems</strong> like robotics, healthcare, and recommendation systems.</p>
<hr />







  
    
  
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="August 29, 2025 05:05:01 UTC">August 29, 2025</span>
  </span>

    
    
    
    
  </aside>





                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../lec5/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Sequential Learning">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Sequential Learning
              </div>
            </div>
          </a>
        
        
          
          <a href="../../cheatsheets/" class="md-footer__link md-footer__link--next" aria-label="Next: Cheatsheets">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Cheatsheets
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="https://github.com/adityachauhan0" target="_blank" rel="noopener" title="GitHub" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.instant", "navigation.tracking", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.footer", "toc.integrate", "toc.follow", "search.suggest", "search.highlight", "search.share", "content.code.copy", "content.code.annotate", "content.tooltips", "header.autohide", "content.action.edit", "content.action.view"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.92b07e13.min.js"></script>
      
        <script src="https://unpkg.com/mermaid@10/dist/mermaid.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
      
        <script src="../../js/katex-init.js"></script>
      
    
  </body>
</html>