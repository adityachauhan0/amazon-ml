
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="My ML notes, cheatsheets, and experiments">
      
      
      
        <link rel="canonical" href="https://adityachauhan0.github.io/amazon-ml/notes/lec6/">
      
      
        <link rel="prev" href="../lec5/">
      
      
        <link rel="next" href="../lec8/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.17">
    
    
      
        <title>Reinforcement Learning - Machine Learning Notes</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.7e37652d.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="pink">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#reinforcement-learning-rl-notes" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Machine Learning Notes" class="md-header__button md-logo" aria-label="Machine Learning Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Machine Learning Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Reinforcement Learning
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="pink"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="pink"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="pink"  aria-label="Switch to high contrast"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to high contrast" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 20c-2.21 0-4.21-.9-5.66-2.34L17.66 6.34A8.01 8.01 0 0 1 20 12a8 8 0 0 1-8 8M6 8h2V6h1.5v2h2v1.5h-2v2H8v-2H6M12 2A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10A10 10 0 0 0 12 2m0 14h5v-1.5h-5z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/adityachauhan0/amazon-ml" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    adityachauhan0/amazon-ml
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../lecture3/" class="md-tabs__link">
          
  
  
    
  
  Notes

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../cheatsheets/" class="md-tabs__link">
        
  
  
    
  
  Cheatsheets

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Machine Learning Notes" class="md-nav__button md-logo" aria-label="Machine Learning Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Machine Learning Notes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/adityachauhan0/amazon-ml" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    adityachauhan0/amazon-ml
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Notes
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Notes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lecture3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dimensionality Reduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lecture4/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Unsupervised Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lec5/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sequential Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#what-is-reinforcement-learning-rl" class="md-nav__link">
    <span class="md-ellipsis">
      📌 What is Reinforcement Learning (RL)?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#difference-between-supervised-learning-and-rl" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Difference Between Supervised Learning and RL
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#agentenvironment-loop" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Agent–Environment Loop
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#markov-decision-process-mdp" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Markov Decision Process (MDP)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#discount-factor-gamma" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Discount Factor (\(\gamma\))
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#exploration-vs-exploitation-dilemma" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Exploration vs Exploitation Dilemma
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multi-armed-bandit-mab-problem" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Multi-Armed Bandit (MAB) Problem
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regret-in-bandits" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Regret in Bandits
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#epsilon-greedy-strategy" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Epsilon-Greedy Strategy
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#upper-confidence-bound-ucb" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Upper Confidence Bound (UCB)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#thompson-sampling" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Thompson Sampling
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contextual-bandits" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Contextual Bandits
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contextual-bandit-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Contextual Bandit Algorithms
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary-of-contextual-bandits" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Summary of Contextual Bandits
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#big-picture" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Big Picture
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lec8/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Causal Inference
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cheatsheets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Cheatsheets
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/adityachauhan0/amazon-ml/edit/main/docs/notes/lec6.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/adityachauhan0/amazon-ml/raw/main/docs/notes/lec6.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


<h1 id="reinforcement-learning-rl-notes">Reinforcement Learning (RL) Notes<a class="headerlink" href="#reinforcement-learning-rl-notes" title="Permanent link">&para;</a></h1>
<h2 id="what-is-reinforcement-learning-rl">📌 What is Reinforcement Learning (RL)?<a class="headerlink" href="#what-is-reinforcement-learning-rl" title="Permanent link">&para;</a></h2>
<p>Reinforcement Learning (RL) is a <strong>computational framework for sequential decision making</strong>. The central idea is that an <strong>agent</strong> learns to interact with an <strong>environment</strong> in order to maximize some notion of <strong>cumulative reward</strong>.</p>
<ul>
<li><strong>Key Characteristics:</strong></li>
<li><strong>Sequential:</strong> Decisions happen over time, not independently.</li>
<li><strong>Trial-and-error learning:</strong> The agent must try actions and learn from feedback.</li>
<li><strong>Delayed consequences:</strong> Rewards may not be immediate, requiring planning.</li>
<li><strong>Exploration vs exploitation:</strong> Balance between trying new actions vs. leveraging known good ones.</li>
</ul>
<pre class="mermaid"><code>flowchart TD
    subgraph RL [Reinforcement Learning Framework]
        A[Agent] -- Action/Control --&gt; E[Environment]
        E -- State/Observation --&gt; A
        E -- Reward/Cost --&gt; A
    end</code></pre>
<hr />
<h2 id="difference-between-supervised-learning-and-rl">📌 Difference Between Supervised Learning and RL<a class="headerlink" href="#difference-between-supervised-learning-and-rl" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Supervised Learning</th>
<th>Reinforcement Learning</th>
</tr>
</thead>
<tbody>
<tr>
<td>Training Data</td>
<td>Pre-collected, labeled</td>
<td>No initial dataset; learns through interaction</td>
</tr>
<tr>
<td>Labels</td>
<td>Always known</td>
<td>Rewards are <strong>unknown</strong>, possibly stochastic</td>
</tr>
<tr>
<td>Assumptions</td>
<td>IID data</td>
<td>Data is sequential, non-IID</td>
</tr>
<tr>
<td>Goal</td>
<td>Minimize prediction error</td>
<td>Maximize cumulative reward over time</td>
</tr>
<tr>
<td>Example</td>
<td>Spam filtering, regression</td>
<td>Robot navigation, recommendation systems</td>
</tr>
</tbody>
</table>
<pre class="mermaid"><code>flowchart LR
    SL[Supervised Learning] --&gt;|Pre-collected Data| Model
    RL[Reinforcement Learning] --&gt;|Interaction| Env
    Env --&gt;|Reward/Next State| RL</code></pre>
<hr />
<h2 id="agentenvironment-loop">📌 Agent–Environment Loop<a class="headerlink" href="#agentenvironment-loop" title="Permanent link">&para;</a></h2>
<p>At each discrete time step <span class="arithmatex">\(t\)</span>:
1. Agent observes <strong>state</strong> <span class="arithmatex">\(S_t\)</span>.
2. Agent selects an <strong>action</strong> <span class="arithmatex">\(A_t\)</span>.
3. Environment responds with a <strong>reward</strong> <span class="arithmatex">\(R_t\)</span> and a new <strong>state</strong> <span class="arithmatex">\(S_{t+1}\)</span>.</p>
<pre class="mermaid"><code>sequenceDiagram
    participant Agent
    participant Environment

    Agent-&gt;&gt;Environment: Selects Action $A_t$
    Environment--&gt;&gt;Agent: Returns Reward $R_t$
    Environment--&gt;&gt;Agent: Provides Next State $S_{t+1}$</code></pre>
<p>Mathematically:
$$
S_t \xrightarrow{A_t} S_{t+1}, \quad R_t = R(S_t, A_t)
$$</p>
<hr />
<h2 id="markov-decision-process-mdp">📌 Markov Decision Process (MDP)<a class="headerlink" href="#markov-decision-process-mdp" title="Permanent link">&para;</a></h2>
<p>An MDP provides the <strong>mathematical foundation</strong> for RL. Defined as a tuple:
$$
(S, A, R, P, \gamma)
$$</p>
<ul>
<li><strong>States (S):</strong> The set of all possible configurations of the environment.</li>
<li><strong>Actions (A):</strong> The set of possible moves the agent can make.</li>
<li><strong>Reward function (R):</strong> Maps <span class="arithmatex">\((s, a)\)</span> to a scalar signal.</li>
<li><strong>Transition probabilities (P):</strong>
  $$ P_a(s,s') = Pr(S_{t+1}=s'|S_t=s, A_t=a) $$</li>
<li><strong>Discount factor (<span class="arithmatex">\(\gamma\)</span>):</strong> Weighs immediate vs. future rewards.</li>
</ul>
<blockquote>
<p><strong>Objective:</strong>
Maximize the expected discounted return:
$$
G_t = E\left[ \sum_{k=0}^\infty \gamma^k R_{t+k+1} \right]
$$</p>
</blockquote>
<pre class="mermaid"><code>flowchart LR
    S0((S0)) --&gt;|A0| S1((S1))
    S1 --&gt;|A1| S2((S2))
    S2 --&gt;|A2| S3((S3))
    S3 --&gt;|...| S4((...))</code></pre>
<hr />
<h2 id="discount-factor-gamma">📌 Discount Factor (<span class="arithmatex">\(\gamma\)</span>)<a class="headerlink" href="#discount-factor-gamma" title="Permanent link">&para;</a></h2>
<p>The <strong>discount factor</strong> <span class="arithmatex">\(\gamma \in [0,1]\)</span> represents how much future rewards are valued relative to immediate ones.</p>
<ul>
<li>If <span class="arithmatex">\(\gamma = 0\)</span> → Only immediate rewards matter.</li>
<li>If <span class="arithmatex">\(\gamma = 1\)</span> → Future rewards are valued equally as present ones.</li>
<li>Typically: <span class="arithmatex">\(0 &lt; \gamma &lt; 1\)</span> for balance.</li>
</ul>
<p>Example:
- Predicting stock prices 5 years from now is uncertain → lower weight.
- Receiving $10 today vs. $10 after 5 years → prefer immediate.</p>
<div class="arithmatex">\[
V(s) = E\left[ R_0 + \gamma R_1 + \gamma^2 R_2 + \dots \mid S_0=s \right]
\]</div>
<pre class="mermaid"><code>flowchart TD
    Now[Immediate Reward] -. High Weight .-&gt; Utility
    Future[Future Reward] -. Discounted by γ .-&gt; Utility</code></pre>
<hr />
<h2 id="exploration-vs-exploitation-dilemma">📌 Exploration vs Exploitation Dilemma<a class="headerlink" href="#exploration-vs-exploitation-dilemma" title="Permanent link">&para;</a></h2>
<p>The agent must <strong>explore</strong> new actions to learn but also <strong>exploit</strong> known good ones to maximize reward.</p>
<ul>
<li><strong>Exploration:</strong> Try less-known actions → gather information.</li>
<li><strong>Exploitation:</strong> Choose best-known action → maximize immediate reward.</li>
</ul>
<p>Examples:
- Restaurant choice: try new vs. go to favorite.
- Music: listen to fav vs. discover new.</p>
<pre class="mermaid"><code>flowchart TD
    Start --&gt;|Exploration| TryNew[Try a New Action]
    Start --&gt;|Exploitation| Best[Choose Best Known Action]</code></pre>
<hr />
<h2 id="multi-armed-bandit-mab-problem">📌 Multi-Armed Bandit (MAB) Problem<a class="headerlink" href="#multi-armed-bandit-mab-problem" title="Permanent link">&para;</a></h2>
<p>Simplest RL setting:
- One state only.
- Multiple actions (arms).
- Each arm yields a random reward.</p>
<ul>
<li><strong>Tuple:</strong> <span class="arithmatex">\((A, R)\)</span></li>
<li><strong>Actions (A):</strong> Finite set of arms <span class="arithmatex">\(\{a_1, a_2, ..., a_m\}\)</span></li>
<li><strong>Rewards (R):</strong> Unknown distributions.</li>
</ul>
<p>At each step <span class="arithmatex">\(t\)</span>:
- Agent picks <span class="arithmatex">\(A_t\)</span>.
- Environment provides <span class="arithmatex">\(R_t\)</span>.</p>
<p><strong>Goal:</strong>
$$
\max E \left[ \sum_{t=1}^T R_t \right]
$$</p>
<pre class="mermaid"><code>flowchart LR
    Agent --&gt;|Pull Arm| Slot[Slot Machine Arms]
    Slot --&gt;|Reward| Agent</code></pre>
<hr />
<h2 id="regret-in-bandits">📌 Regret in Bandits<a class="headerlink" href="#regret-in-bandits" title="Permanent link">&para;</a></h2>
<p>Regret quantifies <strong>how much reward was lost compared to the optimal policy</strong>.</p>
<ul>
<li>Mean reward of action <span class="arithmatex">\(A\)</span>:
$$ V_A = E_{R \sim R_A}[R|A] $$</li>
<li>Optimal action:
$$ A^* = \arg\max_A V_A $$</li>
<li>Regret after <span class="arithmatex">\(T\)</span> rounds:
$$ Regret(T) = T \cdot V_{A^*} - \sum_{t=1}^T V_{A_t} $$</li>
</ul>
<pre class="mermaid"><code>flowchart LR
    Opt[Optimal Arm A*] --&gt; HighReward
    Agent[Chosen Arms] --&gt; ActualReward
    HighReward -. difference .-&gt; Regret</code></pre>
<hr />
<h2 id="epsilon-greedy-strategy">📌 Epsilon-Greedy Strategy<a class="headerlink" href="#epsilon-greedy-strategy" title="Permanent link">&para;</a></h2>
<ul>
<li>With probability <span class="arithmatex">\(\epsilon\)</span>: pick a random arm (exploration).</li>
<li>With probability <span class="arithmatex">\(1-\epsilon\)</span>: pick the best-known arm (exploitation).</li>
</ul>
<pre class="mermaid"><code>flowchart TD
    Start --&gt;|ε| Rand[Choose Random Arm]
    Start --&gt;|1-ε| Best[Choose Best Arm]
    Rand --&gt; Arms
    Best --&gt; Arms
    Arms[Reward Observed]</code></pre>
<p>Variants:
- <strong>ε-first:</strong> Explore first few rounds, then exploit.
- <strong>ε-decreasing:</strong> Reduce <span class="arithmatex">\(ε\)</span> over time.</p>
<hr />
<h2 id="upper-confidence-bound-ucb">📌 Upper Confidence Bound (UCB)<a class="headerlink" href="#upper-confidence-bound-ucb" title="Permanent link">&para;</a></h2>
<p>Balances exploration and exploitation via confidence intervals.</p>
<ul>
<li>Estimate mean reward <span class="arithmatex">\(\bar V_a\)</span>.</li>
<li>Add exploration bonus:
$$
UCB_a(t) = \bar V_a + \sqrt{\frac{2 \ln t}{N_a(t)}}
$$</li>
</ul>
<pre class="mermaid"><code>flowchart LR
    Est[Estimate Mean Reward] --&gt; Bonus[Add Confidence Bonus]
    Bonus --&gt; UCB[UCB Score]
    UCB --&gt; Select[Choose Best Arm]</code></pre>
<hr />
<h2 id="thompson-sampling">📌 Thompson Sampling<a class="headerlink" href="#thompson-sampling" title="Permanent link">&para;</a></h2>
<p>Bayesian method: sample parameters from posterior and act greedily.</p>
<p>Steps:
1. Assume prior distribution <span class="arithmatex">\(p(\theta)\)</span>.
2. Update posterior <span class="arithmatex">\(p(\theta|D_t)\)</span> using Bayes’ theorem.
3. Sample parameter from posterior.
4. Choose action maximizing expected reward.</p>
<p>For Bernoulli rewards:
- Prior: Beta(<span class="arithmatex">\(\alpha, \beta\)</span>).
- Posterior after <span class="arithmatex">\(N\)</span> trials:
$$
p(\theta|D_t) = Beta(\alpha + \text{successes}, \beta + \text{failures})
$$</p>
<pre class="mermaid"><code>flowchart TD
    Prior["Prior Beta(α,β)"] --&gt; Posterior["Update with Data"]
    Posterior --&gt; Sample["Sample θ from Posterior"]
    Sample --&gt; Action["Choose Arm with Max θ"]
</code></pre>
<hr />
<h2 id="contextual-bandits">📌 Contextual Bandits<a class="headerlink" href="#contextual-bandits" title="Permanent link">&para;</a></h2>
<p>Unlike MAB, <strong>contextual bandits</strong> incorporate <strong>features of states and actions</strong>.</p>
<ul>
<li>State (context): user demographics, time, history.</li>
<li>Action: item/arm to show.</li>
<li>Reward: observed click/purchase.</li>
</ul>
<p>Tuple: <span class="arithmatex">\((S,A,R)\)</span></p>
<p>At each step:
1. Environment provides context <span class="arithmatex">\(s_t\)</span>.
2. Agent selects action <span class="arithmatex">\(a_t\)</span>.
3. Environment returns reward <span class="arithmatex">\(r_t\)</span>.</p>
<p><strong>Objective:</strong>
$$
\max \sum_{t=1}^T E[r_t | s_t, a_t]
$$</p>
<pre class="mermaid"><code>flowchart LR
    Context[Context Features] --&gt; Agent
    Agent --&gt;|Action a_t| Item[Chosen Item]
    Item --&gt; Reward[r_t]</code></pre>
<hr />
<h2 id="contextual-bandit-algorithms">📌 Contextual Bandit Algorithms<a class="headerlink" href="#contextual-bandit-algorithms" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>ε-greedy with context:</strong> Use predictive model <span class="arithmatex">\(f(s,a)\)</span> for reward estimation.</li>
<li><strong>Linear UCB:</strong>
  $$ q_\theta(s,a) = \phi(s,a)^T \theta $$</li>
<li><strong>Contextual Thompson Sampling:</strong> Posterior inference over contextual parameters.</li>
</ul>
<hr />
<h2 id="summary-of-contextual-bandits">📌 Summary of Contextual Bandits<a class="headerlink" href="#summary-of-contextual-bandits" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Strengths</th>
<th>Weaknesses</th>
</tr>
</thead>
<tbody>
<tr>
<td>ε-Greedy</td>
<td>Easy, simple</td>
<td>Sensitive to ε tuning</td>
</tr>
<tr>
<td>UCB</td>
<td>Theoretically strong</td>
<td>Computationally heavy</td>
</tr>
<tr>
<td>Thompson Sampling</td>
<td>Handles uncertainty</td>
<td>Posterior inference hard</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="big-picture">📌 Big Picture<a class="headerlink" href="#big-picture" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Multi-armed bandits:</strong> No context.</li>
<li><strong>Contextual bandits:</strong> Context-aware recommendations.</li>
<li><strong>Full RL (MDP):</strong> Sequential, long-term decision-making.</li>
</ul>
<pre class="mermaid"><code>flowchart LR
    Bandit[Multi-Armed Bandit] --&gt; Contextual[Contextual Bandit]
    Contextual --&gt; RL[Full Reinforcement Learning]</code></pre>
<p>Reinforcement Learning spans from <strong>simple bandits</strong> to <strong>complex long-horizon planning problems</strong> like robotics, healthcare, and recommendation systems.</p>
<hr />
<h1 id="reinforcement-learning-advanced-nodetailleftbehind-notes">Reinforcement Learning — Advanced, No‑Detail‑Left‑Behind Notes<a class="headerlink" href="#reinforcement-learning-advanced-nodetailleftbehind-notes" title="Permanent link">&para;</a></h1>
<p>These notes expand your earlier summary with deep, implementation‑ready detail that covers <strong>tabular Q‑learning</strong>, <strong>policy gradients / REINFORCE</strong>, and <strong>Deep Q‑Learning (DQN)</strong> with the practical tricks used in Atari‑style setups.</p>
<hr />
<h2 id="1-qlearning-update-from-bellman-optimality-to-stochastic-updates">1) Q‑Learning Update — From Bellman Optimality to Stochastic Updates<a class="headerlink" href="#1-qlearning-update-from-bellman-optimality-to-stochastic-updates" title="Permanent link">&para;</a></h2>
<h3 id="bellman-optimality-actionvalue-form">Bellman Optimality (action‑value form)<a class="headerlink" href="#bellman-optimality-actionvalue-form" title="Permanent link">&para;</a></h3>
<p>For any MDP with optimal <span class="arithmatex">\(Q^*\)</span>, the fixed point satisfies
$$
Q^<em>(s,a) = \mathbb E\left[ r_{t+1} + \gamma \max_{a'} Q^</em>(s_{t+1},a') \mid s_t=s, a_t=a \right].
$$
The operator <span class="arithmatex">\(\mathcal T^*\)</span> defined by <span class="arithmatex">\((\mathcal T^*Q)(s,a) = \mathbb E\left[ r + \gamma \max_{a'} Q(s',a')\right]\)</span> is a <strong><span class="arithmatex">\(\gamma\)</span>‑contraction</strong> in the sup‑norm; therefore it has a unique fixed point <span class="arithmatex">\(Q^*\)</span>.</p>
<h3 id="samplebased-target-and-td-error">Sample‑based target and TD error<a class="headerlink" href="#samplebased-target-and-td-error" title="Permanent link">&para;</a></h3>
<p>Given a single transition <span class="arithmatex">\((s_t,a_t,r_{t+1},s_{t+1})\)</span>, form a <strong>sample target</strong>
$$
\underbrace{y_t}<em t_1="t+1">{\text{target}} \;=\; r</em>,a')
$$
and the } + \gamma \max_{a'} Q(s_{t+1<strong>TD error</strong>
$$
\delta_t \;=\; y_t - Q(s_t,a_t).
$$</p>
<h3 id="decayed-exponentialmovingaverage-update">Decayed (exponential‑moving‑average) update<a class="headerlink" href="#decayed-exponentialmovingaverage-update" title="Permanent link">&para;</a></h3>
<p>Blend the old estimate with the new target using a learning rate <span class="arithmatex">\(\alpha_t\)</span>:
$$
Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha_t\,\delta_t
\;=\; (1-\alpha_t)\,Q(s_t,a_t) + \alpha_t\,\big(r_{t+1} + \gamma \max_{a'} Q(s_{t+1},a')\big).
$$
This is exactly the final line on your slide rewritten as an <strong>incremental update</strong>.</p>
<h3 id="convergence-tabular-case">Convergence (tabular case)<a class="headerlink" href="#convergence-tabular-case" title="Permanent link">&para;</a></h3>
<p>Under standard conditions (finite MDP, <strong>GLIE</strong> exploration so every <span class="arithmatex">\((s,a)\)</span> is visited infinitely often; step sizes satisfy <span class="arithmatex">\(\sum_t \alpha_t=\infty\)</span>, <span class="arithmatex">\(\sum_t \alpha_t^2&lt;\infty\)</span>), tabular Q‑learning converges to <span class="arithmatex">\(Q^*\)</span> with probability 1. In practice, a <strong>constant</strong> <span class="arithmatex">\(\alpha\)</span> is common; then you trade strict convergence guarantees for faster tracking.</p>
<h3 id="onpolicy-vs-offpolicy-and-sarsa">On‑policy vs Off‑policy and SARSA<a class="headerlink" href="#onpolicy-vs-offpolicy-and-sarsa" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Q‑learning (off‑policy):</strong> learns <span class="arithmatex">\(Q^*\)</span> using the max over <span class="arithmatex">\(a'\)</span> irrespective of the behavior policy (often <span class="arithmatex">\(\epsilon\)</span>‑greedy).</li>
<li><strong>SARSA (on‑policy):</strong> target uses the <strong>actual next action</strong> <span class="arithmatex">\(a_{t+1}\)</span> sampled by the behavior policy:
  <span class="arithmatex">\(y_t = r_{t+1} + \gamma Q(s_{t+1}, a_{t+1})\)</span> — safer under stochastic dynamics or when function approximation is unstable.</li>
</ul>
<h3 id="recovering-the-greedy-policy">Recovering the greedy policy<a class="headerlink" href="#recovering-the-greedy-policy" title="Permanent link">&para;</a></h3>
<p>Given a learned <span class="arithmatex">\(Q\)</span>, the greedy policy is
$$
\pi^<em>(s) \;=\; \arg\max_{a\in\mathcal A} Q(s,a).
$$
During learning, use </em><em><span class="arithmatex">\(\epsilon\)</span>‑greedy</em>* (or softmax) to ensure exploration: pick a random action w.p. <span class="arithmatex">\(\epsilon\)</span>, otherwise the argmax.</p>
<h3 id="practical-hyperparameters-tabular">Practical hyperparameters (tabular)<a class="headerlink" href="#practical-hyperparameters-tabular" title="Permanent link">&para;</a></h3>
<ul>
<li><span class="arithmatex">\(\epsilon\)</span> schedule: start high (0.9–1.0), <strong>linearly decay</strong> to 0.05–0.1; or cosine/exp schedules.</li>
<li>Learning rate: <span class="arithmatex">\(\alpha\in[0.05,0.5]\)</span> depending on reward scale.</li>
<li>Discount: <span class="arithmatex">\(\gamma\in[0.95,0.999]\)</span> for continuing tasks; smaller if horizons are short.</li>
</ul>
<h3 id="common-failure-modes">Common failure modes<a class="headerlink" href="#common-failure-modes" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Insufficient exploration:</strong> Q values overfit early experience.</li>
<li><strong>Non‑stationarity:</strong> changing dynamics reward requires non‑decaying or adaptive <span class="arithmatex">\(\alpha\)</span>.</li>
<li><strong>Deadly triad</strong> (with function approximation + bootstrapping + off‑policy): divergence. Use target networks, double estimators, or on‑policy control.</li>
</ul>
<hr />
<h2 id="2-policybased-rl-reinforce-the-policy-gradient-theorem">2) Policy‑based RL (REINFORCE &amp; the Policy Gradient Theorem)<a class="headerlink" href="#2-policybased-rl-reinforce-the-policy-gradient-theorem" title="Permanent link">&para;</a></h2>
<h3 id="episodic-objective">Episodic objective<a class="headerlink" href="#episodic-objective" title="Permanent link">&para;</a></h3>
<p>Let a trajectory be <span class="arithmatex">\(\tau = (s_0,a_0,r_1,\dots,s_T)\)</span> generated by a differentiable policy <span class="arithmatex">\(\pi_\theta(a\mid s)\)</span>. Define the expected return of an episode
$$
J(\pi_\theta) = \mathbb E_{\tau\sim p_\theta(\tau)}[R(\tau)],\quad R(\tau)=\sum_{t=0}^{T-1} r_{t+1}.
$$
The trajectory density factorizes as
$$
p_\theta(\tau) = p(s_0)\prod_{t=0}^{T-1} \pi_\theta(a_t\mid s_t)\,p(s_{t+1}\mid s_t,a_t).
$$</p>
<h3 id="policy-gradient-theorem-logderivative-trick">Policy Gradient Theorem (log‑derivative trick)<a class="headerlink" href="#policy-gradient-theorem-logderivative-trick" title="Permanent link">&para;</a></h3>
<p>Bring the gradient under the integral and use <span class="arithmatex">\(\nabla_\theta \log \pi_\theta\)</span>:
$$
\nabla_\theta J(\pi_\theta)
= \mathbb E_{\tau}\Bigg[\sum_{t=0}^{T-1} \nabla_\theta\log\pi_\theta(a_t\mid s_t)\, G_t\Bigg],
$$
where <span class="arithmatex">\(G_t=\sum_{k=t}^{T-1} r_{k+1}\)</span> is the <strong>return‑to‑go</strong>. This result holds because <span class="arithmatex">\(\nabla_\theta \log p(s_{t+1}\mid s_t,a_t)=0\)</span> (dynamics do not depend on <span class="arithmatex">\(\theta\)</span> in model‑free settings).</p>
<h3 id="reinforce-algorithm-monte-carlo-policy-gradient">REINFORCE algorithm (Monte Carlo policy gradient)<a class="headerlink" href="#reinforce-algorithm-monte-carlo-policy-gradient" title="Permanent link">&para;</a></h3>
<p><strong>Vanilla form (without baseline):</strong>
1. Roll out episodes with current <span class="arithmatex">\(\theta\)</span>.
2. For each time step, compute <span class="arithmatex">\(G_t\)</span>.
3. Ascend the gradient: <span class="arithmatex">\(\theta \leftarrow \theta + \alpha\,\sum_t \nabla_\theta\log\pi_\theta(a_t\mid s_t)\,G_t\)</span>.</p>
<p><strong>With a baseline (variance reduction):</strong> replace <span class="arithmatex">\(G_t\)</span> by <strong>advantage</strong> <span class="arithmatex">\(A_t = G_t - b(s_t)\)</span>; unbiased if <span class="arithmatex">\(b\)</span> does not depend on <span class="arithmatex">\(a\)</span>. Common <span class="arithmatex">\(b\)</span>: state‑value <span class="arithmatex">\(V_\phi(s_t)\)</span> learned by regression. Update becomes
$$
\theta \leftarrow \theta + \alpha\,\sum_t \nabla_\theta\log\pi_\theta(a_t\mid s_t)\,\underbrace{(G_t - V_\phi(s_t))}_{A_t}.
$$</p>
<h3 id="practical-improvements">Practical improvements<a class="headerlink" href="#practical-improvements" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Reward‑to‑go:</strong> use <span class="arithmatex">\(G_t\)</span> not full <span class="arithmatex">\(R(\tau)\)</span> to reduce variance.</li>
<li><strong>Normalize advantages</strong> within a batch (zero mean / unit variance).</li>
<li><strong>Entropy regularization:</strong> add <span class="arithmatex">\(+\beta\,\mathbb E[\mathcal H(\pi_\theta(\cdot\mid s))]\)</span> to encourage exploration; equivalent to a soft‑maximization of reward.</li>
<li><strong>Generalized Advantage Estimation (GAE):</strong> exponentially‑weighted TD(<span class="arithmatex">\(\lambda\)</span>) advantages: <span class="arithmatex">\(A_t^{\text{GAE}(\lambda)}\)</span> balances bias/variance.</li>
<li><strong>Trust regions / clipping:</strong> TRPO/PPO stabilize large updates; PPO’s clipped surrogate is widely used in practice.</li>
</ul>
<h3 id="strengths-vs-weaknesses-policy-gradients">Strengths vs weaknesses (policy gradients)<a class="headerlink" href="#strengths-vs-weaknesses-policy-gradients" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Pros:</strong> works in <strong>continuous action spaces</strong>, directly optimizes stochastic policies, easy to combine with differentiable nets.</li>
<li><strong>Cons:</strong> high variance, sample‑inefficient, typically on‑policy; mitigated by baselines, variance reduction, and richer critics (actor‑critic).</li>
</ul>
<hr />
<h2 id="3-qlearning-vs-reinforce-biasvariance-onoff-policy">3) Q‑Learning vs REINFORCE — Bias/Variance, On/Off Policy<a class="headerlink" href="#3-qlearning-vs-reinforce-biasvariance-onoff-policy" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Q‑Learning</th>
<th>REINFORCE</th>
</tr>
</thead>
<tbody>
<tr>
<td>Learning type</td>
<td>Value‑based, bootstrapped</td>
<td>Policy‑based, Monte‑Carlo</td>
</tr>
<tr>
<td>Policy</td>
<td>Greedy/<span class="arithmatex">\(\epsilon\)</span>-greedy derived from <span class="arithmatex">\(Q\)</span></td>
<td>Directly parameterized <span class="arithmatex">\(\pi_\theta\)</span></td>
</tr>
<tr>
<td>On/Off policy</td>
<td><strong>Off‑policy</strong> (can learn from any behavior)</td>
<td><strong>On‑policy</strong> (data must match current policy)</td>
</tr>
<tr>
<td>Bias/Variance</td>
<td><strong>High bias</strong> (bootstrapping) but low variance</td>
<td><strong>Unbiased</strong> gradient; <strong>high variance</strong></td>
</tr>
<tr>
<td>Action spaces</td>
<td>Discrete (tabular/approx.)</td>
<td>Naturally handles <strong>continuous</strong> actions</td>
</tr>
<tr>
<td>Sample efficiency</td>
<td>Good with replay</td>
<td>Poor without replay; PPO/TRPO/A2C improve</td>
</tr>
<tr>
<td>Stability</td>
<td>Sensitive with function approx. (deadly triad)</td>
<td>Sensitive to step size; stabilized by baselines/critics</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="4-deep-qlearning-dqn-why-deep-learning">4) Deep Q‑Learning (DQN) — Why Deep Learning?<a class="headerlink" href="#4-deep-qlearning-dqn-why-deep-learning" title="Permanent link">&para;</a></h2>
<h3 id="curse-of-dimensionality-from-pixels">Curse of dimensionality from pixels<a class="headerlink" href="#curse-of-dimensionality-from-pixels" title="Permanent link">&para;</a></h3>
<p>If a state is a stack of the <strong>last 4 grayscale frames</strong> at <strong>84×84</strong> resolution (256 intensities), the raw state space size is roughly <span class="arithmatex">\(256^{84\times84\times4} \approx 10^{67970}\)</span> — tabular methods are impossible. We approximate <span class="arithmatex">\(Q(s,a)\)</span> with a <strong>convolutional neural network</strong>.</p>
<h3 id="qnetwork-parameterization">Q‑Network parameterization<a class="headerlink" href="#qnetwork-parameterization" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Single‑head form:</strong> one forward pass outputs a vector <span class="arithmatex">\(Q_\mathbf w(s,\cdot)\in\mathbb R^{|\mathcal A|}\)</span>; the entry for index <span class="arithmatex">\(a\)</span> is <span class="arithmatex">\(Q(s,a;\mathbf w)\)</span>.</li>
<li>Train by minimizing a <strong>temporal‑difference regression loss</strong> over mini‑batches of replayed transitions.</li>
</ul>
<h3 id="experience-replay-break-temporal-correlations">Experience Replay (break temporal correlations)<a class="headerlink" href="#experience-replay-break-temporal-correlations" title="Permanent link">&para;</a></h3>
<p>Maintain a buffer <span class="arithmatex">\(\mathcal D\)</span> of tuples <span class="arithmatex">\((s,a,r,s',\text{done})\)</span>. Periodically sample i.i.d. minibatches to compute the TD loss
$$
\mathcal L(\mathbf w) = \mathbb E_{(s,a,r,s')\sim\mathcal D} \Big[\;\underbrace{\big(r + \gamma\,\mathbf 1_{\neg\text{done}}\,\max_{a'} Q_{\mathbf w^-}(s',a')\; -\; Q_{\mathbf w}(s,a)\big)^2}_{\text{TD error squared (or Huber)}}\;\Big].
$$
Here <span class="arithmatex">\(\mathbf w^-\)</span> are <strong>target network</strong> parameters, copied from <span class="arithmatex">\(\mathbf w\)</span> every <span class="arithmatex">\(C\)</span> steps (or updated by Polyak averaging). Targets computed with <span class="arithmatex">\(\mathbf w^-\)</span> stabilize training.</p>
<h3 id="dqn-algorithm-pseudocode">DQN Algorithm (pseudocode)<a class="headerlink" href="#dqn-algorithm-pseudocode" title="Permanent link">&para;</a></h3>
<div class="language-text highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>Initialize Q-network w with random weights
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>Initialize target network w^- ← w
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>Initialize replay buffer D
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>for episode = 1..M:
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>  s ← env.reset();  ε ← schedule(t)
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>  for t = 1..T:
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    with prob ε choose a random action; else a ← argmax_a Q_w(s,a)
</span><span id="__span-0-8"><a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    s&#39;, r, done ← env.step(a)
</span><span id="__span-0-9"><a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>    D.add(s,a,r,s&#39;,done)
</span><span id="__span-0-10"><a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    if |D| ≥ batch_size:
</span><span id="__span-0-11"><a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>      B ← sample_minibatch(D)
</span><span id="__span-0-12"><a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>      y ← r + γ·1_{¬done}·max_{a&#39;} Q_{w^-}(s&#39;,a&#39;)
</span><span id="__span-0-13"><a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>      L ← Huber(y - Q_w(s,a))
</span><span id="__span-0-14"><a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>      w ← w - η·∇_w L
</span><span id="__span-0-15"><a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>    every C steps: w^- ← w (or soft update)
</span><span id="__span-0-16"><a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>    s ← s&#39;
</span><span id="__span-0-17"><a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a>    if done: break
</span></code></pre></div>
<h3 id="ataristyle-preprocessing-typical-choices">Atari‑style Preprocessing (typical choices)<a class="headerlink" href="#ataristyle-preprocessing-typical-choices" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Frame preprocessing:</strong> convert to grayscale; downsample to <strong>84×84</strong>; <strong>max‑pool</strong> over 2 frames to remove flicker.</li>
<li><strong>Stack the last 4 frames</strong> to encode velocity.</li>
<li><strong>Action repeat / frame skip (e.g., 4)</strong> to reduce computation.</li>
<li><strong>Reward clipping</strong> to <span class="arithmatex">\([-1, 1]\)</span> for scale‑invariance; <strong>gradient clipping</strong> in optimizer.</li>
<li><strong>Exploration schedule:</strong> <span class="arithmatex">\(\epsilon\)</span> linearly decays from 1.0 to 0.1 over 1e6 steps (or 0.01 for evaluation).</li>
<li><strong>Optimizer:</strong> RMSProp or Adam with learning rate around <span class="arithmatex">\(10^{-4}\)</span>–<span class="arithmatex">\(10^{-5}\)</span>; minibatch sizes 32–64.</li>
<li><strong>Target update period <span class="arithmatex">\(C\)</span>:</strong> 10^3–10^4 env steps; replay buffer size 1e5–1e6.</li>
</ul>
<h3 id="key-stability-tricks">Key stability tricks<a class="headerlink" href="#key-stability-tricks" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Target networks</strong> (fixed <span class="arithmatex">\(\mathbf w^-\)</span> for targets).</li>
<li><strong>Huber loss</strong> instead of MSE (less sensitive to outliers):
  <span class="arithmatex">\(\text{Huber}_\kappa(x)=\begin{cases} \tfrac12x^2 &amp; |x|\le\kappa \\ \kappa(|x|-\tfrac12\kappa) &amp; |x|&gt;\kappa.\end{cases}\)</span></li>
<li><strong>Gradient clipping</strong> (e.g., global norm to 10).</li>
<li><strong>Replay buffer warmup</strong> before learning starts.</li>
</ul>
<h3 id="reducing-overestimation-double-dqn">Reducing overestimation: Double DQN<a class="headerlink" href="#reducing-overestimation-double-dqn" title="Permanent link">&para;</a></h3>
<p>Use the <strong>online network</strong> to select the action and the <strong>target network</strong> to evaluate it:
$$
 y = r + \gamma\,Q_{\mathbf w^-}!\Big(s',\arg\max_{a'} Q_{\mathbf w}(s',a')\Big).
$$
This curbs the positive bias from <span class="arithmatex">\(\max\)</span> over noisy estimates.</p>
<h3 id="prioritized-replay">Prioritized Replay<a class="headerlink" href="#prioritized-replay" title="Permanent link">&para;</a></h3>
<p>Sample transitions with probability <span class="arithmatex">\(p_i \propto (|\delta_i| + \epsilon)^\alpha\)</span>; correct the bias with <strong>importance sampling</strong> weights <span class="arithmatex">\(w_i = (N\,p_i)^{-\beta}\)</span> (normalized) inside the loss. Typical <span class="arithmatex">\(\alpha\in[0.5,0.7]\)</span>, <span class="arithmatex">\(\beta\)</span> annealed to 1.0.</p>
<h3 id="dueling-networks">Dueling Networks<a class="headerlink" href="#dueling-networks" title="Permanent link">&para;</a></h3>
<p>Decompose Q into <strong>state value</strong> and <strong>advantage</strong>: <span class="arithmatex">\(Q(s,a)=V(s)+A(s,a)-\tfrac1{|\mathcal A|}\sum_{a'}A(s,a')\)</span>. Helps when many actions share similar value.</p>
<h3 id="multistep-returns-rainbow">Multi‑step returns &amp; Rainbow<a class="headerlink" href="#multistep-returns-rainbow" title="Permanent link">&para;</a></h3>
<p>Use <span class="arithmatex">\(n\)</span>‑step targets: <span class="arithmatex">\(y = \sum_{i=0}^{n-1} \gamma^i r_{t+i+1} + \gamma^n \max_{a'} Q_{\mathbf w^-}(s_{t+n},a')\)</span>. Combine Double DQN, Prioritized Replay, Dueling, Noisy Nets, Distributional RL (Categorical/Quantile) → <strong>Rainbow DQN</strong> gains large performance boosts.</p>
<hr />
<h2 id="5-dqn-in-atari-endtoend-from-pixels">5) DQN in Atari — End‑to‑End from Pixels<a class="headerlink" href="#5-dqn-in-atari-endtoend-from-pixels" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Input:</strong> stack of 4 processed frames <span class="arithmatex">\(\in\mathbb R^{84\times84\times4}\)</span>.</li>
<li><strong>Conv net:</strong> e.g., conv(32,8×8,stride4) → conv(64,4×4,stride2) → conv(64,3×3,stride1) → FC(512) → FC(<span class="arithmatex">\(|\mathcal A|\)</span>).</li>
<li><strong>Output:</strong> one scalar per <strong>discrete joystick/button combination</strong> (≈18 in ALE).</li>
<li><strong>Reward:</strong> per‑step score change; often clipped.</li>
<li><strong>Evaluation:</strong> average over multiple seeds and <strong>sticky actions</strong> for robustness.</li>
</ul>
<p>Implementation notes:
- Maintain separate <strong>training</strong> and <strong>evaluation</strong> <span class="arithmatex">\(\epsilon\)</span>.
- Reset <strong>lives</strong> handling consistently (some works treat loss of life as terminal for training only).
- Monitor: average return, <strong>TD error histogram</strong>, Q‑values magnitude, buffer age, action visitation.</p>
<hr />
<h2 id="6-deep-rl-breakthroughs-context-intuition">6) Deep RL Breakthroughs (context &amp; intuition)<a class="headerlink" href="#6-deep-rl-breakthroughs-context-intuition" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Atari (2013/2015):</strong> A single DQN architecture reached human‑level performance on many Atari 2600 games using only pixels and score as reward. Key ideas: replay, target network, convnets.</li>
<li><strong>AlphaZero (2017):</strong> Self‑play with <strong>Monte Carlo Tree Search</strong> guided by a shared <strong>policy–value</strong> network; <strong>no human data</strong>, exceeded prior programs in chess, shogi, Go.</li>
<li><strong>AlphaStar (2019):</strong> Multi‑agent RL + imitation learning + league training reached Grandmaster level in <strong>StarCraft II</strong> (partial observability, macro/micro decisions, long horizons).</li>
</ul>
<p>These illustrate how <strong>value learning</strong>, <strong>policy learning</strong>, and <strong>planning</strong> can be combined at scale.</p>
<hr />
<h2 id="7-putting-it-all-together-practical-recipes">7) Putting It All Together — Practical Recipes<a class="headerlink" href="#7-putting-it-all-together-practical-recipes" title="Permanent link">&para;</a></h2>
<h3 id="choosing-an-algorithm">Choosing an algorithm<a class="headerlink" href="#choosing-an-algorithm" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Small discrete spaces:</strong> tabular <strong>Q‑learning/SARSA</strong>.</li>
<li><strong>High‑dimensional discrete (pixels):</strong> <strong>DQN</strong> (Double + Dueling + PER + n‑step = Rainbow).</li>
<li><strong>Continuous actions:</strong> <strong>Policy gradients</strong> with actor‑critic: PPO/SAC/TD3 (not covered in slides but essential in practice).</li>
</ul>
<h3 id="hyperparameter-starter-kit-dqn">Hyperparameter starter kit (DQN)<a class="headerlink" href="#hyperparameter-starter-kit-dqn" title="Permanent link">&para;</a></h3>
<ul>
<li>Buffer 1e6; batch 32/64; learning rate <span class="arithmatex">\(1\times10^{-4}\)</span>; target update 1e4 steps; <span class="arithmatex">\(\gamma=0.99\)</span>; warmup 5e4 steps; total env steps 2e7+ for difficult games.</li>
</ul>
<h3 id="debugging-checklist">Debugging checklist<a class="headerlink" href="#debugging-checklist" title="Permanent link">&para;</a></h3>
<ul>
<li>Verify <strong>reward scaling/clipping</strong> and <strong>done</strong> flags.</li>
<li>Ensure <strong>no bootstrapping at terminal</strong> (mask with <span class="arithmatex">\((1-\text{done})\)</span>).</li>
<li>Plot <span class="arithmatex">\(\epsilon\)</span> vs steps, replay age distribution, Q‑value ranges, loss curves.</li>
<li>Sanity check: on a tiny MDP, overfit with a tiny network to confirm learning signal.</li>
</ul>
<hr />
<h2 id="8-mathematics-why-the-updates-work">8) Mathematics: Why the Updates Work<a class="headerlink" href="#8-mathematics-why-the-updates-work" title="Permanent link">&para;</a></h2>
<h3 id="contraction-and-fixed-point">Contraction and fixed point<a class="headerlink" href="#contraction-and-fixed-point" title="Permanent link">&para;</a></h3>
<p>The Bellman optimality operator <span class="arithmatex">\(\mathcal T^*\)</span> is a <span class="arithmatex">\(\gamma\)</span>‑contraction: for any <span class="arithmatex">\(Q_1,Q_2\)</span>,
<span class="arithmatex">\(\|\mathcal T^*Q_1-\mathcal T^*Q_2\|_\infty \le \gamma\,\|Q_1-Q_2\|_\infty\)</span>. Banach’s fixed‑point theorem ⇒ unique <span class="arithmatex">\(Q^*\)</span>.</p>
<h3 id="stochastic-approximation-view">Stochastic Approximation view<a class="headerlink" href="#stochastic-approximation-view" title="Permanent link">&para;</a></h3>
<p>Q‑learning is Robbins–Monro SA tracking the root of <span class="arithmatex">\(\mathbb E[\delta_t\mid s_t,a_t]=0\)</span>. Conditions on <span class="arithmatex">\(\alpha_t\)</span> and visitation ensure almost‑sure convergence in tabular settings.</p>
<h3 id="policy-gradient-derivation-sketch">Policy gradient derivation sketch<a class="headerlink" href="#policy-gradient-derivation-sketch" title="Permanent link">&para;</a></h3>
<p>Use <span class="arithmatex">\(\nabla_\theta J = \int R(\tau)\,\nabla_\theta p_\theta(\tau)\,d\tau = \int R(\tau)\,p_\theta(\tau)\,\nabla_\theta\log p_\theta(\tau)\,d\tau\)</span>. Since <span class="arithmatex">\(\log p_\theta(\tau)\)</span> sums only <span class="arithmatex">\(\log\pi_\theta\)</span> terms, the gradient becomes an expectation of <span class="arithmatex">\(\sum_t \nabla\log\pi_\theta(a_t\mid s_t)\,G_t\)</span>.</p>
<hr />
<h2 id="9-algorithm-boxes-copypaste-friendly">9) Algorithm Boxes (copy‑paste friendly)<a class="headerlink" href="#9-algorithm-boxes-copypaste-friendly" title="Permanent link">&para;</a></h2>
<h3 id="tabular-qlearning-episodic">Tabular Q‑learning (episodic)<a class="headerlink" href="#tabular-qlearning-episodic" title="Permanent link">&para;</a></h3>
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="n">Q</span> <span class="o">=</span> <span class="n">zeros</span><span class="p">(</span><span class="o">|</span><span class="n">S</span><span class="o">|</span><span class="p">,</span> <span class="o">|</span><span class="n">A</span><span class="o">|</span><span class="p">)</span>
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>    <span class="n">s</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
</span><span id="__span-1-5"><a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>        <span class="n">a</span> <span class="o">=</span> <span class="n">ε_greedy</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">])</span>
</span><span id="__span-1-6"><a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>        <span class="n">s2</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</span><span id="__span-1-7"><a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>        <span class="n">td_target</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">γ</span> <span class="o">*</span> <span class="p">(</span><span class="mi">0</span> <span class="k">if</span> <span class="n">done</span> <span class="k">else</span> <span class="nb">max</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">s2</span><span class="p">]))</span>
</span><span id="__span-1-8"><a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a>        <span class="n">td_error</span>  <span class="o">=</span> <span class="n">td_target</span> <span class="o">-</span> <span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span>
</span><span id="__span-1-9"><a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>        <span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span>   <span class="o">+=</span> <span class="n">α</span> <span class="o">*</span> <span class="n">td_error</span>
</span><span id="__span-1-10"><a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>        <span class="n">s</span> <span class="o">=</span> <span class="n">s2</span>
</span><span id="__span-1-11"><a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a>        <span class="k">if</span> <span class="n">done</span><span class="p">:</span> <span class="k">break</span>
</span></code></pre></div>
<h3 id="reinforce-with-baseline">REINFORCE (with baseline)<a class="headerlink" href="#reinforce-with-baseline" title="Permanent link">&para;</a></h3>
<div class="language-python highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="k">for</span> <span class="n">update</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">U</span><span class="p">):</span>
</span><span id="__span-2-2"><a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>    <span class="n">traj</span> <span class="o">=</span> <span class="n">collect_episodes</span><span class="p">(</span><span class="n">π_θ</span><span class="p">)</span>
</span><span id="__span-2-3"><a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>    <span class="n">Gt</span> <span class="o">=</span> <span class="n">returns_to_go</span><span class="p">(</span><span class="n">traj</span><span class="o">.</span><span class="n">rewards</span><span class="p">,</span> <span class="n">γ</span><span class="p">)</span>
</span><span id="__span-2-4"><a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>    <span class="n">b</span>  <span class="o">=</span> <span class="n">V_φ</span><span class="p">(</span><span class="n">traj</span><span class="o">.</span><span class="n">states</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span><span id="__span-2-5"><a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>    <span class="n">adv</span> <span class="o">=</span> <span class="p">(</span><span class="n">Gt</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">normalize</span><span class="p">()</span>
</span><span id="__span-2-6"><a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>    <span class="n">loss_actor</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">logπ_θ</span><span class="p">(</span><span class="n">traj</span><span class="o">.</span><span class="n">actions</span><span class="o">|</span><span class="n">traj</span><span class="o">.</span><span class="n">states</span><span class="p">)</span> <span class="o">*</span> <span class="n">adv</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span><span id="__span-2-7"><a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>    <span class="n">loss_critic</span> <span class="o">=</span> <span class="p">((</span><span class="n">V_φ</span><span class="p">(</span><span class="n">traj</span><span class="o">.</span><span class="n">states</span><span class="p">)</span> <span class="o">-</span> <span class="n">Gt</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span><span id="__span-2-8"><a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a>    <span class="n">optimize</span><span class="p">(</span><span class="n">loss_actor</span> <span class="o">+</span> <span class="n">c_v</span><span class="o">*</span><span class="n">loss_critic</span> <span class="o">-</span> <span class="n">c_ent</span><span class="o">*</span><span class="n">entropy</span><span class="p">(</span><span class="n">π_θ</span><span class="p">))</span>
</span></code></pre></div>
<h3 id="dqn-double-huber-target-net">DQN (Double + Huber + target net)<a class="headerlink" href="#dqn-double-huber-target-net" title="Permanent link">&para;</a></h3>
<div class="language-python highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="n">batch</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
</span><span id="__span-3-2"><a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">,</span><span class="n">r</span><span class="p">,</span><span class="n">s2</span><span class="p">,</span><span class="n">d</span> <span class="o">=</span> <span class="n">batch</span>
</span><span id="__span-3-3"><a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span><span id="__span-3-4"><a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>    <span class="n">a2</span> <span class="o">=</span> <span class="n">Q_w</span><span class="p">(</span><span class="n">s2</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-3-5"><a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a>    <span class="n">y</span>  <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">γ</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">d</span><span class="p">)</span><span class="o">*</span><span class="n">Q_wminus</span><span class="p">(</span><span class="n">s2</span><span class="p">)</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">a2</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-3-6"><a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a><span class="n">qsa</span> <span class="o">=</span> <span class="n">Q_w</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">a</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-3-7"><a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a><span class="n">loss</span> <span class="o">=</span> <span class="n">huber</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">qsa</span><span class="p">)</span>
</span><span id="__span-3-8"><a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a><span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">();</span> <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">();</span> <span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">Q_w</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="mi">10</span><span class="p">);</span> <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></code></pre></div>
<hr />
<h2 id="10-glossary-of-symbols">10) Glossary of Symbols<a class="headerlink" href="#10-glossary-of-symbols" title="Permanent link">&para;</a></h2>
<ul>
<li><span class="arithmatex">\(s_t,a_t,r_{t+1},s_{t+1}\)</span> — state, action, reward, next state at time <span class="arithmatex">\(t\)</span>.</li>
<li><span class="arithmatex">\(\gamma\)</span> — discount factor.</li>
<li><span class="arithmatex">\(Q(s,a)\)</span> — action‑value; <span class="arithmatex">\(V(s)\)</span> — state‑value.</li>
<li><span class="arithmatex">\(\pi_\theta(a\mid s)\)</span> — (stochastic) policy parameterized by <span class="arithmatex">\(\theta\)</span>.</li>
<li><span class="arithmatex">\(G_t\)</span> — return‑to‑go; <span class="arithmatex">\(A_t\)</span> — advantage (e.g., <span class="arithmatex">\(G_t - V(s_t)\)</span>).</li>
<li><span class="arithmatex">\(\mathbf w,\mathbf w^-\)</span> — online and target Q‑network parameters.</li>
<li>PER hyperparams: <span class="arithmatex">\(\alpha\)</span> (priority exponent), <span class="arithmatex">\(\beta\)</span> (IS correction exponent).</li>
</ul>
<hr />
<h2 id="11-extended-topics-quick-references">11) Extended Topics (quick references)<a class="headerlink" href="#11-extended-topics-quick-references" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Expected‑SARSA</strong>: target uses expectation over behavior policy at <span class="arithmatex">\(s'\)</span>; less variance than SARSA.</li>
<li><strong>Distributional RL</strong>: learn <strong>full return distribution</strong> <span class="arithmatex">\(Z(s,a)\)</span> (categorical/quantile). Targets use distributional Bellman operator.</li>
<li><strong>Noisy Nets</strong>: parameterized noise in linear layers for exploration.</li>
<li><strong>n‑step Q‑learning</strong>: blend Monte‑Carlo and bootstrapping; better credit assignment.</li>
<li><strong>Actor‑Critic family</strong> (beyond slides): A2C/A3C (synchronous/asynchronous), <strong>PPO</strong> (clipped surrogate), <strong>SAC/TD3/DDPG</strong> (continuous control, entropy regularization or twin critics).</li>
</ul>
<hr />
<h3 id="closing">Closing<a class="headerlink" href="#closing" title="Permanent link">&para;</a></h3>
<p>You now have slide‑level theory <strong>and</strong> practitioner‑level knobs for Q‑learning, policy gradients, and DQN—from the Bellman equations and policy gradient theorem all the way to Double/Dueling/PR replay and Atari‑grade pipelines. If you want, we can add <strong>worked examples</strong> (gridworld, bandit, CartPole, or Pong) with step‑by‑step calculations and reference hyperparameters.</p>







  
    
  
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="September 2, 2025 09:11:12 UTC">September 2, 2025</span>
  </span>

    
    
    
    
  </aside>





                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../lec5/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Sequential Learning">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Sequential Learning
              </div>
            </div>
          </a>
        
        
          
          <a href="../lec8/" class="md-footer__link md-footer__link--next" aria-label="Next: Causal Inference">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Causal Inference
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="https://github.com/adityachauhan0" target="_blank" rel="noopener" title="GitHub" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.instant", "navigation.tracking", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.footer", "toc.integrate", "toc.follow", "search.suggest", "search.highlight", "search.share", "content.code.copy", "content.code.annotate", "content.tooltips", "header.autohide", "content.action.edit", "content.action.view"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.92b07e13.min.js"></script>
      
        <script src="https://unpkg.com/mermaid@10/dist/mermaid.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
      
        <script src="../../js/katex-init.js"></script>
      
    
  </body>
</html>