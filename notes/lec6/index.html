
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="My ML notes, cheatsheets, and experiments">
      
      
      
        <link rel="canonical" href="https://adityachauhan0.github.io/amazon-ml/notes/lec6/">
      
      
        <link rel="prev" href="../lec5/">
      
      
        <link rel="next" href="../../cheatsheets/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.17">
    
    
      
        <title>Reinforcement Learning - Machine Learning Notes</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.7e37652d.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="pink">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#reinforcement-learning-rl-notes" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Machine Learning Notes" class="md-header__button md-logo" aria-label="Machine Learning Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Machine Learning Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Reinforcement Learning
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="pink"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="pink"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="pink"  aria-label="Switch to high contrast"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to high contrast" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 20c-2.21 0-4.21-.9-5.66-2.34L17.66 6.34A8.01 8.01 0 0 1 20 12a8 8 0 0 1-8 8M6 8h2V6h1.5v2h2v1.5h-2v2H8v-2H6M12 2A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10A10 10 0 0 0 12 2m0 14h5v-1.5h-5z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/adityachauhan0/amazon-ml" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    adityachauhan0/amazon-ml
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../lecture3/" class="md-tabs__link">
          
  
  
    
  
  Notes

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../cheatsheets/" class="md-tabs__link">
        
  
  
    
  
  Cheatsheets

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Machine Learning Notes" class="md-nav__button md-logo" aria-label="Machine Learning Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Machine Learning Notes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/adityachauhan0/amazon-ml" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    adityachauhan0/amazon-ml
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Notes
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Notes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lecture3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dimensionality Reduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lecture4/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Unsupervised Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lec5/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sequential Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#what-is-reinforcement-learning-rl" class="md-nav__link">
    <span class="md-ellipsis">
      📌 What is Reinforcement Learning (RL)?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-makes-rl-different-from-supervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      📌 What Makes RL Different from Supervised Learning?
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#rl-in-action-recommendations" class="md-nav__link">
    <span class="md-ellipsis">
      📌 RL in Action: Recommendations
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#applications-of-rl" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Applications of RL
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#agent-and-environment-interaction" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Agent and Environment Interaction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#markov-decision-process-mdp" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Markov Decision Process (MDP)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#discount-factor-gamma" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Discount Factor (\(\gamma\))
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#exploration-vs-exploitation-dilemma" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Exploration vs Exploitation Dilemma
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multi-armed-bandit-mab-problem" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Multi-Armed Bandit (MAB) Problem
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multi-armed-bandits-and-rl" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Multi-Armed Bandits and RL
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multi-armed-bandit-regret" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Multi-Armed Bandit: Regret
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#epsilon-greedy-strategy" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Epsilon-Greedy Strategy
    </span>
  </a>
  
    <nav class="md-nav" aria-label="📌 Epsilon-Greedy Strategy">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#variants" class="md-nav__link">
    <span class="md-ellipsis">
      Variants:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#upper-confidence-bound-ucb" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Upper Confidence Bound (UCB)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ucb-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      📌 UCB Algorithm
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#thompson-sampling-bayesian-perspective" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Thompson Sampling (Bayesian Perspective)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="📌 Thompson Sampling (Bayesian Perspective)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#thompson-sampling-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      Thompson Sampling Algorithm
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#thompson-sampling-with-bernoulli-rewards" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Thompson Sampling with Bernoulli Rewards
    </span>
  </a>
  
    <nav class="md-nav" aria-label="📌 Thompson Sampling with Bernoulli Rewards">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#prior-distribution" class="md-nav__link">
    <span class="md-ellipsis">
      Prior Distribution:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#posterior-distribution" class="md-nav__link">
    <span class="md-ellipsis">
      Posterior Distribution:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#thompson-sampling-algorithm-pseudo-code" class="md-nav__link">
    <span class="md-ellipsis">
      Thompson Sampling Algorithm (Pseudo-code)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contextual-bandits" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Contextual Bandits
    </span>
  </a>
  
    <nav class="md-nav" aria-label="📌 Contextual Bandits">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#motivation" class="md-nav__link">
    <span class="md-ellipsis">
      Motivation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mathematical-formulation" class="md-nav__link">
    <span class="md-ellipsis">
      Mathematical Formulation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#comparison" class="md-nav__link">
    <span class="md-ellipsis">
      Comparison
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contextual-bandit-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Contextual Bandit Algorithms
    </span>
  </a>
  
    <nav class="md-nav" aria-label="📌 Contextual Bandit Algorithms">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#epsilon-greedy-with-context" class="md-nav__link">
    <span class="md-ellipsis">
      Epsilon-Greedy (with context)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ucb-with-context" class="md-nav__link">
    <span class="md-ellipsis">
      UCB with Context
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#thompson-sampling-with-context" class="md-nav__link">
    <span class="md-ellipsis">
      Thompson Sampling with Context
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#contextual-bandit-summary" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Contextual Bandit Summary
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#big-picture" class="md-nav__link">
    <span class="md-ellipsis">
      📌 Big Picture
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cheatsheets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Cheatsheets
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/adityachauhan0/amazon-ml/edit/main/docs/notes/lec6.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/adityachauhan0/amazon-ml/raw/main/docs/notes/lec6.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


<h1 id="reinforcement-learning-rl-notes">Reinforcement Learning (RL) Notes<a class="headerlink" href="#reinforcement-learning-rl-notes" title="Permanent link">&para;</a></h1>
<h2 id="what-is-reinforcement-learning-rl">📌 What is Reinforcement Learning (RL)?<a class="headerlink" href="#what-is-reinforcement-learning-rl" title="Permanent link">&para;</a></h2>
<p>Reinforcement Learning (RL) is a <strong>sequential decision-making framework</strong> where an <strong>agent</strong> learns to interact with an <strong>environment</strong> to maximize long-term rewards.</p>
<ul>
<li><strong>Key Ideas:</strong></li>
<li>Rewards &amp; punishments drive behavior.</li>
<li>Learning happens through <strong>interaction with the environment</strong>.</li>
<li>Unlike supervised learning, the learner is not told the correct action—it must discover good strategies through <strong>trial and error</strong>.</li>
</ul>
<pre class="mermaid"><code>flowchart LR
    subgraph RL [Reinforcement Learning]
        A[Agent] --&gt;|Action / Response / Control| E[Environment]
        E --&gt;|State / Stimulus / Situation| A
        E --&gt;|Reward / Gain / Payoff / Cost| A
    end</code></pre>
<hr />
<h2 id="what-makes-rl-different-from-supervised-learning">📌 What Makes RL Different from Supervised Learning?<a class="headerlink" href="#what-makes-rl-different-from-supervised-learning" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Supervised Learning</th>
<th>Reinforcement Learning</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Training Data</strong></td>
<td>Available at start</td>
<td>No training data initially</td>
</tr>
<tr>
<td><strong>Labels</strong></td>
<td>Known</td>
<td>Rewards are <strong>unknown</strong> &amp; stochastic</td>
</tr>
<tr>
<td><strong>Setup</strong></td>
<td>IID (independent &amp; identically distributed)</td>
<td>Non-IID (depends on sequence of actions)</td>
</tr>
<tr>
<td><strong>Goal</strong></td>
<td>Maximize prediction accuracy</td>
<td>Maximize <strong>cumulative reward</strong> over time</td>
</tr>
<tr>
<td><strong>Examples</strong></td>
<td>Spam classification, Object detection</td>
<td>Search ranking, Ad recommendations</td>
</tr>
</tbody>
</table>
<blockquote>
<p>⚡ RL is trial-and-error based, with <strong>delayed evaluative feedback</strong>.</p>
</blockquote>
<hr />
<h2 id="rl-in-action-recommendations">📌 RL in Action: Recommendations<a class="headerlink" href="#rl-in-action-recommendations" title="Permanent link">&para;</a></h2>
<p><strong>Problem:</strong> What products to recommend to maximize revenue?</p>
<pre class="mermaid"><code>flowchart LR
    U[Users] --&gt;|See Recommendations| P[Products Displayed]
    P --&gt;|Click / Purchase| R1[Short-Term Reward]
    P --&gt;|Long-Term Engagement| R2[Long-Term Reward]
    P --&gt;|Ignore| N[Shop Elsewhere]</code></pre>
<ul>
<li><strong>Short-term reward</strong> → clicks &amp; purchases.</li>
<li><strong>Long-term reward</strong> → sustained engagement &amp; revenue growth.</li>
</ul>
<hr />
<h2 id="applications-of-rl">📌 Applications of RL<a class="headerlink" href="#applications-of-rl" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Games:</strong> Tesauro’s Backgammon (1995), DeepMind’s Atari (2015), AlphaGo (2015).</li>
<li><strong>Web:</strong> Ad placement, product recommendations.</li>
<li><strong>AI Assistants:</strong> IBM Watson (2011).</li>
<li><strong>Autonomous Systems:</strong> Self-driving cars.</li>
</ul>
<blockquote>
<p>✅ In all these cases, RL <strong>outperformed traditional methods</strong> without explicit human instruction.</p>
</blockquote>
<hr />
<h2 id="agent-and-environment-interaction">📌 Agent and Environment Interaction<a class="headerlink" href="#agent-and-environment-interaction" title="Permanent link">&para;</a></h2>
<p>At each time step <span class="arithmatex">\(t\)</span>:
- <strong>Agent</strong> observes <span class="arithmatex">\(S_t\)</span>, executes <span class="arithmatex">\(A_t\)</span>.
- <strong>Environment</strong> receives <span class="arithmatex">\(A_t\)</span>, emits reward <span class="arithmatex">\(R_t\)</span>, and transitions to new state.</p>
<pre class="mermaid"><code>flowchart LR
    St[State S_t] --&gt; Agent
    Agent --&gt;|Action A_t| Env[Environment]
    Env --&gt;|Reward R_t| Agent
    Env --&gt;|Next State S_{t+1}| Agent</code></pre>
<p>Mathematically:
[
S_t \xrightarrow{A_t} S_{t+1}, \quad R_t = R(S_t, A_t)
]</p>
<hr />
<h2 id="markov-decision-process-mdp">📌 Markov Decision Process (MDP)<a class="headerlink" href="#markov-decision-process-mdp" title="Permanent link">&para;</a></h2>
<p>An MDP is defined as a <strong>5-tuple</strong>:
[(S, A, R, P, \gamma)]</p>
<ul>
<li><strong>States (S):</strong> Knowledge of the environment.</li>
<li><strong>Actions (A):</strong> Choices available to the agent.</li>
<li><strong>Transition Probabilities (P):</strong>
  [ P_a(s,s') = Pr(S_{t+1}=s' \mid S_t=s, A_t=a) ]</li>
<li><strong>Rewards (R):</strong> Reward received after taking action.</li>
<li><strong>Discount Factor (<span class="arithmatex">\(\gamma\)</span>):</strong> Importance of future vs. immediate rewards.</li>
</ul>
<blockquote>
<p><strong>Objective:</strong>
[
\max E \left[ R_0 + \gamma R_1 + \gamma^2 R_2 + \dots \right]
]</p>
</blockquote>
<hr />
<h2 id="discount-factor-gamma">📌 Discount Factor (<span class="arithmatex">\(\gamma\)</span>)<a class="headerlink" href="#discount-factor-gamma" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Uncertainty of future:</strong> e.g., predicting stock prices.</li>
<li><strong>Preference for immediate rewards:</strong> <em>"Be happy now than 5 years later."</em></li>
<li><strong>Mathematical convenience:</strong> Ensures convergence of infinite sums.</li>
</ul>
<div class="arithmatex">\[
V(s) = E\left[ \sum_{t=0}^\infty \gamma^t R_t \mid S_0 = s \right]
\]</div>
<hr />
<h2 id="exploration-vs-exploitation-dilemma">📌 Exploration vs Exploitation Dilemma<a class="headerlink" href="#exploration-vs-exploitation-dilemma" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Exploration:</strong> Try new actions to gather knowledge.</li>
<li><strong>Exploitation:</strong> Use known information to maximize reward.</li>
</ul>
<p>Examples:
- Restaurant selection: Try new vs. go to favorite.
- Music: Listen to fav vs. discover new.
- Ads: Show best-performing vs. test new.</p>
<pre class="mermaid"><code>graph LR
    Start --&gt;|Exploration| New[New Action: More Info]
    Start --&gt;|Exploitation| Known[Known Action: Immediate Reward]</code></pre>
<hr />
<h2 id="multi-armed-bandit-mab-problem">📌 Multi-Armed Bandit (MAB) Problem<a class="headerlink" href="#multi-armed-bandit-mab-problem" title="Permanent link">&para;</a></h2>
<p>The MAB problem is a simplified version of RL:</p>
<ul>
<li><strong>Tuple:</strong> <span class="arithmatex">\((A, R)\)</span></li>
<li><strong>Actions (A):</strong> Finite set of arms <span class="arithmatex">\(\{a_1, a_2, ..., a_m\}\)</span>.</li>
<li><strong>Rewards (R):</strong> Unknown probability distribution for each arm.</li>
</ul>
<p>At each step <span class="arithmatex">\(t = 1,2,...,T\)</span>:
- Agent selects <span class="arithmatex">\(A_t \in A\)</span>.
- Environment provides reward <span class="arithmatex">\(R_t \sim R_{A_t}\)</span>.</p>
<blockquote>
<p>🎯 <strong>Goal:</strong>
[
\max E \left[ \sum_{t=1}^T R_t \right]
]</p>
</blockquote>
<pre class="mermaid"><code>flowchart LR
    Agent --&gt;|Choose Arm A_t| Slot[Slot Machines]
    Slot --&gt;|Reward R_t| Agent</code></pre>
<hr />
<h2 id="multi-armed-bandits-and-rl">📌 Multi-Armed Bandits and RL<a class="headerlink" href="#multi-armed-bandits-and-rl" title="Permanent link">&para;</a></h2>
<ul>
<li>In <strong>RL</strong>, actions affect both <strong>rewards</strong> and <strong>future states</strong>.</li>
<li>In <strong>MAB</strong>, there are <strong>no state transitions</strong>, only rewards from arms.</li>
</ul>
<pre class="mermaid"><code>flowchart LR
    subgraph RL [Reinforcement Learning]
        S0((S0)) --&gt;|A0| S1((S1))
        S1 --&gt;|A1| S2((S2))
        S2 --&gt;|A2| End((...))
    end

    subgraph MAB [Multi-Armed Bandit]
        A0 --&gt; R0
        A1 --&gt; R1
        A2 --&gt; R2
    end</code></pre>
<hr />
<h2 id="multi-armed-bandit-regret">📌 Multi-Armed Bandit: Regret<a class="headerlink" href="#multi-armed-bandit-regret" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Goal:</strong> Maximize cumulative reward.</li>
<li>But raw reward sum isn’t informative → we define <strong>regret</strong>.</li>
</ul>
<div class="arithmatex">\[
V_A = E_{R\sim R_A}[R|A], \quad A^* = \arg\max_A V_A
\]</div>
<ul>
<li><strong>Total Regret:</strong>
[
T \cdot V_{A^*} - \sum_{t=1}^T V_{A_t}
]</li>
</ul>
<blockquote>
<p>✅ Minimizing regret is equivalent to maximizing cumulative reward.</p>
</blockquote>
<hr />
<h2 id="epsilon-greedy-strategy">📌 Epsilon-Greedy Strategy<a class="headerlink" href="#epsilon-greedy-strategy" title="Permanent link">&para;</a></h2>
<ul>
<li>With probability <span class="arithmatex">\(\epsilon\)</span>: pick a <strong>random arm</strong> (exploration).</li>
<li>With probability <span class="arithmatex">\(1-\epsilon\)</span>: pick the <strong>best arm</strong> (exploitation).</li>
</ul>
<pre class="mermaid"><code>flowchart TD
    Start --&gt;|ε| RandomArm
    Start --&gt;|1-ε| BestArm
    RandomArm --&gt; A1[Arm1]
    RandomArm --&gt; A2[Arm2]
    RandomArm --&gt; An[Arm n]</code></pre>
<ul>
<li><strong>Practical Tip:</strong> Start with <span class="arithmatex">\(\epsilon = 0.1\)</span>, reduce it over time.</li>
<li>Production friendly: easy to implement.</li>
</ul>
<h3 id="variants">Variants:<a class="headerlink" href="#variants" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>ε-First:</strong> Pure exploration first, then exploitation.</li>
<li><strong>ε-Decreasing:</strong> <span class="arithmatex">\(ε_t ∝ 1/t\)</span>, regret <span class="arithmatex">\(O(\log T)\)</span>.</li>
<li>Adaptive ε schedules.</li>
</ol>
<hr />
<h2 id="upper-confidence-bound-ucb">📌 Upper Confidence Bound (UCB)<a class="headerlink" href="#upper-confidence-bound-ucb" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Idea:</strong> Use confidence intervals to balance exploration &amp; exploitation.</li>
<li>Reward distribution is unknown → sample multiple times.</li>
</ul>
<div class="arithmatex">\[
\hat V_k = \frac{1}{n} \sum_{i=1}^{n_k} R_k^{(i)}
\]</div>
<ul>
<li>
<p><strong>UCB Formula:</strong>
[
UCB_a(t) = \bar V_a + \sqrt{\frac{2 \ln t}{N_a(t)}}
]</p>
</li>
<li>
<p>Choose arm with maximum UCB.</p>
</li>
</ul>
<hr />
<h2 id="ucb-algorithm">📌 UCB Algorithm<a class="headerlink" href="#ucb-algorithm" title="Permanent link">&para;</a></h2>
<p><strong>Initialization:</strong>
- Pull each arm once.</p>
<p><strong>For each iteration t:</strong>
1. For each arm <span class="arithmatex">\(a\)</span>, compute:
   [ \bar V_{a,t} + \sqrt{\frac{2 \ln t}{N_a(t)}} ]
2. Select arm with maximum value.
3. Collect reward, update counts.</p>
<hr />
<h2 id="thompson-sampling-bayesian-perspective">📌 Thompson Sampling (Bayesian Perspective)<a class="headerlink" href="#thompson-sampling-bayesian-perspective" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Model for rewards:</strong> <span class="arithmatex">\(p(R_a | \theta_a), \theta_a \text{ unknown}\)</span>.</li>
<li><strong>Prior distribution on parameters:</strong> <span class="arithmatex">\(p(\theta_a)\)</span>.</li>
<li><strong>Posterior distribution of rewards:</strong> <span class="arithmatex">\(p(\theta_a | D_t)\)</span>, where <span class="arithmatex">\(D_t = (a_1, r_1, ..., a_t, r_t)\)</span>.</li>
<li>Use posterior to guide exploration.</li>
</ul>
<pre class="mermaid"><code>flowchart LR
    Prior[Prior Distribution] --&gt; Bayes[Posterior Inference]
    Bayes --&gt; Posterior[Posterior Distribution]
    Posterior --&gt; Action[Choose Arm]</code></pre>
<ul>
<li>Improved performance if distributional assumptions hold.</li>
</ul>
<h3 id="thompson-sampling-algorithm">Thompson Sampling Algorithm<a class="headerlink" href="#thompson-sampling-algorithm" title="Permanent link">&para;</a></h3>
<ol>
<li>Apply Bayes’ theorem to compute posterior <span class="arithmatex">\(p(\theta_a | D_t)\)</span>.</li>
<li>Sample reward distribution <span class="arithmatex">\(\theta_a\)</span> from posterior.</li>
<li>Compute expected value <span class="arithmatex">\(V_a = E[R_a]\)</span>.</li>
<li>Choose action:
   [
   a_t = \arg\max_{a \in A} V_a
   ]</li>
</ol>
<hr />
<h2 id="thompson-sampling-with-bernoulli-rewards">📌 Thompson Sampling with Bernoulli Rewards<a class="headerlink" href="#thompson-sampling-with-bernoulli-rewards" title="Permanent link">&para;</a></h2>
<ul>
<li>Model binary rewards <span class="arithmatex">\(R_x \sim Bernoulli(\theta_x)\)</span>.</li>
<li><span class="arithmatex">\(\theta_x = P(\text{purchase | product }=x)\)</span>.</li>
</ul>
<h3 id="prior-distribution">Prior Distribution:<a class="headerlink" href="#prior-distribution" title="Permanent link">&para;</a></h3>
<ul>
<li>Use <strong>Beta distribution</strong>:
[
p(\theta | \alpha, \beta) \propto \theta^{\alpha-1} (1-\theta)^{\beta-1}
]</li>
<li>Support: [0,1].</li>
<li>Conjugate prior for Bernoulli.</li>
</ul>
<h3 id="posterior-distribution">Posterior Distribution:<a class="headerlink" href="#posterior-distribution" title="Permanent link">&para;</a></h3>
<div class="arithmatex">\[
p(\theta_x | D_t) \propto p(D_t | \theta_x) p(\theta_x)
\]</div>
<p>If <span class="arithmatex">\(P_{x,t}\)</span> purchases, <span class="arithmatex">\(N_{x,t}\)</span> impressions:
[
p(\theta_x | D_t) = Beta(P_{x,t}+\alpha, N_{x,t}-P_{x,t}+\beta)
]</p>
<h3 id="thompson-sampling-algorithm-pseudo-code">Thompson Sampling Algorithm (Pseudo-code)<a class="headerlink" href="#thompson-sampling-algorithm-pseudo-code" title="Permanent link">&para;</a></h3>
<div class="language-python highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="n">Initialize</span> <span class="n">α</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">β</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">each</span> <span class="n">arm</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="n">For</span> <span class="n">each</span> <span class="nb">round</span> <span class="n">t</span><span class="p">:</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">For</span> <span class="n">each</span> <span class="n">arm</span> <span class="n">x</span><span class="p">:</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>        <span class="n">Sample</span> <span class="n">θ_x</span> <span class="o">~</span> <span class="n">Beta</span><span class="p">(</span><span class="n">P_x</span> <span class="o">+</span> <span class="n">α</span><span class="p">,</span> <span class="n">N_x</span> <span class="o">-</span> <span class="n">P_x</span> <span class="o">+</span> <span class="n">β</span><span class="p">)</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">Select</span> <span class="n">arm</span> <span class="n">x</span><span class="o">*</span> <span class="o">=</span> <span class="n">argmax_x</span> <span class="n">θ_x</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    <span class="n">Observe</span> <span class="n">reward</span> <span class="n">r</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    <span class="n">Update</span> <span class="n">counts</span> <span class="n">N_x</span><span class="p">,</span> <span class="n">P_x</span>
</span></code></pre></div>
<hr />
<h2 id="contextual-bandits">📌 Contextual Bandits<a class="headerlink" href="#contextual-bandits" title="Permanent link">&para;</a></h2>
<h3 id="motivation">Motivation<a class="headerlink" href="#motivation" title="Permanent link">&para;</a></h3>
<ul>
<li>Standard bandits treat all users/items the same.</li>
<li><strong>Personalized recommendation:</strong></li>
<li>One bandit problem per user (too expensive to train individually).</li>
<li>Better: incorporate <strong>context information</strong> (user demographics, browsing history, time, location).</li>
<li>Items (arms) also have features: genre, price, actors.</li>
</ul>
<h3 id="mathematical-formulation">Mathematical Formulation<a class="headerlink" href="#mathematical-formulation" title="Permanent link">&para;</a></h3>
<ul>
<li>Tuple: <span class="arithmatex">\((S, A, R)\)</span>.</li>
<li><span class="arithmatex">\(S\)</span>: context distribution.</li>
<li><span class="arithmatex">\(A\)</span>: set of actions.</li>
<li><span class="arithmatex">\(R_{s,a}\)</span>: reward distribution given context <span class="arithmatex">\(s\)</span> and action <span class="arithmatex">\(a\)</span>.</li>
</ul>
<p>At each step <span class="arithmatex">\(t\)</span>:
1. Environment generates context <span class="arithmatex">\(s_t\)</span>.
2. Agent selects action <span class="arithmatex">\(a_t\)</span>.
3. Environment returns reward <span class="arithmatex">\(r_t \sim R_{s_t,a_t}\)</span>.</p>
<blockquote>
<p>🎯 Goal: maximize cumulative reward:
[
\sum_{t=1}^T r_t
]</p>
</blockquote>
<pre class="mermaid"><code>flowchart LR
    UserContext[Context s_t] --&gt; Agent
    Agent --&gt;|Choose Action a_t| Arm[Action/Arm]
    Arm --&gt; Reward[r_t]</code></pre>
<h3 id="comparison">Comparison<a class="headerlink" href="#comparison" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>MAB:</strong> No context.</li>
<li><strong>Contextual Bandit:</strong> Includes features (user/item).</li>
<li><strong>Full RL:</strong> Adds state transitions.</li>
</ul>
<hr />
<h2 id="contextual-bandit-algorithms">📌 Contextual Bandit Algorithms<a class="headerlink" href="#contextual-bandit-algorithms" title="Permanent link">&para;</a></h2>
<h3 id="epsilon-greedy-with-context">Epsilon-Greedy (with context)<a class="headerlink" href="#epsilon-greedy-with-context" title="Permanent link">&para;</a></h3>
<ul>
<li>Works without modification.</li>
<li>Use a supervised model <span class="arithmatex">\(f(s,a)\)</span> to estimate rewards.</li>
</ul>
<h3 id="ucb-with-context">UCB with Context<a class="headerlink" href="#ucb-with-context" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>Estimate value function with linear approximation:
[
q_θ(s,a) = φ(s,a)^T θ
]</p>
</li>
<li>
<p>Pull arm:
[
a^* = \arg\max_a (\hat q(s,a) + c \cdot \sqrt{Var(\hat q(s,a))})
]</p>
</li>
</ul>
<h3 id="thompson-sampling-with-context">Thompson Sampling with Context<a class="headerlink" href="#thompson-sampling-with-context" title="Permanent link">&para;</a></h3>
<ul>
<li>Reward depends on (state, action, parameters).</li>
<li>Use Bayesian posterior inference.</li>
<li>Posterior inference can be intractable → use approximations (e.g., Gibbs sampling).</li>
</ul>
<hr />
<h2 id="contextual-bandit-summary">📌 Contextual Bandit Summary<a class="headerlink" href="#contextual-bandit-summary" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Algorithm</th>
<th>Pros</th>
<th>Cons</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ε-Greedy</strong></td>
<td>Simple, easy to implement</td>
<td>Requires careful tuning of ε</td>
</tr>
<tr>
<td><strong>UCB</strong></td>
<td>Theoretically grounded, balances exploration</td>
<td>Computationally expensive (depends on feature dimension)</td>
</tr>
<tr>
<td><strong>Thompson Sampling</strong></td>
<td>Handles uncertainty well, resilient to delayed feedback</td>
<td>Posterior inference may be computationally heavy</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="big-picture">📌 Big Picture<a class="headerlink" href="#big-picture" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Multi-armed bandits</strong> → Simplest framework.</li>
<li><strong>Contextual bandits</strong> → Personalized recommendations, online ads.</li>
<li><strong>Full RL (MDP)</strong> → Sequential decisions with long-term consequences.</li>
</ul>
<p>Reinforcement Learning unifies these frameworks, providing a spectrum from simple one-shot decisions to complex long-horizon planning.</p>
<hr />







  
    
  
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="August 29, 2025 04:54:44 UTC">August 29, 2025</span>
  </span>

    
    
    
    
  </aside>





                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../lec5/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Sequential Learning">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Sequential Learning
              </div>
            </div>
          </a>
        
        
          
          <a href="../../cheatsheets/" class="md-footer__link md-footer__link--next" aria-label="Next: Cheatsheets">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Cheatsheets
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="https://github.com/adityachauhan0" target="_blank" rel="noopener" title="GitHub" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.instant", "navigation.tracking", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.footer", "toc.integrate", "toc.follow", "search.suggest", "search.highlight", "search.share", "content.code.copy", "content.code.annotate", "content.tooltips", "header.autohide", "content.action.edit", "content.action.view"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.92b07e13.min.js"></script>
      
        <script src="https://unpkg.com/mermaid@10/dist/mermaid.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
      
        <script src="../../js/katex-init.js"></script>
      
    
  </body>
</html>