
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="My ML notes, cheatsheets, and experiments">
      
      
      
        <link rel="canonical" href="https://adityachauhan0.github.io/amazon-ml/notes/lec7/">
      
      
        <link rel="prev" href="../lec6/">
      
      
        <link rel="next" href="../lec8/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.17">
    
    
      
        <title>Generative AI and LLMS - Machine Learning Notes</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.7e37652d.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="pink">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#generative-ai-and-large-language-models-llms" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Machine Learning Notes" class="md-header__button md-logo" aria-label="Machine Learning Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Machine Learning Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Generative AI and LLMS
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="pink"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="pink"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_2" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="black" data-md-color-accent="pink"  aria-label="Switch to high contrast"  type="radio" name="__palette" id="__palette_2">
    
      <label class="md-header__button md-icon" title="Switch to high contrast" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 20c-2.21 0-4.21-.9-5.66-2.34L17.66 6.34A8.01 8.01 0 0 1 20 12a8 8 0 0 1-8 8M6 8h2V6h1.5v2h2v1.5h-2v2H8v-2H6M12 2A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10A10 10 0 0 0 12 2m0 14h5v-1.5h-5z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/adityachauhan0/amazon-ml" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    adityachauhan0/amazon-ml
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../lecture3/" class="md-tabs__link">
          
  
  
    
  
  Notes

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../cheatsheets/" class="md-tabs__link">
        
  
  
    
  
  Cheatsheets

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Machine Learning Notes" class="md-nav__button md-logo" aria-label="Machine Learning Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Machine Learning Notes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/adityachauhan0/amazon-ml" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    adityachauhan0/amazon-ml
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Notes
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Notes
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lecture3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dimensionality Reduction
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lecture4/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Unsupervised Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lec5/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sequential Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lec6/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Reinforcement Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Generative AI and LLMS
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Generative AI and LLMS
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-what-is-generative-ai-and-llm" class="md-nav__link">
    <span class="md-ellipsis">
      1. What is Generative AI and LLM?
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. What is Generative AI and LLM?">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hierarchical-relationship" class="md-nav__link">
    <span class="md-ellipsis">
      Hierarchical Relationship
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-applications-of-generative-ai" class="md-nav__link">
    <span class="md-ellipsis">
      2. Applications of Generative AI
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Applications of Generative AI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#content" class="md-nav__link">
    <span class="md-ellipsis">
      Content
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#customer-service" class="md-nav__link">
    <span class="md-ellipsis">
      Customer Service
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#healthcare" class="md-nav__link">
    <span class="md-ellipsis">
      Healthcare
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#finance" class="md-nav__link">
    <span class="md-ellipsis">
      Finance
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#education" class="md-nav__link">
    <span class="md-ellipsis">
      Education
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-transformer-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      3. Transformer Architecture
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-gpt-1" class="md-nav__link">
    <span class="md-ellipsis">
      4. GPT-1
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-bert-bidirectional-encoder-representations-from-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      5. BERT (Bidirectional Encoder Representations from Transformers)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-t5-text-to-text-transfer-transformer" class="md-nav__link">
    <span class="md-ellipsis">
      6. T5 (Text-to-Text Transfer Transformer)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-gpt-2" class="md-nav__link">
    <span class="md-ellipsis">
      7. GPT-2
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-gpt-3" class="md-nav__link">
    <span class="md-ellipsis">
      8. GPT-3
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-instructgpt" class="md-nav__link">
    <span class="md-ellipsis">
      9. InstructGPT
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10-four-stage-training-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      10. Four-Stage Training Pipeline
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10. Four-Stage Training Pipeline">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#stages" class="md-nav__link">
    <span class="md-ellipsis">
      Stages
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11-supervised-fine-tuning-sft" class="md-nav__link">
    <span class="md-ellipsis">
      11. Supervised Fine-Tuning (SFT)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#12-rlhf-reinforcement-learning-with-human-feedback" class="md-nav__link">
    <span class="md-ellipsis">
      12. RLHF (Reinforcement Learning with Human Feedback)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#13-gpt-4" class="md-nav__link">
    <span class="md-ellipsis">
      13. GPT-4
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#14-llama-models" class="md-nav__link">
    <span class="md-ellipsis">
      14. LLAMA Models
    </span>
  </a>
  
    <nav class="md-nav" aria-label="14. LLAMA Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llama-results" class="md-nav__link">
    <span class="md-ellipsis">
      LLAMA Results
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-2" class="md-nav__link">
    <span class="md-ellipsis">
      LLaMA-2
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-llama-chat" class="md-nav__link">
    <span class="md-ellipsis">
      Training LLaMA-Chat
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llama-3" class="md-nav__link">
    <span class="md-ellipsis">
      LLaMA-3
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#15-clip-contrastive-languageimage-pretraining" class="md-nav__link">
    <span class="md-ellipsis">
      15. CLIP (Contrastive Language–Image Pretraining)
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#16-flamingo" class="md-nav__link">
    <span class="md-ellipsis">
      16. Flamingo
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#17-current-llm-leaderboard" class="md-nav__link">
    <span class="md-ellipsis">
      17. Current LLM Leaderboard
    </span>
  </a>
  
    <nav class="md-nav" aria-label="17. Current LLM Leaderboard">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#lm-sys-arena" class="md-nav__link">
    <span class="md-ellipsis">
      LM-Sys Arena
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../lec8/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Causal Inference
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cheatsheets/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Cheatsheets
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/adityachauhan0/amazon-ml/edit/main/docs/notes/lec7.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"/></svg>
    </a>
  
  
    
      
    
    <a href="https://github.com/adityachauhan0/amazon-ml/raw/main/docs/notes/lec7.md" title="View source of this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 18c.56 0 1 .44 1 1s-.44 1-1 1-1-.44-1-1 .44-1 1-1m0-3c-2.73 0-5.06 1.66-6 4 .94 2.34 3.27 4 6 4s5.06-1.66 6-4c-.94-2.34-3.27-4-6-4m0 6.5a2.5 2.5 0 0 1-2.5-2.5 2.5 2.5 0 0 1 2.5-2.5 2.5 2.5 0 0 1 2.5 2.5 2.5 2.5 0 0 1-2.5 2.5M9.27 20H6V4h7v5h5v4.07c.7.08 1.36.25 2 .49V8l-6-6H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h4.5a8.2 8.2 0 0 1-1.23-2"/></svg>
    </a>
  


<h1 id="generative-ai-and-large-language-models-llms">Generative AI and Large Language Models (LLMs)<a class="headerlink" href="#generative-ai-and-large-language-models-llms" title="Permanent link">&para;</a></h1>
<h2 id="1-what-is-generative-ai-and-llm">1. What is Generative AI and LLM?<a class="headerlink" href="#1-what-is-generative-ai-and-llm" title="Permanent link">&para;</a></h2>
<p>Generative Artificial Intelligence (GenAI) is a <strong>subset of machine learning</strong> focused on creating new content based on existing data.</p>
<ul>
<li><strong>Definition</strong>: GenAI uses neural networks to analyze large datasets (text, images, audio, videos) and generate new outputs that may resemble or differ from the original data.</li>
<li><strong>LLMs</strong>: Large Language Models are a specific class of GenAI trained on massive text datasets to generate human-like text.</li>
<li><strong>Examples</strong>: ChatGPT, Bard, Claude.</li>
</ul>
<h3 id="hierarchical-relationship">Hierarchical Relationship<a class="headerlink" href="#hierarchical-relationship" title="Permanent link">&para;</a></h3>
<pre class="mermaid"><code>flowchart TD
    A[Artificial Intelligence]
    B[Machine Learning]
    C[Deep Learning]
    D[Generative AI]
    E[Large Language Models]

    A --&gt; B --&gt; C --&gt; D --&gt; E</code></pre>
<hr />
<h2 id="2-applications-of-generative-ai">2. Applications of Generative AI<a class="headerlink" href="#2-applications-of-generative-ai" title="Permanent link">&para;</a></h2>
<h3 id="content">Content<a class="headerlink" href="#content" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Text Generation</strong>: Articles, blogs, reports, social posts.</li>
<li><strong>Creative Writing</strong>: Stories, scripts, poetry, dialogues.</li>
<li><strong>Image/Art Generation</strong>: DALL·E, Stable Diffusion for illustrations and designs.</li>
</ul>
<h3 id="customer-service">Customer Service<a class="headerlink" href="#customer-service" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Chatbots</strong>: 24/7 automated support.</li>
<li><strong>Personalized Recommendations</strong>: Product/service suggestions.</li>
<li><strong>Automated Responses</strong>: Handling routine queries.</li>
</ul>
<h3 id="healthcare">Healthcare<a class="headerlink" href="#healthcare" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Drug Discovery</strong>: Generate new compounds and predict effectiveness.</li>
<li><strong>Medical Imaging</strong>: Enhance and generate diagnostic images.</li>
<li><strong>Personalized Medicine</strong>: Tailored treatment plans.</li>
</ul>
<h3 id="finance">Finance<a class="headerlink" href="#finance" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Fraud Detection</strong>: Pattern recognition for fraudulent activity.</li>
<li><strong>Risk Assessment</strong>: Predict risks and opportunities.</li>
<li><strong>Report Generation</strong>: Automating financial summaries.</li>
</ul>
<h3 id="education">Education<a class="headerlink" href="#education" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Tutoring Systems</strong>: Personalized learning support.</li>
<li><strong>Content Generation</strong>: Quizzes, interactive lessons.</li>
<li><strong>Language Learning</strong>: Conversation practice, grammar tools.</li>
</ul>
<hr />
<h2 id="3-transformer-architecture">3. Transformer Architecture<a class="headerlink" href="#3-transformer-architecture" title="Permanent link">&para;</a></h2>
<p>Transformers revolutionized NLP by removing recurrence and convolutions:</p>
<ul>
<li>Relies only on <strong>self-attention</strong>.</li>
<li><strong>Parallelizable</strong> → faster training.</li>
<li>Handles <strong>long sequences</strong> effectively.</li>
</ul>
<pre class="mermaid"><code>graph LR
    subgraph Encoder
        E1[Encoder Layer 1] --&gt; E2[Encoder Layer 2]
        E2 --&gt; E3[Encoder Layer N]
    end

    subgraph Decoder
        D1[Decoder Layer 1] --&gt; D2[Decoder Layer 2]
        D2 --&gt; D3[Decoder Layer N]
    end

    Input[Input Sequence] --&gt; E1
    E3 --&gt; D1
    D3 --&gt; Output[Output Sequence]</code></pre>
<hr />
<h2 id="4-gpt-1">4. GPT-1<a class="headerlink" href="#4-gpt-1" title="Permanent link">&para;</a></h2>
<ul>
<li>Motivation: Lack of labeled NLP data.</li>
<li>Uses <strong>Transformer Decoder</strong> only.</li>
<li><strong>Two-stage training</strong>:</li>
<li><strong>Pre-training</strong>: Unsupervised learning on large corpus.</li>
<li><strong>Fine-tuning</strong>: Supervised on labeled datasets.</li>
</ul>
<hr />
<h2 id="5-bert-bidirectional-encoder-representations-from-transformers">5. BERT (Bidirectional Encoder Representations from Transformers)<a class="headerlink" href="#5-bert-bidirectional-encoder-representations-from-transformers" title="Permanent link">&para;</a></h2>
<ul>
<li>Uses <strong>Masked Language Modeling (MLM)</strong> and <strong>Next Sentence Prediction (NSP)</strong>.</li>
<li><strong>Two-stage training</strong>:</li>
<li>Pre-training (unsupervised)</li>
<li>Fine-tuning (supervised)</li>
</ul>
<p>Mathematically, MLM masks random tokens:
$$ \text{Loss} = - \sum_{i \in M} \log P(x_i | x_{\setminus M}) $$
Where <span class="arithmatex">\(M\)</span> is the set of masked tokens.</p>
<hr />
<h2 id="6-t5-text-to-text-transfer-transformer">6. T5 (Text-to-Text Transfer Transformer)<a class="headerlink" href="#6-t5-text-to-text-transfer-transformer" title="Permanent link">&para;</a></h2>
<ul>
<li>Reformulates all NLP tasks as <strong>text-to-text</strong>.</li>
<li>Pre-trained on <strong>C4 dataset (700GB)</strong>.</li>
<li>Fine-tuning with <strong>task-specific prefixes</strong>.</li>
<li>Even regression tasks → <strong>text generation</strong>.</li>
</ul>
<p>Example:
- Task: Translation → Input: <code>translate English to German: That is good.</code> → Output: <code>Das ist gut.</code></p>
<hr />
<h2 id="7-gpt-2">7. GPT-2<a class="headerlink" href="#7-gpt-2" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>1.5B parameters</strong>, much larger than GPT-1.</li>
<li>Introduced <strong>zero-shot learning</strong> → model performs tasks without fine-tuning.</li>
<li>Variants: 117M, 345M, 762M, 1.5B parameters.</li>
<li>Perplexity decreases with scale:</li>
</ul>
<div class="language-text highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>Perplexity ↓ as Model Size ↑
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>117M ▄▄▄▄▄▄▄▄▄▄ 16
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>345M ▄▄▄▄▄▄▄    12
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>762M ▄▄▄▄▄      10
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>1.5B ▄▄▄        8
</span></code></pre></div>
<hr />
<h2 id="8-gpt-3">8. GPT-3<a class="headerlink" href="#8-gpt-3" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>175B parameters</strong>.</li>
<li>Introduced <strong>in-context learning</strong> (ICL).</li>
<li><strong>Zero-shot, One-shot, Few-shot</strong> prompting.</li>
<li>Better generalization with scale.</li>
</ul>
<p>Example:
- Zero-shot: <code>Translate English to French: cheese → fromage</code>
- One-shot: With one example given.
- Few-shot: With multiple examples.</p>
<hr />
<h2 id="9-instructgpt">9. InstructGPT<a class="headerlink" href="#9-instructgpt" title="Permanent link">&para;</a></h2>
<ul>
<li>Improved <strong>instruction-following</strong> over GPT-3.</li>
<li>Uses <strong>Supervised Fine-tuning (SFT)</strong>.</li>
<li>Enhanced with <strong>RLHF (Reinforcement Learning with Human Feedback)</strong> for safety and helpfulness.</li>
</ul>
<hr />
<h2 id="10-four-stage-training-pipeline">10. Four-Stage Training Pipeline<a class="headerlink" href="#10-four-stage-training-pipeline" title="Permanent link">&para;</a></h2>
<pre class="mermaid"><code>graph TD
    A[Pre-training]
    B[Supervised Fine-tuning]
    C[Reward Modeling]
    D[Reinforcement Learning]

    A --&gt; B --&gt; C --&gt; D</code></pre>
<h3 id="stages">Stages<a class="headerlink" href="#stages" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Pre-training</strong></li>
<li>Data: Raw internet (trillions of words).</li>
<li>Task: Predict next token.</li>
<li>
<p>Output: Base model.</p>
</li>
<li>
<p><strong>Supervised Fine-tuning (SFT)</strong></p>
</li>
<li>Data: Human-written demonstrations (10k–100k examples).</li>
<li>
<p>Output: SFT model.</p>
</li>
<li>
<p><strong>Reward Modeling (RM)</strong></p>
</li>
<li>Data: Human preference comparisons (100k–1M).</li>
<li>Task: Binary classification of preferred outputs.</li>
<li>
<p>Output: Reward model.</p>
</li>
<li>
<p><strong>Reinforcement Learning (RLHF)</strong></p>
</li>
<li>Data: Prompts + human feedback.</li>
<li>Task: Optimize generation to maximize human preference reward.</li>
<li>Output: Aligned model (e.g., ChatGPT, Claude).</li>
</ol>
<hr />
<h2 id="11-supervised-fine-tuning-sft">11. Supervised Fine-Tuning (SFT)<a class="headerlink" href="#11-supervised-fine-tuning-sft" title="Permanent link">&para;</a></h2>
<p>SFT aligns pre-trained models with human instructions by training on <strong>prompt-response pairs</strong>.</p>
<p>Example training data:
<div class="language-text highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>Prompt: What is the capital of France?
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>Text: The capital of France is Paris.
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>Prompt: What is the best way to save money?
</span><span id="__span-1-5"><a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>Text: Create a budget, track spending, avoid debt, and invest wisely.
</span></code></pre></div></p>
<p>Workflow:
- Input: Prompts &amp; text dataset.
- Output: Language model aligned with supervised responses.</p>
<pre class="mermaid"><code>graph LR
    A[Prompts &amp; Responses] --&gt; B[Supervised Fine-Tuning]
    B --&gt; C[Language Model]</code></pre>
<hr />
<h2 id="12-rlhf-reinforcement-learning-with-human-feedback">12. RLHF (Reinforcement Learning with Human Feedback)<a class="headerlink" href="#12-rlhf-reinforcement-learning-with-human-feedback" title="Permanent link">&para;</a></h2>
<p>Steps:
1. <strong>Collect demonstrations</strong> → human-written responses fine-tune the model.
2. <strong>Collect comparisons</strong> → humans rank multiple model outputs.
3. <strong>Reward modeling</strong> → train a model to predict human preference.
4. <strong>RL Optimization</strong> → use PPO (Proximal Policy Optimization) to maximize human-aligned reward.</p>
<pre class="mermaid"><code>flowchart TD
    A[Human Demonstrations] --&gt; B[SFT Policy]
    B --&gt; C[Generate Candidate Responses]
    D[Human Rankings] --&gt; E[Reward Model]
    C --&gt; E --&gt; F[PPO Optimization]
    F --&gt; G[Aligned Model]</code></pre>
<hr />
<h2 id="13-gpt-4">13. GPT-4<a class="headerlink" href="#13-gpt-4" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Multimodal</strong> → accepts <strong>text + images</strong>.</li>
<li>Trained with <strong>SFT + RLHF</strong>.</li>
<li>Larger architecture, significantly better than GPT-3.5.</li>
</ul>
<hr />
<h2 id="14-llama-models">14. LLAMA Models<a class="headerlink" href="#14-llama-models" title="Permanent link">&para;</a></h2>
<h3 id="llama-results">LLAMA Results<a class="headerlink" href="#llama-results" title="Permanent link">&para;</a></h3>
<ul>
<li>Shows benchmark comparisons across BoolQ, PIQA, SIQA, HellaSwag, ARC, etc.</li>
<li><strong>Observation</strong>: LLaMA performs competitively despite being smaller in size compared to GPT-3/PaLM.</li>
</ul>
<h3 id="llama-2">LLaMA-2<a class="headerlink" href="#llama-2" title="Permanent link">&para;</a></h3>
<ul>
<li>Trained on <strong>40% more data</strong> than LLaMA-1.</li>
<li>Context length: <strong>4096 tokens</strong>.</li>
<li>Variants: 7B, 13B, 70B.</li>
<li>Introduced <strong>chat-tuned models</strong> (LLaMA-2-chat) with SFT + RLHF.</li>
</ul>
<h3 id="training-llama-chat">Training LLaMA-Chat<a class="headerlink" href="#training-llama-chat" title="Permanent link">&para;</a></h3>
<p>Workflow:
1. Pretraining → self-supervised.
2. SFT → align with demonstrations.
3. RLHF → optimize with human preference + rejection sampling.</p>
<h3 id="llama-3">LLaMA-3<a class="headerlink" href="#llama-3" title="Permanent link">&para;</a></h3>
<ul>
<li>Released with <strong>8B and 70B parameters</strong>.</li>
<li>Vocabulary size: <strong>128k tokens</strong>, max context: <strong>8192</strong>.</li>
<li>Trained on <strong>7x larger dataset</strong> and <strong>4x more code</strong> than LLaMA-2.</li>
<li>Plans for <strong>400B+ parameter multimodal, multilingual models</strong>.</li>
</ul>
<hr />
<h2 id="15-clip-contrastive-languageimage-pretraining">15. CLIP (Contrastive Language–Image Pretraining)<a class="headerlink" href="#15-clip-contrastive-languageimage-pretraining" title="Permanent link">&para;</a></h2>
<ul>
<li>Developed by OpenAI.</li>
<li>Trains text &amp; image encoders jointly using <strong>contrastive loss</strong>.</li>
<li>Learns alignment between image and text representations.</li>
<li>Enables <strong>zero-shot classification</strong> without fine-tuning.</li>
</ul>
<pre class="mermaid"><code>graph TD
    T[Text Encoder] --&gt; Z[Joint Embedding Space]
    I[Image Encoder] --&gt; Z
    Z --&gt; Output[Zero-Shot Classifier]</code></pre>
<hr />
<h2 id="16-flamingo">16. Flamingo<a class="headerlink" href="#16-flamingo" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Visual Language Model (VLM)</strong> by DeepMind.</li>
<li>Input: <strong>Text + Images</strong>.</li>
<li>Output: <strong>Text</strong>.</li>
<li>Applications: answering questions about images, describing videos, multimodal reasoning.</li>
<li>Uses <strong>vision encoder + perceiver resampler + language model</strong> with gated cross-attention.</li>
</ul>
<p>Workflow:
<pre class="mermaid"><code>graph TD
    I[Image Input] --&gt; V[Vision Encoder]
    V --&gt; R[Perceiver Resampler]
    T[Text Input] --&gt; LM[Language Model Blocks]
    R --&gt; LM
    LM --&gt; O[Text Output]</code></pre></p>
<hr />
<h2 id="17-current-llm-leaderboard">17. Current LLM Leaderboard<a class="headerlink" href="#17-current-llm-leaderboard" title="Permanent link">&para;</a></h2>
<h3 id="lm-sys-arena">LM-Sys Arena<a class="headerlink" href="#lm-sys-arena" title="Permanent link">&para;</a></h3>
<ul>
<li>Open platform for LLM evaluations.</li>
<li>Over <strong>1M human pairwise comparisons</strong>.</li>
<li>Models ranked by <strong>Arena score</strong>.</li>
</ul>
<p>Example (2024 snapshot):
1. GPT-4o (OpenAI)
2. Claude 3.5 Sonnet (Anthropic)
3. Gemini Advanced (Google DeepMind)
4. GPT-4 Turbo
5. Yi-Large, Mixtral, and others</p>
<p>Leaderboard evolves as new models are released.</p>
<hr />
<h1 id="using-llms-for-your-task">Using LLMs for Your Task<a class="headerlink" href="#using-llms-for-your-task" title="Permanent link">&para;</a></h1>
<h3 id="problem-statement">Problem Statement<a class="headerlink" href="#problem-statement" title="Permanent link">&para;</a></h3>
<p>Develop a chatbot which can answer questions related to your website.</p>
<h3 id="approaches">Approaches<a class="headerlink" href="#approaches" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Train a model from scratch</strong> on your data.</li>
<li><strong>Use an existing LLM as-is</strong> without modification.</li>
<li><strong>Fine-tune</strong> an LLM on the data from your website.</li>
<li><strong>Prompt-tuning</strong> using In-Context Learning (ICL) examples.</li>
<li><strong>Retrieval Augmented Generation (RAG)</strong> for knowledge-grounded responses.</li>
</ol>
<hr />
<h1 id="what-is-fine-tuning">What is Fine-Tuning?<a class="headerlink" href="#what-is-fine-tuning" title="Permanent link">&para;</a></h1>
<p>Fine-tuning is the process of taking a pre-trained LLM and further training it on a <strong>specific, smaller dataset</strong> to adapt it to particular tasks or domains.</p>
<h3 id="purpose">Purpose<a class="headerlink" href="#purpose" title="Permanent link">&para;</a></h3>
<ul>
<li>Improves performance on specialized tasks.</li>
<li>Reduces need for training from scratch.</li>
<li>Customizes the model for domain-specific terminology and contexts.</li>
</ul>
<pre class="mermaid"><code>graph LR
    A[Giant Corpus of Data] --&gt; B[Base LLM]
    C[Domain Specific Data] --&gt; D[Fine-Tuning]
    B --&gt; D
    D --&gt; E[Fine-Tuned LLM]
    User[User Query] --&gt; E
    E --&gt; R[Response]</code></pre>
<hr />
<h1 id="why-fine-tuning">Why Fine-Tuning?<a class="headerlink" href="#why-fine-tuning" title="Permanent link">&para;</a></h1>
<ol>
<li><strong>Enhanced Performance</strong></li>
<li>Tailors the model for specific tasks → higher accuracy.</li>
<li>
<p>Example: Medical diagnosis support, legal document analysis, customer support.</p>
</li>
<li>
<p><strong>Cost and Data Efficiency</strong></p>
</li>
<li>Uses pre-trained models instead of training from scratch.</li>
<li>Requires fewer computational resources.</li>
<li>Speeds up deployment.</li>
<li>
<p>Smaller, targeted datasets are enough.</p>
</li>
<li>
<p><strong>Domain Adaptation</strong></p>
</li>
<li>Customizes models for specific industries.</li>
<li>Example: Legal language, technical jargon, medical terminology.</li>
</ol>
<hr />
<h1 id="pre-trained-vs-fine-tuned-models">Pre-Trained vs Fine-Tuned Models<a class="headerlink" href="#pre-trained-vs-fine-tuned-models" title="Permanent link">&para;</a></h1>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Pre-Trained Model</th>
<th>Fine-Tuned Model</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Training Data</strong></td>
<td>Terabytes</td>
<td>100–1000 MB</td>
</tr>
<tr>
<td><strong>Training Time</strong></td>
<td>Weeks</td>
<td>Hours</td>
</tr>
<tr>
<td><strong>Cost</strong></td>
<td>$ Millions</td>
<td>$ Hundreds</td>
</tr>
<tr>
<td><strong>Compute</strong></td>
<td>Thousands of GPUs</td>
<td>One or few GPUs</td>
</tr>
</tbody>
</table>
<h3 id="example">Example<a class="headerlink" href="#example" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Pre-trained</strong>: Learns broad world knowledge.</li>
<li><strong>Fine-tuned</strong>: Learns task-specific queries (e.g., “When was Barack Obama born?” → <code>1961</code>).</li>
</ul>
<hr />
<h1 id="data-selection-for-fine-tuning">Data Selection for Fine-Tuning<a class="headerlink" href="#data-selection-for-fine-tuning" title="Permanent link">&para;</a></h1>
<ol>
<li><strong>Relevance</strong>: Data must be task-specific (e.g., medical texts for healthcare AI).</li>
<li><strong>Quality</strong>: Remove errors, duplicates, noise.</li>
<li><strong>Diversity</strong>: Wide range of examples for better generalization.</li>
<li><strong>Size</strong>: Sufficient coverage without being computationally prohibitive.</li>
<li><strong>Augmentation</strong>: Use synthetic data where real data is limited.</li>
<li><strong>Ethical Considerations</strong>: Avoid bias, ensure fairness.</li>
</ol>
<hr />
<h1 id="fine-tuning-methods">Fine-Tuning Methods<a class="headerlink" href="#fine-tuning-methods" title="Permanent link">&para;</a></h1>
<ol>
<li><strong>Full Fine-Tuning</strong></li>
<li>Updates all parameters.</li>
<li>High flexibility.</li>
<li>
<p>Longer training time.</p>
</li>
<li>
<p><strong>Parameter Efficient Fine-Tuning (PEFT)</strong></p>
</li>
<li>Updates only a subset of parameters.</li>
<li>Less flexible but more efficient.</li>
<li>Includes adapters, low-rank adaptations, freezing large parts.</li>
</ol>
<hr />
<h1 id="peft-parameter-efficient-fine-tuning">PEFT (Parameter Efficient Fine-Tuning)<a class="headerlink" href="#peft-parameter-efficient-fine-tuning" title="Permanent link">&para;</a></h1>
<ul>
<li>ML models have billions of parameters.</li>
<li>Tuning all is <strong>expensive and slow</strong>.</li>
<li>PEFT modifies only a <strong>subset of parameters</strong>.</li>
<li>Greatly reduces computation and memory needs.</li>
</ul>
<pre class="mermaid"><code>graph TD
    A[Pretrained Weights W] --&gt;|Frozen| M[Model]
    B[Subset of Parameters] --&gt;|Trainable| M
    M --&gt; O[Fine-Tuned Output]</code></pre>
<hr />
<h1 id="lora-low-rank-adaptation">LoRA (Low-Rank Adaptation)<a class="headerlink" href="#lora-low-rank-adaptation" title="Permanent link">&para;</a></h1>
<ul>
<li>Decomposes weight matrices into <strong>low-rank matrices</strong> during fine-tuning.</li>
<li>Reduces number of trainable parameters.</li>
<li>Maintains high performance with minimal overhead.</li>
<li>Ideal for resource-limited scenarios.</li>
</ul>
<h3 id="key-ideas">Key Ideas<a class="headerlink" href="#key-ideas" title="Permanent link">&para;</a></h3>
<ul>
<li>Only low-rank matrices are updated → rest remain frozen.</li>
<li>Reduces memory + compute usage.</li>
<li>Effective for NLP, legal, healthcare, and domain-specific tasks.</li>
</ul>
<div class="arithmatex">\[W' = W + A \times B\]</div>
<p>Where:
- <span class="arithmatex">\(W\)</span> = frozen pre-trained weight matrix.
- <span class="arithmatex">\(A, B\)</span> = small trainable low-rank matrices.</p>
<hr />
<h1 id="qlora-quantization-lora">QLoRA (Quantization + LoRA)<a class="headerlink" href="#qlora-quantization-lora" title="Permanent link">&para;</a></h1>
<ul>
<li><strong>Quantization</strong>: Reduces precision (e.g., 32-bit → 8-bit/4-bit) → lower memory usage &amp; higher efficiency.</li>
<li><strong>QLoRA</strong> = Quantization + LoRA.</li>
<li>Enables training large models on smaller GPUs.</li>
<li>Maintains high accuracy with reduced compute &amp; memory.</li>
</ul>
<hr />
<h1 id="prompts-and-prompt-engineering">Prompts and Prompt Engineering<a class="headerlink" href="#prompts-and-prompt-engineering" title="Permanent link">&para;</a></h1>
<h2 id="what-is-a-prompt">What is a Prompt?<a class="headerlink" href="#what-is-a-prompt" title="Permanent link">&para;</a></h2>
<p>A <strong>prompt</strong> is an input/query given to the model to generate a desired response.
- Provides context &amp; guidance.
- Examples:
  - <strong>Story Prompt</strong>: “Write a short story about a robot discovering a new planet.”
  - <strong>Technical Prompt</strong>: “Explain the concept of blockchain technology.”</p>
<hr />
<h2 id="writing-good-prompts">Writing Good Prompts<a class="headerlink" href="#writing-good-prompts" title="Permanent link">&para;</a></h2>
<ol>
<li><strong>Be clear and specific</strong></li>
<li>✅ “What are the main challenges of remote work?”</li>
<li>❌ “Is remote work good?”</li>
<li><strong>Provide context</strong></li>
<li>✅ “Explain photosynthesis to a 10-year-old.”</li>
<li><strong>Set tone/style</strong></li>
<li>✅ “Write a funny story about a cat who thinks it’s a superhero.”</li>
<li><strong>Use examples/templates</strong></li>
<li>✅ “Write a smartphone review covering features, pros, cons.”</li>
</ol>
<hr />
<h2 id="prompt-engineering-few-shot">Prompt Engineering: Few-Shot<a class="headerlink" href="#prompt-engineering-few-shot" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>Few-shot prompting</strong>: Provide a few examples in the prompt.</li>
<li>Enables models to generalize without fine-tuning.</li>
</ul>
<p>Example:
<div class="language-text highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>Translate English → French
</span><span id="__span-2-2"><a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>Input: I am going for a vacation → Output: je pars en vacances
</span><span id="__span-2-3"><a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>Input: How was your weekend? → Output: Comment c&#39;est passé ton week-end
</span><span id="__span-2-4"><a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>Input: Hi, how are you → Output: ?
</span></code></pre></div></p>
<hr />
<h2 id="chain-of-thought-prompting">Chain-of-Thought Prompting<a class="headerlink" href="#chain-of-thought-prompting" title="Permanent link">&para;</a></h2>
<ul>
<li>Encourages model to <strong>show intermediate reasoning steps</strong>.</li>
<li>Improves performance on reasoning/math tasks.</li>
</ul>
<p>Example:
- Q: Roger has 5 balls, buys 2 cans with 3 balls each. How many total?
- <strong>Standard</strong>: “11” (no explanation).
- <strong>CoT</strong>: “5 + (2×3) = 11. Answer: 11.”</p>
<hr />
<h2 id="prompt-tuning">Prompt-Tuning<a class="headerlink" href="#prompt-tuning" title="Permanent link">&para;</a></h2>
<ul>
<li>Focuses on <strong>optimizing prompt tokens</strong> instead of model weights.</li>
<li>Efficient: fewer resources needed.</li>
<li>Base model stays intact → general knowledge preserved.</li>
<li>Prompts are optimized for specific tasks.</li>
</ul>
<pre class="mermaid"><code>graph LR
    A[Pretrained Model] --&gt; B[Prompt Tuning Layer]
    B --&gt; C[Task-Specific Adaptation]</code></pre>
<hr />
<h1 id="rag-retrieval-augmented-generation">RAG (Retrieval-Augmented Generation)<a class="headerlink" href="#rag-retrieval-augmented-generation" title="Permanent link">&para;</a></h1>
<ul>
<li>Combines <strong>retrieval + generation</strong>.</li>
<li>Workflow:</li>
<li>Query → embedding model.</li>
<li>Retrieve relevant docs from vector DB.</li>
<li>LLM generates answer grounded in retrieved info.</li>
</ul>
<pre class="mermaid"><code>graph LR
    Q[User Query] --&gt; E[Embedding Model]
    E --&gt; VS[Vector Store]
    VS --&gt; D[Retrieved Docs]
    D --&gt; L[LLM]
    L --&gt; A[Answer]</code></pre>
<hr />
<h1 id="evaluating-llms">Evaluating LLMs<a class="headerlink" href="#evaluating-llms" title="Permanent link">&para;</a></h1>
<h3 id="challenges">Challenges<a class="headerlink" href="#challenges" title="Permanent link">&para;</a></h3>
<ul>
<li>Difficult task, requires multiple methods.</li>
</ul>
<h3 id="methods">Methods<a class="headerlink" href="#methods" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Human Evaluation</strong></li>
<li>Experts assess quality, helpfulness, accuracy.</li>
<li><strong>Test Data</strong></li>
<li>High quality, generalizable, not seen during training.</li>
<li><strong>Elo Rankings</strong></li>
<li>Models ranked via A/B comparisons (e.g., LMSys Arena).</li>
</ol>
<hr />
<h2 id="evaluation-metrics">Evaluation Metrics<a class="headerlink" href="#evaluation-metrics" title="Permanent link">&para;</a></h2>
<h3 id="automated-metrics">Automated Metrics<a class="headerlink" href="#automated-metrics" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Perplexity</strong></li>
<li><strong>BLEU, ROUGE, METEOR</strong> (text similarity)</li>
</ul>
<h3 id="human-evaluation">Human Evaluation<a class="headerlink" href="#human-evaluation" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Likert scales, A/B testing</strong></li>
<li><strong>Elo ratings</strong></li>
</ul>
<h3 id="task-specific-metrics">Task-Specific Metrics<a class="headerlink" href="#task-specific-metrics" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Q&amp;A</strong>: Exact match, F1 score.</li>
<li><strong>Summarization</strong>: ROUGE, BERTScore.</li>
<li><strong>Dialogue</strong>: Appropriateness, engagement.</li>
<li><strong>Code Gen</strong>: Functional correctness, readability.</li>
</ul>
<hr />
<h1 id="why-evaluation">Why Evaluation?<a class="headerlink" href="#why-evaluation" title="Permanent link">&para;</a></h1>
<ul>
<li>Quantify model performance.</li>
<li>Compare models objectively.</li>
<li>Identify areas for improvement.</li>
<li>Track progress in field.</li>
</ul>
<p><strong>Resources</strong>:
- HuggingFace Leaderboard → <a href="https://huggingface.co/spaces/open-llm-leaderboard">Open LLM Leaderboard</a>
- LMSys Arena → <a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard">Chatbot Arena Leaderboard</a></p>
<hr />
<p># Common Metrics in LLM Evaluation</p>
<h3 id="key-metrics">Key Metrics<a class="headerlink" href="#key-metrics" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Perplexity</strong>: Measures how well a model predicts the next word.</li>
<li><strong>BLEU (Bilingual Evaluation Understudy)</strong>: Machine translation accuracy.</li>
<li><strong>ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</strong>: Summarization quality.</li>
<li><strong>METEOR (Metric for Evaluation of Translation with Explicit ORdering)</strong>: Translation accuracy with word order.</li>
<li><strong>Win-Rate</strong>: Percentage of wins in pairwise comparisons.</li>
<li><strong>FID (Fréchet Inception Distance)</strong>: Image generation quality.</li>
</ul>
<hr />
<h1 id="perplexity">Perplexity<a class="headerlink" href="#perplexity" title="Permanent link">&para;</a></h1>
<ul>
<li>Quantifies the <strong>uncertainty</strong> of predicting the next word given a sequence.</li>
<li>Lower perplexity → better predictive performance.</li>
</ul>
<div class="arithmatex">\[Perplexity = 2^{ -\frac{1}{N} \sum_{i=1}^{N} \log_2 p(x_i) }\]</div>
<hr />
<h1 id="reporting-metrics">Reporting Metrics<a class="headerlink" href="#reporting-metrics" title="Permanent link">&para;</a></h1>
<p>Metrics are often reported in benchmark plots:
- <strong>X-axis</strong>: Model size (# of parameters).
- <strong>Y-axis</strong>: Performance score (e.g., MMLU).
- Larger models typically perform better but efficiency varies.</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>Model Performance vs Parameters (MMLU)
</span><span id="__span-3-2"><a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>
</span><span id="__span-3-3"><a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a> 10B | ████████████████████ 60
</span><span id="__span-3-4"><a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a> 30B | ███████████████████████████ 66
</span><span id="__span-3-5"><a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a> 50B | ██████████████████████████████████ 72
</span><span id="__span-3-6"><a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a> 70B | ██████████████████████████████████████████ 78
</span><span id="__span-3-7"><a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a> 90B | █████████████████████████████████████████████ 81
</span></code></pre></div>
<hr />
<h1 id="visualizing-model-benchmarks">Visualizing Model Benchmarks<a class="headerlink" href="#visualizing-model-benchmarks" title="Permanent link">&para;</a></h1>
<ul>
<li><strong>Radar Charts</strong>: Show multi-domain performance (writing, math, coding, reasoning, etc.).</li>
<li><strong>Spider Plots</strong>: Compare multiple models across benchmarks (MMLU, HellaSwag, GSM8K, etc.).</li>
</ul>
<hr />
<h1 id="challenges-in-evaluation">Challenges in Evaluation<a class="headerlink" href="#challenges-in-evaluation" title="Permanent link">&para;</a></h1>
<ol>
<li><strong>Subjectivity</strong>: Hard to measure creativity.</li>
<li><strong>Lack of standardized benchmarks</strong> across domains.</li>
<li><strong>Factual consistency</strong> is difficult to ensure.</li>
<li><strong>Balancing multiple metrics</strong> for holistic performance.</li>
</ol>
<hr />
<h1 id="ethical-considerations-in-llms">Ethical Considerations in LLMs<a class="headerlink" href="#ethical-considerations-in-llms" title="Permanent link">&para;</a></h1>
<h3 id="data-privacy">Data Privacy<a class="headerlink" href="#data-privacy" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Challenges</strong>: Avoid violating privacy (GDPR, CCPA).</li>
<li><strong>Strategies</strong>: Differential privacy → adding noise to training data.</li>
</ul>
<h3 id="hallucinations">Hallucinations<a class="headerlink" href="#hallucinations" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Definition</strong>: Model generates plausible but false info.</li>
<li><strong>Strategies</strong>: Continuous updates, validation checks.</li>
</ul>
<hr />
<h1 id="hallucination-mitigation">Hallucination Mitigation<a class="headerlink" href="#hallucination-mitigation" title="Permanent link">&para;</a></h1>
<h3 id="1-prompt-engineering">1. Prompt Engineering<a class="headerlink" href="#1-prompt-engineering" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>RAG</strong>: Integrates external knowledge bases.</li>
<li><strong>Self-Refinement</strong>: Multiple draft reasoning.</li>
<li><strong>Prompt Tuning</strong>: Optimize prompts for factual accuracy.</li>
</ul>
<h3 id="2-new-architectures">2. New Architectures<a class="headerlink" href="#2-new-architectures" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Knowledge Graph Utilization</strong>: Fact grounding.</li>
<li><strong>CAD (Context-Aware Decoding)</strong>: Prioritizes relevant info.</li>
</ul>
<h3 id="3-supervised-fine-tuning">3. Supervised Fine-Tuning<a class="headerlink" href="#3-supervised-fine-tuning" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Knowledge Injection</strong>: Verified factual training.</li>
<li><strong>R-Tuning</strong>: Refusal-aware instruction tuning.</li>
</ul>
<h3 id="4-validation-feedback">4. Validation &amp; Feedback<a class="headerlink" href="#4-validation-feedback" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>External Knowledge Checks</strong>: Database grounding.</li>
<li><strong>HITL (Human-in-the-Loop)</strong>: Continuous correction.</li>
</ul>
<h3 id="5-advanced-techniques">5. Advanced Techniques<a class="headerlink" href="#5-advanced-techniques" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Chain-of-Thought Prompting</strong>: Encourage reasoning.</li>
<li><strong>Hybrid Approaches</strong>: Combine RAG + knowledge graphs.</li>
</ul>
<hr />
<h1 id="algorithmic-bias-misuse">Algorithmic Bias &amp; Misuse<a class="headerlink" href="#algorithmic-bias-misuse" title="Permanent link">&para;</a></h1>
<h3 id="algorithmic-bias">Algorithmic Bias<a class="headerlink" href="#algorithmic-bias" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Challenges</strong>: Bias from humans in training data.</li>
<li><strong>Strategies</strong>: Diverse datasets, audits, bias detection.</li>
</ul>
<h3 id="potential-for-misuse">Potential for Misuse<a class="headerlink" href="#potential-for-misuse" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Challenges</strong>: Deepfakes, spam, extremist content.</li>
<li><strong>Strategies</strong>: Strict usage policies, monitoring tools.</li>
</ul>
<hr />
<h1 id="mitigating-bias-ensuring-ethical-deployment">Mitigating Bias &amp; Ensuring Ethical Deployment<a class="headerlink" href="#mitigating-bias-ensuring-ethical-deployment" title="Permanent link">&para;</a></h1>
<ul>
<li>Train teams on AI ethics (red teaming).</li>
<li>Engage stakeholders in ethical discussions.</li>
<li>Transparent reporting of AI decisions.</li>
</ul>
<h3 id="llms-in-determining-harmfulness">LLMs in Determining Harmfulness<a class="headerlink" href="#llms-in-determining-harmfulness" title="Permanent link">&para;</a></h3>
<ul>
<li>Detect hate speech, harassment, toxicity.</li>
<li><strong>Applications</strong>: Moderation on online platforms.</li>
<li><strong>Challenges</strong>: Balancing censorship vs free speech.</li>
</ul>
<hr />
<h1 id="example-llama-guard-2-8b">Example: LLaMA Guard 2 (8B)<a class="headerlink" href="#example-llama-guard-2-8b" title="Permanent link">&para;</a></h1>
<ul>
<li>Open-source model for <strong>safety classification</strong>.</li>
<li>Detects harmful content in both prompts and responses.</li>
<li>Ensures safe interaction and moderation.</li>
</ul>
<p><strong>Resource</strong>: <a href="https://huggingface.co/meta-llama/Llama-Guard-2-8b">Meta-LLaMA Guard 2-8B on Hugging Face</a></p>
<hr />
<h1 id="harm-categories-in-llm-safety">Harm Categories in LLM Safety<a class="headerlink" href="#harm-categories-in-llm-safety" title="Permanent link">&para;</a></h1>
<h3 id="common-categories-of-harmful-content">Common Categories of Harmful Content<a class="headerlink" href="#common-categories-of-harmful-content" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>S1</strong>: Violent Crimes  </li>
<li><strong>S2</strong>: Non-Violent Crimes  </li>
<li><strong>S3</strong>: Sex-Related Crimes  </li>
<li><strong>S4</strong>: Child Sexual Exploitation  </li>
<li><strong>S5</strong>: Specialized Advice (e.g., medical/legal)  </li>
<li><strong>S6</strong>: Privacy Violations  </li>
<li><strong>S7</strong>: Intellectual Property Violations  </li>
<li><strong>S8</strong>: Indiscriminate Weapons  </li>
<li><strong>S9</strong>: Hate Speech  </li>
<li><strong>S10</strong>: Suicide &amp; Self-Harm  </li>
<li><strong>S11</strong>: Sexual Content  </li>
</ul>
<hr />
<h1 id="business-aspects-of-safety-moderation">Business Aspects of Safety &amp; Moderation<a class="headerlink" href="#business-aspects-of-safety-moderation" title="Permanent link">&para;</a></h1>
<h3 id="e-commerce-chatbots">E-commerce Chatbots<a class="headerlink" href="#e-commerce-chatbots" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Focus</strong>: Handle product queries, order details, customer support.</li>
<li><strong>Handling OOD</strong>: Recognize unrelated queries (e.g., medical/legal) and politely refuse/redirect.</li>
<li><strong>Example</strong>: Online bookstore bot answering book-related queries only.</li>
</ul>
<h3 id="general-purpose-assistants-alexa-siri">General-Purpose Assistants (Alexa, Siri)<a class="headerlink" href="#general-purpose-assistants-alexa-siri" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Focus</strong>: Wide range of general queries.</li>
<li><strong>Handling OOD</strong>: Provide generic responses or redirect to human experts.</li>
<li><strong>Example</strong>: Alexa suggests consulting a tax professional for tax queries.</li>
</ul>
<h3 id="ai-development-platforms-chatgpt-copilot">AI Development Platforms (ChatGPT, Copilot)<a class="headerlink" href="#ai-development-platforms-chatgpt-copilot" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Focus</strong>: Coding, data analysis, explanations.</li>
<li><strong>Handling OOD</strong>: Refuse unsafe/personal data queries.</li>
<li><strong>Example</strong>: Copilot suggests code snippets but doesn’t execute.</li>
</ul>
<h3 id="industry-specific-bots-healthcare-legal">Industry-Specific Bots (Healthcare, Legal)<a class="headerlink" href="#industry-specific-bots-healthcare-legal" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Focus</strong>: Domain expertise.</li>
<li><strong>Handling OOD</strong>: Avoid queries outside domain; add disclaimers.</li>
<li><strong>Example</strong>: Healthcare bot advises general wellness but not diagnoses.</li>
</ul>
<h3 id="customer-support-bots-hospitality-retail">Customer Support Bots (Hospitality, Retail)<a class="headerlink" href="#customer-support-bots-hospitality-retail" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Focus</strong>: Non-technical queries (booking, service).</li>
<li><strong>Handling OOD</strong>: Pass sensitive or complex issues to humans.</li>
<li><strong>Example</strong>: Hotel bot redirects dietary concerns to staff.</li>
</ul>
<hr />
<h1 id="alphafold-ai-in-protein-folding">AlphaFold: AI in Protein Folding<a class="headerlink" href="#alphafold-ai-in-protein-folding" title="Permanent link">&para;</a></h1>
<ul>
<li><strong>Developed by</strong>: Google DeepMind.</li>
<li><strong>Goal</strong>: Predict 3D structure of proteins from amino acid sequences.</li>
</ul>
<h3 id="applications">Applications<a class="headerlink" href="#applications" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Drug Discovery</strong>: Identifying drug interactions and designing effective therapies.</li>
<li><strong>Biotech &amp; Genomics</strong>: Enzyme engineering, industrial processes, waste recycling.</li>
<li><strong>Medical Research</strong>: Understanding structural basis of diseases.</li>
</ul>
<hr />
<h1 id="real-world-applications-of-generative-ai">Real-World Applications of Generative AI<a class="headerlink" href="#real-world-applications-of-generative-ai" title="Permanent link">&para;</a></h1>
<ul>
<li><strong>Chatbots &amp; Assistants</strong>: Customer inquiries, support, personalization.</li>
<li><strong>Automated Code Generation</strong>: GitHub Copilot, improving developer productivity.</li>
<li><strong>Music &amp; Art Generation</strong>: Novel compositions, digital art.</li>
</ul>
<hr />
<h1 id="creative-ai-songs-from-prompts">Creative AI: Songs from Prompts<a class="headerlink" href="#creative-ai-songs-from-prompts" title="Permanent link">&para;</a></h1>
<ul>
<li>Tools like <strong>Suno AI</strong> can generate full songs from natural language prompts.</li>
<li>Example: Song about bachelor students in India learning AI.</li>
</ul>
<hr />
<h1 id="future-of-generative-ai-llms">Future of Generative AI &amp; LLMs<a class="headerlink" href="#future-of-generative-ai-llms" title="Permanent link">&para;</a></h1>
<h3 id="key-trends">Key Trends<a class="headerlink" href="#key-trends" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Multi-Modal LLMs</strong>: Florence 2, Kosmos-2, GPT-4V, Llava-1.6.</li>
<li><strong>Faster Attention Mechanisms</strong>: Flash attention, Grouped Query Attention.</li>
<li><strong>Resurgence of State-Space Models</strong>: Mamba, RWKV.</li>
</ul>
<h3 id="efficiency-scale">Efficiency &amp; Scale<a class="headerlink" href="#efficiency-scale" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Efficiency Improvements</strong>: Low-parameter LLMs (GLM4, Phi-2, Mistral 7B).</li>
<li><strong>Extended Context</strong>: Infini-Attention for long-term memory.</li>
</ul>
<h3 id="ai-agents">AI Agents<a class="headerlink" href="#ai-agents" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Examples</strong>: OpenDevin (coding), Browser Agents, Langchain.</li>
<li><strong>Role</strong>: Execute tasks, chain multiple tools, reason step-by-step.</li>
</ul>
<hr />
<h1 id="measuring-llm-performance">Measuring LLM Performance<a class="headerlink" href="#measuring-llm-performance" title="Permanent link">&para;</a></h1>
<h3 id="challenges_1">Challenges<a class="headerlink" href="#challenges_1" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Diversity of Applications</strong>: Different domains require tailored metrics.</li>
<li><strong>Subjectivity</strong>: Creative tasks are hard to evaluate.</li>
<li><strong>Traditional Metrics Limitations</strong>: BLEU/ROUGE don’t capture full depth.</li>
</ul>
<h3 id="emerging-approaches">Emerging Approaches<a class="headerlink" href="#emerging-approaches" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Dynamic Datasets</strong>: Regularly updated test sets.</li>
<li><strong>Multi-Dimensional Evaluation</strong>: Combine quantitative + human evaluation.</li>
<li><strong>Cross-Disciplinary</strong>: Insights from linguistics, psychology, cognitive science.</li>
</ul>
<hr />







  
    
  
  


  <aside class="md-source-file">
    
      
  <span class="md-source-file__fact">
    <span class="md-icon" title="Last update">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"/></svg>
    </span>
    <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="September 5, 2025 10:03:13 UTC">September 5, 2025</span>
  </span>

    
    
    
    
  </aside>





                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../lec6/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Reinforcement Learning">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Reinforcement Learning
              </div>
            </div>
          </a>
        
        
          
          <a href="../lec8/" class="md-footer__link md-footer__link--next" aria-label="Next: Causal Inference">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Causal Inference
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="https://github.com/adityachauhan0" target="_blank" rel="noopener" title="GitHub" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.instant", "navigation.tracking", "navigation.indexes", "navigation.sections", "navigation.tabs", "navigation.top", "navigation.footer", "toc.integrate", "toc.follow", "search.suggest", "search.highlight", "search.share", "content.code.copy", "content.code.annotate", "content.tooltips", "header.autohide", "content.action.edit", "content.action.view"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.92b07e13.min.js"></script>
      
        <script src="https://unpkg.com/mermaid@10/dist/mermaid.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
      
        <script src="../../js/katex-init.js"></script>
      
    
  </body>
</html>